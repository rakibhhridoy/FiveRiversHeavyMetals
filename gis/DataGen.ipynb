{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01bdbf67-098d-4460-b56e-7abe10c6b607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading 200 river points from '200Sampling.shp'...\n",
      "✅ Successfully loaded 200 points from the shapefile.\n",
      "Step 1 Complete.\n",
      "\n",
      "Step 2: Interpolating initial features...\n",
      "Step 2 Complete.\n",
      "\n",
      "Step 3: Calculating hydrological and LULC features...\n",
      "  - Aligning rasters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_3420/1988492302.py:25: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river_sample_gdf[\"geometry\"] = river_sample_gdf.geometry.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Loading DEM and NDWI...\n",
      "  - Reading vector files (brickfields and industries)...\n",
      "  - Calculating Euclidean-based flow-path distances...\n",
      "  - Starting LULC extraction loop...\n",
      "    - Processing LULC for year 2017...\n",
      "    - Processing LULC for year 2018...\n",
      "    - Processing LULC for year 2019...\n",
      "    - Processing LULC for year 2020...\n",
      "    - Processing LULC for year 2021...\n",
      "    - Processing LULC for year 2022...\n",
      "  - LULC extraction loop complete.\n",
      "  - Calculating year-to-year LULC changes...\n",
      "Step 3 Complete.\n",
      "\n",
      "Step 4: Saving final output...\n",
      "✅ Final dataset with 200 sample(s) saved.\n",
      "\n",
      "Step 5: Calculating and printing mean LULC changes...\n",
      "  - Mean value for 'LULC_change_2017_2018': 0.2150\n",
      "  - Mean value for 'LULC_change_2018_2019': 0.2050\n",
      "  - Mean value for 'LULC_change_2019_2020': -0.3550\n",
      "  - Mean value for 'LULC_change_2020_2021': 0.2650\n",
      "  - Mean value for 'LULC_change_2021_2022': -0.1450\n",
      "  - Mean value for 'LULC_change_17_22': 0.1850\n",
      "Step 5 Complete.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_3420/1988492302.py:176: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  samples_gdf.to_file(f\"data/{output_name}.shp\")\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'hydro_dist_brick' to 'hydro_dist'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'hydro_dist_ind' to 'hydro_di_1'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'LULC_change_2017_2018' to 'LULC_chang'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'LULC_change_2018_2019' to 'LULC_cha_1'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'LULC_change_2019_2020' to 'LULC_cha_2'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'LULC_change_2020_2021' to 'LULC_cha_3'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'LULC_change_2021_2022' to 'LULC_cha_4'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'LULC_change_17_22' to 'LULC_cha_5'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from rasterio.features import shapes\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from shapely.geometry import shape, Point\n",
    "from scipy.spatial import cKDTree\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "# ===================== 1. Load 200 River Points from File ===================== #\n",
    "print(\"Step 1: Loading 200 river points from '200Sampling.shp'...\")\n",
    "sampling_file_path = \"200Sampling.shp\"\n",
    "try:\n",
    "    # Load the user's pre-created shapefile\n",
    "    river_sample_gdf = gpd.read_file(sampling_file_path)\n",
    "    num_samples = len(river_sample_gdf)\n",
    "    \n",
    "    # Ensure the GeoDataFrame has the correct CRS for further processing\n",
    "    if river_sample_gdf.crs != \"EPSG:4326\":\n",
    "        river_sample_gdf = river_sample_gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Ensure all geometries are points\n",
    "    river_sample_gdf[\"geometry\"] = river_sample_gdf.geometry.centroid\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file '{sampling_file_path}' was not found. Please ensure it is in the correct directory.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: Could not read '{sampling_file_path}'. Please check the file's integrity and format. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "river_coords = np.array([[p.x, p.y] for p in river_sample_gdf.geometry])\n",
    "print(f\"✅ Successfully loaded {num_samples} points from the shapefile.\")\n",
    "print(\"Step 1 Complete.\\n\")\n",
    "\n",
    "# ===================== 2. Interpolate Initial Features ===================== #\n",
    "print(\"Step 2: Interpolating initial features...\")\n",
    "try:\n",
    "    data = pd.read_csv(\"../data/RainySeason.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: RainySeason.csv not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "coords = data[['Long', 'Lat']].values\n",
    "features_to_interpolate = ['CrR', 'NiR', 'CuR', 'AsR', 'CdR', 'PbR', 'MR', 'SandR', 'SiltR', 'ClayR', 'FeR']\n",
    "numeric_features = data[features_to_interpolate]\n",
    "\n",
    "def idw_interpolation(known_coords, known_values, query_coords, power=2):\n",
    "    tree = cKDTree(known_coords)\n",
    "    dists, idxs = tree.query(query_coords, k=4)\n",
    "    dists[dists == 0] = 1e-10\n",
    "    weights = 1 / (dists ** power)\n",
    "    weights /= weights.sum(axis=1)[:, None]\n",
    "    return np.sum(weights * known_values[idxs], axis=1)\n",
    "\n",
    "interpolated_features = np.zeros((len(river_coords), numeric_features.shape[1]))\n",
    "for i, col in enumerate(numeric_features.columns):\n",
    "    interpolated_features[:, i] = idw_interpolation(coords, numeric_features[col].values, river_coords)\n",
    "\n",
    "river_df = pd.DataFrame(interpolated_features, columns=numeric_features.columns)\n",
    "river_df['Long'] = river_coords[:, 0]\n",
    "river_df['Lat'] = river_coords[:, 1]\n",
    "river_df['Source'] = 'River_Interpolated'\n",
    "print(\"Step 2 Complete.\\n\")\n",
    "\n",
    "# ===================== 3. Calculate Hydrological and LULC Features (Optimized) ===================== #\n",
    "print(\"Step 3: Calculating hydrological and LULC features...\")\n",
    "dem_path = \"DEMF.tif\"\n",
    "ndwi_path = \"CalIndices/ndwi.tif\"\n",
    "aligned_ndwi_path = \"ndwi_aligned.tif\"\n",
    "\n",
    "print(\"  - Aligning rasters...\")\n",
    "def align_rasters(base_raster_path, match_raster_path, out_raster_path):\n",
    "    try:\n",
    "        with rasterio.open(base_raster_path) as base:\n",
    "            base_meta = base.meta.copy()\n",
    "            with rasterio.open(match_raster_path) as match:\n",
    "                data = match.read(1)\n",
    "                reprojected = np.empty((base.height, base.width), dtype=np.float32)\n",
    "                reproject(\n",
    "                    source=data,\n",
    "                    destination=reprojected,\n",
    "                    src_transform=match.transform,\n",
    "                    src_crs=match.crs,\n",
    "                    dst_transform=base.transform,\n",
    "                    dst_crs=base.crs,\n",
    "                    resampling=Resampling.bilinear\n",
    "                )\n",
    "                kwargs = base_meta\n",
    "                with rasterio.open(out_raster_path, 'w', **kwargs) as dst:\n",
    "                    dst.write(reprojected, 1)\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"❌ Error: Could not open raster files for alignment. Please check the file paths. Error: {e}\")\n",
    "        exit()\n",
    "    return out_raster_path\n",
    "\n",
    "align_rasters(dem_path, ndwi_path, aligned_ndwi_path)\n",
    "\n",
    "print(\"  - Loading DEM and NDWI...\")\n",
    "with rasterio.open(dem_path) as dem_src:\n",
    "    dem_transform = dem_src.transform\n",
    "    dem_crs = dem_src.crs\n",
    "    dem_resolution = dem_src.res[0]\n",
    "\n",
    "# Create samples_gdf from the loaded points\n",
    "samples_gdf = gpd.GeoDataFrame(river_df, geometry=gpd.points_from_xy(river_df.Long, river_df.Lat), crs=\"EPSG:4326\")\n",
    "samples_gdf = samples_gdf.to_crs(dem_crs)\n",
    "\n",
    "print(\"  - Reading vector files (brickfields and industries)...\")\n",
    "try:\n",
    "    brickfields_path = \"brick_field_point.shp\"\n",
    "    industries_path = \"industry_point.shp\"\n",
    "    brickfields = gpd.read_file(brickfields_path).to_crs(dem_crs)\n",
    "    industries = gpd.read_file(industries_path).to_crs(dem_crs)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: Could not read shapefiles. Please check the file paths and ensure they are valid. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "samples_gdf[\"geometry\"] = samples_gdf.geometry.centroid\n",
    "brickfields[\"geometry\"] = brickfields.geometry.centroid\n",
    "industries[\"geometry\"] = industries.geometry.centroid\n",
    "\n",
    "def world_to_pixel(transform, x, y):\n",
    "    col, row = ~transform * (x, y)\n",
    "    return int(row), int(col)\n",
    "\n",
    "def compute_distances_euclidean(points_gdf, targets_gdf, transform, resolution):\n",
    "    target_pixels = np.array([world_to_pixel(transform, x, y) for x, y in zip(targets_gdf.geometry.x, targets_gdf.geometry.y)])\n",
    "    tree = cKDTree(target_pixels)\n",
    "    distances = []\n",
    "    for px, py in zip(points_gdf.geometry.x, points_gdf.geometry.y):\n",
    "        start = world_to_pixel(transform, px, py)\n",
    "        dist_pixels, _ = tree.query(start)\n",
    "        dist_meters = dist_pixels * resolution\n",
    "        distances.append(dist_meters)\n",
    "    return np.array(distances)\n",
    "\n",
    "print(\"  - Calculating Euclidean-based flow-path distances...\")\n",
    "samples_gdf[\"hydro_dist_brick\"] = compute_distances_euclidean(samples_gdf, brickfields, dem_transform, dem_resolution)\n",
    "samples_gdf[\"hydro_dist_ind\"] = compute_distances_euclidean(samples_gdf, industries, dem_transform, dem_resolution)\n",
    "\n",
    "print(\"  - Starting LULC extraction loop...\")\n",
    "lulc_dir = \"LULCMerged\"\n",
    "years = [2017, 2018, 2019, 2020, 2021, 2022]\n",
    "for y in years:\n",
    "    lulc_path = os.path.join(lulc_dir, f\"LULC{y}.tif\")\n",
    "    print(f\"    - Processing LULC for year {y}...\")\n",
    "    try:\n",
    "        with rasterio.open(lulc_path) as lulc_src:\n",
    "            # Corrected line: convert GeoDataFrame geometry to a list of (x, y) tuples\n",
    "            lulc_values = [x[0] for x in lulc_src.sample([(p.x, p.y) for p in samples_gdf.geometry])]\n",
    "            samples_gdf[f\"LULC_{y}\"] = lulc_values\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"❌ Error processing {lulc_path}: {e}\")\n",
    "        # Add a placeholder column with NaNs if the file can't be read\n",
    "        samples_gdf[f\"LULC_{y}\"] = np.nan\n",
    "        continue\n",
    "print(\"  - LULC extraction loop complete.\")\n",
    "\n",
    "print(\"  - Calculating year-to-year LULC changes...\")\n",
    "for i in range(len(years) - 1):\n",
    "    y1, y2 = years[i], years[i + 1]\n",
    "    # Corrected line: cast to float for numerical representation\n",
    "    samples_gdf[f\"LULC_change_{y1}_{y2}\"] = samples_gdf[f\"LULC_{y2}\"].astype(float) - samples_gdf[f\"LULC_{y1}\"].astype(float)\n",
    "\n",
    "# Corrected line: cast to float for numerical representation\n",
    "samples_gdf[\"LULC_change_17_22\"] = samples_gdf[\"LULC_2022\"].astype(float) - samples_gdf[\"LULC_2017\"].astype(float)\n",
    "print(\"Step 3 Complete.\\n\")\n",
    "\n",
    "# ===================== 4. Save Final Output ===================== #\n",
    "print(\"Step 4: Saving final output...\")\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "output_name = f\"Samples_{num_samples}\"\n",
    "samples_gdf.to_file(f\"data/{output_name}.shp\")\n",
    "samples_gdf.drop(columns=\"geometry\").to_csv(f\"data/{output_name}.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Final dataset with {num_samples} sample(s) saved.\")\n",
    "\n",
    "# ===================== 5. Calculate and Print Mean LULC Changes ===================== #\n",
    "print(\"\\nStep 5: Calculating and printing mean LULC changes...\")\n",
    "lulc_change_columns = [col for col in samples_gdf.columns if \"LULC_change\" in col]\n",
    "for col in lulc_change_columns:\n",
    "    mean_change = samples_gdf[col].mean()\n",
    "    print(f\"  - Mean value for '{col}': {mean_change:.4f}\")\n",
    "print(\"Step 5 Complete.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53098958-d8a7-43e2-ad80-1872d9e4320b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading 200 river points from '200Sampling.shp'...\n",
      "✅ Successfully loaded 200 points from the shapefile.\n",
      "Step 1 Complete.\n",
      "\n",
      "Step 2: Interpolating initial features...\n",
      "Step 2 Complete.\n",
      "\n",
      "Step 3: Calculating hydrological and LULC features...\n",
      "  - Aligning rasters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_3420/787989212.py:25: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river_sample_gdf[\"geometry\"] = river_sample_gdf.geometry.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Loading DEM and NDWI...\n",
      "  - Reading vector files (brickfields and industries)...\n",
      "  - Counting nearby brickfields and industries...\n",
      "  - Counting complete.\n",
      "  - Calculating Euclidean-based flow-path distances...\n",
      "  - Starting LULC extraction loop...\n",
      "    - Processing LULC for year 2017...\n",
      "    - Processing LULC for year 2018...\n",
      "    - Processing LULC for year 2019...\n",
      "    - Processing LULC for year 2020...\n",
      "    - Processing LULC for year 2021...\n",
      "    - Processing LULC for year 2022...\n",
      "  - LULC extraction loop complete.\n",
      "  - Calculating year-to-year LULC changes...\n",
      "Step 3 Complete.\n",
      "\n",
      "Step 4: Saving final output...\n",
      "✅ Final dataset with 200 sample(s) saved.\n",
      "\n",
      "Step 5: Calculating and printing mean LULC changes...\n",
      "  - Mean value for 'LULC_change_2017_2018': 0.2150\n",
      "  - Mean value for 'LULC_change_2018_2019': 0.2050\n",
      "  - Mean value for 'LULC_change_2019_2020': -0.3550\n",
      "  - Mean value for 'LULC_change_2020_2021': 0.2650\n",
      "  - Mean value for 'LULC_change_2021_2022': -0.1450\n",
      "  - Mean value for 'LULC_change_17_22': 0.1850\n",
      "Step 5 Complete.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_3420/787989212.py:198: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  samples_gdf.to_file(f\"data/{output_name}.shp\")\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'num_brick_field' to 'num_brick_'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'num_industry' to 'num_indust'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'hydro_dist_brick' to 'hydro_dist'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'hydro_dist_ind' to 'hydro_di_1'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'LULC_change_2017_2018' to 'LULC_chang'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'LULC_change_2018_2019' to 'LULC_cha_1'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'LULC_change_2019_2020' to 'LULC_cha_2'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'LULC_change_2020_2021' to 'LULC_cha_3'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'LULC_change_2021_2022' to 'LULC_cha_4'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'LULC_change_17_22' to 'LULC_cha_5'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from rasterio.features import shapes\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from shapely.geometry import shape, Point\n",
    "from scipy.spatial import cKDTree\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "# ===================== 1. Load 200 River Points from File ===================== #\n",
    "print(\"Step 1: Loading 200 river points from '200Sampling.shp'...\")\n",
    "sampling_file_path = \"200Sampling.shp\"\n",
    "try:\n",
    "    # Load the user's pre-created shapefile\n",
    "    river_sample_gdf = gpd.read_file(sampling_file_path)\n",
    "    num_samples = len(river_sample_gdf)\n",
    "    \n",
    "    # Ensure the GeoDataFrame has the correct CRS for further processing\n",
    "    if river_sample_gdf.crs != \"EPSG:4326\":\n",
    "        river_sample_gdf = river_sample_gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Ensure all geometries are points\n",
    "    river_sample_gdf[\"geometry\"] = river_sample_gdf.geometry.centroid\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file '{sampling_file_path}' was not found. Please ensure it is in the correct directory.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: Could not read '{sampling_file_path}'. Please check the file's integrity and format. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "river_coords = np.array([[p.x, p.y] for p in river_sample_gdf.geometry])\n",
    "print(f\"✅ Successfully loaded {num_samples} points from the shapefile.\")\n",
    "print(\"Step 1 Complete.\\n\")\n",
    "\n",
    "# ===================== 2. Interpolate Initial Features ===================== #\n",
    "print(\"Step 2: Interpolating initial features...\")\n",
    "try:\n",
    "    data = pd.read_csv(\"../data/RainySeason.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: RainySeason.csv not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "coords = data[['Long', 'Lat']].values\n",
    "features_to_interpolate = ['CrR', 'NiR', 'CuR', 'AsR', 'CdR', 'PbR', 'MR', 'SandR', 'SiltR', 'ClayR', 'FeR']\n",
    "numeric_features = data[features_to_interpolate]\n",
    "\n",
    "def idw_interpolation(known_coords, known_values, query_coords, power=2):\n",
    "    tree = cKDTree(known_coords)\n",
    "    dists, idxs = tree.query(query_coords, k=4)\n",
    "    dists[dists == 0] = 1e-10\n",
    "    weights = 1 / (dists ** power)\n",
    "    weights /= weights.sum(axis=1)[:, None]\n",
    "    return np.sum(weights * known_values[idxs], axis=1)\n",
    "\n",
    "interpolated_features = np.zeros((len(river_coords), numeric_features.shape[1]))\n",
    "for i, col in enumerate(numeric_features.columns):\n",
    "    interpolated_features[:, i] = idw_interpolation(coords, numeric_features[col].values, river_coords)\n",
    "\n",
    "river_df = pd.DataFrame(interpolated_features, columns=numeric_features.columns)\n",
    "river_df['Long'] = river_coords[:, 0]\n",
    "river_df['Lat'] = river_coords[:, 1]\n",
    "river_df['Source'] = 'River_Interpolated'\n",
    "print(\"Step 2 Complete.\\n\")\n",
    "\n",
    "# ===================== 3. Calculate Hydrological and LULC Features (Optimized) ===================== #\n",
    "print(\"Step 3: Calculating hydrological and LULC features...\")\n",
    "dem_path = \"DEMF.tif\"\n",
    "ndwi_path = \"CalIndices/ndwi.tif\"\n",
    "aligned_ndwi_path = \"ndwi_aligned.tif\"\n",
    "\n",
    "print(\"  - Aligning rasters...\")\n",
    "def align_rasters(base_raster_path, match_raster_path, out_raster_path):\n",
    "    try:\n",
    "        with rasterio.open(base_raster_path) as base:\n",
    "            base_meta = base.meta.copy()\n",
    "            with rasterio.open(match_raster_path) as match:\n",
    "                data = match.read(1)\n",
    "                reprojected = np.empty((base.height, base.width), dtype=np.float32)\n",
    "                reproject(\n",
    "                    source=data,\n",
    "                    destination=reprojected,\n",
    "                    src_transform=match.transform,\n",
    "                    src_crs=match.crs,\n",
    "                    dst_transform=base.transform,\n",
    "                    dst_crs=base.crs,\n",
    "                    resampling=Resampling.bilinear\n",
    "                )\n",
    "                kwargs = base_meta\n",
    "                with rasterio.open(out_raster_path, 'w', **kwargs) as dst:\n",
    "                    dst.write(reprojected, 1)\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"❌ Error: Could not open raster files for alignment. Please check the file paths. Error: {e}\")\n",
    "        exit()\n",
    "    return out_raster_path\n",
    "\n",
    "align_rasters(dem_path, ndwi_path, aligned_ndwi_path)\n",
    "\n",
    "print(\"  - Loading DEM and NDWI...\")\n",
    "with rasterio.open(dem_path) as dem_src:\n",
    "    dem_transform = dem_src.transform\n",
    "    dem_crs = dem_src.crs\n",
    "    dem_resolution = dem_src.res[0]\n",
    "\n",
    "# Create samples_gdf from the loaded points\n",
    "samples_gdf = gpd.GeoDataFrame(river_df, geometry=gpd.points_from_xy(river_df.Long, river_df.Lat), crs=\"EPSG:4326\")\n",
    "samples_gdf = samples_gdf.to_crs(dem_crs)\n",
    "\n",
    "print(\"  - Reading vector files (brickfields and industries)...\")\n",
    "try:\n",
    "    brickfields_path = \"brick_field_point.shp\"\n",
    "    industries_path = \"industry_point.shp\"\n",
    "    brickfields = gpd.read_file(brickfields_path).to_crs(dem_crs)\n",
    "    industries = gpd.read_file(industries_path).to_crs(dem_crs)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: Could not read shapefiles. Please check the file paths and ensure they are valid. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "samples_gdf[\"geometry\"] = samples_gdf.geometry.centroid\n",
    "brickfields[\"geometry\"] = brickfields.geometry.centroid\n",
    "industries[\"geometry\"] = industries.geometry.centroid\n",
    "\n",
    "# --- ADDED: Calculate number of brickfields and industries within a buffer ---\n",
    "print(\"  - Counting nearby brickfields and industries...\")\n",
    "buffer_distance = 1000 # 1000 meters\n",
    "samples_gdf['num_brick_field'] = 0\n",
    "samples_gdf['num_industry'] = 0\n",
    "\n",
    "samples_buffered = samples_gdf.copy()\n",
    "samples_buffered['geometry'] = samples_buffered.geometry.buffer(buffer_distance)\n",
    "\n",
    "# Spatial join to count brickfields\n",
    "sjoin_brick = gpd.sjoin(samples_buffered, brickfields, how=\"left\", predicate=\"intersects\")\n",
    "brick_counts = sjoin_brick.groupby(sjoin_brick.index).size()\n",
    "samples_gdf['num_brick_field'] = samples_gdf.index.map(brick_counts).fillna(0).astype(int)\n",
    "\n",
    "# Spatial join to count industries\n",
    "sjoin_ind = gpd.sjoin(samples_buffered, industries, how=\"left\", predicate=\"intersects\")\n",
    "ind_counts = sjoin_ind.groupby(sjoin_ind.index).size()\n",
    "samples_gdf['num_industry'] = samples_gdf.index.map(ind_counts).fillna(0).astype(int)\n",
    "\n",
    "print(\"  - Counting complete.\")\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def world_to_pixel(transform, x, y):\n",
    "    col, row = ~transform * (x, y)\n",
    "    return int(row), int(col)\n",
    "\n",
    "def compute_distances_euclidean(points_gdf, targets_gdf, transform, resolution):\n",
    "    target_pixels = np.array([world_to_pixel(transform, x, y) for x, y in zip(targets_gdf.geometry.x, targets_gdf.geometry.y)])\n",
    "    tree = cKDTree(target_pixels)\n",
    "    distances = []\n",
    "    for px, py in zip(points_gdf.geometry.x, points_gdf.geometry.y):\n",
    "        start = world_to_pixel(transform, px, py)\n",
    "        dist_pixels, _ = tree.query(start)\n",
    "        dist_meters = dist_pixels * resolution\n",
    "        distances.append(dist_meters)\n",
    "    return np.array(distances)\n",
    "\n",
    "print(\"  - Calculating Euclidean-based flow-path distances...\")\n",
    "samples_gdf[\"hydro_dist_brick\"] = compute_distances_euclidean(samples_gdf, brickfields, dem_transform, dem_resolution)\n",
    "samples_gdf[\"hydro_dist_ind\"] = compute_distances_euclidean(samples_gdf, industries, dem_transform, dem_resolution)\n",
    "\n",
    "print(\"  - Starting LULC extraction loop...\")\n",
    "lulc_dir = \"LULCMerged\"\n",
    "years = [2017, 2018, 2019, 2020, 2021, 2022]\n",
    "for y in years:\n",
    "    lulc_path = os.path.join(lulc_dir, f\"LULC{y}.tif\")\n",
    "    print(f\"    - Processing LULC for year {y}...\")\n",
    "    try:\n",
    "        with rasterio.open(lulc_path) as lulc_src:\n",
    "            # Corrected line: convert GeoDataFrame geometry to a list of (x, y) tuples\n",
    "            lulc_values = [x[0] for x in lulc_src.sample([(p.x, p.y) for p in samples_gdf.geometry])]\n",
    "            samples_gdf[f\"LULC_{y}\"] = lulc_values\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"❌ Error processing {lulc_path}: {e}\")\n",
    "        # Add a placeholder column with NaNs if the file can't be read\n",
    "        samples_gdf[f\"LULC_{y}\"] = np.nan\n",
    "        continue\n",
    "print(\"  - LULC extraction loop complete.\")\n",
    "\n",
    "print(\"  - Calculating year-to-year LULC changes...\")\n",
    "for i in range(len(years) - 1):\n",
    "    y1, y2 = years[i], years[i + 1]\n",
    "    # Corrected line: cast to float for numerical representation\n",
    "    samples_gdf[f\"LULC_change_{y1}_{y2}\"] = samples_gdf[f\"LULC_{y2}\"].astype(float) - samples_gdf[f\"LULC_{y1}\"].astype(float)\n",
    "\n",
    "# Corrected line: cast to float for numerical representation\n",
    "samples_gdf[\"LULC_change_17_22\"] = samples_gdf[\"LULC_2022\"].astype(float) - samples_gdf[\"LULC_2017\"].astype(float)\n",
    "print(\"Step 3 Complete.\\n\")\n",
    "\n",
    "# ===================== 4. Save Final Output ===================== #\n",
    "print(\"Step 4: Saving final output...\")\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "output_name = f\"Samples_{num_samples}\"\n",
    "samples_gdf.to_file(f\"data/{output_name}.shp\")\n",
    "samples_gdf.drop(columns=\"geometry\").to_csv(f\"data/{output_name}.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Final dataset with {num_samples} sample(s) saved.\")\n",
    "\n",
    "# ===================== 5. Calculate and Print Mean LULC Changes ===================== #\n",
    "print(\"\\nStep 5: Calculating and printing mean LULC changes...\")\n",
    "lulc_change_columns = [col for col in samples_gdf.columns if \"LULC_change\" in col]\n",
    "for col in lulc_change_columns:\n",
    "    mean_change = samples_gdf[col].mean()\n",
    "    print(f\"  - Mean value for '{col}': {mean_change:.4f}\")\n",
    "print(\"Step 5 Complete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a22bb97a-9620-4ff8-996c-a82575559085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading 200 river points from '200Sampling.shp'...\n",
      "✅ Successfully loaded 200 points from the shapefile.\n",
      "Step 1 Complete.\n",
      "\n",
      "Step 2: Interpolating initial features...\n",
      "Step 2 Complete.\n",
      "\n",
      "Step 3: Calculating hydrological and LULC features...\n",
      "  - Aligning rasters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_3420/4127496946.py:25: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river_sample_gdf[\"geometry\"] = river_sample_gdf.geometry.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Loading DEM and NDWI...\n",
      "  - Reading vector files (brickfields and industries)...\n",
      "  - Counting nearby brickfields and industries...\n",
      "  - Counting complete.\n",
      "  - Calculating Euclidean-based flow-path distances...\n",
      "  - Starting LULC extraction loop...\n",
      "    - Processing LULC for year 2017...\n",
      "    - Processing LULC for year 2018...\n",
      "    - Processing LULC for year 2019...\n",
      "    - Processing LULC for year 2020...\n",
      "    - Processing LULC for year 2021...\n",
      "    - Processing LULC for year 2022...\n",
      "  - LULC extraction loop complete.\n",
      "  - Calculating LULC changes using Change Vector Analysis (CVA)...\n",
      "Step 3 Complete.\n",
      "\n",
      "Step 4: Saving final output...\n",
      "✅ Final dataset with 200 sample(s) saved.\n",
      "\n",
      "Step 5: Calculating and printing mean LULC changes...\n",
      "  - Mean value for 'CVA_Magnitude': 0.5950\n",
      "  - Mean value for 'CVA_Direction': 0.1850\n",
      "Step 5 Complete.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_3420/4127496946.py:198: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  samples_gdf.to_file(f\"data/{output_name}.shp\")\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'num_brick_field' to 'num_brick_'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'num_industry' to 'num_indust'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'hydro_dist_brick' to 'hydro_dist'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'hydro_dist_ind' to 'hydro_di_1'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'CVA_Magnitude' to 'CVA_Magnit'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'CVA_Direction' to 'CVA_Direct'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from rasterio.features import shapes\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from shapely.geometry import shape, Point\n",
    "from scipy.spatial import cKDTree\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "# ===================== 1. Load 200 River Points from File ===================== #\n",
    "print(\"Step 1: Loading 200 river points from '200Sampling.shp'...\")\n",
    "sampling_file_path = \"200Sampling.shp\"\n",
    "try:\n",
    "    # Load the user's pre-created shapefile\n",
    "    river_sample_gdf = gpd.read_file(sampling_file_path)\n",
    "    num_samples = len(river_sample_gdf)\n",
    "    \n",
    "    # Ensure the GeoDataFrame has the correct CRS for further processing\n",
    "    if river_sample_gdf.crs != \"EPSG:4326\":\n",
    "        river_sample_gdf = river_sample_gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Ensure all geometries are points\n",
    "    river_sample_gdf[\"geometry\"] = river_sample_gdf.geometry.centroid\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file '{sampling_file_path}' was not found. Please ensure it is in the correct directory.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: Could not read '{sampling_file_path}'. Please check the file's integrity and format. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "river_coords = np.array([[p.x, p.y] for p in river_sample_gdf.geometry])\n",
    "print(f\"✅ Successfully loaded {num_samples} points from the shapefile.\")\n",
    "print(\"Step 1 Complete.\\n\")\n",
    "\n",
    "# ===================== 2. Interpolate Initial Features ===================== #\n",
    "print(\"Step 2: Interpolating initial features...\")\n",
    "try:\n",
    "    data = pd.read_csv(\"../data/RainySeason.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: RainySeason.csv not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "coords = data[['Long', 'Lat']].values\n",
    "features_to_interpolate = ['CrR', 'NiR', 'CuR', 'AsR', 'CdR', 'PbR', 'MR', 'SandR', 'SiltR', 'ClayR', 'FeR']\n",
    "numeric_features = data[features_to_interpolate]\n",
    "\n",
    "def idw_interpolation(known_coords, known_values, query_coords, power=2):\n",
    "    tree = cKDTree(known_coords)\n",
    "    dists, idxs = tree.query(query_coords, k=4)\n",
    "    dists[dists == 0] = 1e-10\n",
    "    weights = 1 / (dists ** power)\n",
    "    weights /= weights.sum(axis=1)[:, None]\n",
    "    return np.sum(weights * known_values[idxs], axis=1)\n",
    "\n",
    "interpolated_features = np.zeros((len(river_coords), numeric_features.shape[1]))\n",
    "for i, col in enumerate(numeric_features.columns):\n",
    "    interpolated_features[:, i] = idw_interpolation(coords, numeric_features[col].values, river_coords)\n",
    "\n",
    "river_df = pd.DataFrame(interpolated_features, columns=numeric_features.columns)\n",
    "river_df['Long'] = river_coords[:, 0]\n",
    "river_df['Lat'] = river_coords[:, 1]\n",
    "river_df['Source'] = 'River_Interpolated'\n",
    "print(\"Step 2 Complete.\\n\")\n",
    "\n",
    "# ===================== 3. Calculate Hydrological and LULC Features (Optimized) ===================== #\n",
    "print(\"Step 3: Calculating hydrological and LULC features...\")\n",
    "dem_path = \"DEMF.tif\"\n",
    "ndwi_path = \"CalIndices/ndwi.tif\"\n",
    "aligned_ndwi_path = \"ndwi_aligned.tif\"\n",
    "\n",
    "print(\"  - Aligning rasters...\")\n",
    "def align_rasters(base_raster_path, match_raster_path, out_raster_path):\n",
    "    try:\n",
    "        with rasterio.open(base_raster_path) as base:\n",
    "            base_meta = base.meta.copy()\n",
    "            with rasterio.open(match_raster_path) as match:\n",
    "                data = match.read(1)\n",
    "                reprojected = np.empty((base.height, base.width), dtype=np.float32)\n",
    "                reproject(\n",
    "                    source=data,\n",
    "                    destination=reprojected,\n",
    "                    src_transform=match.transform,\n",
    "                    src_crs=match.crs,\n",
    "                    dst_transform=base.transform,\n",
    "                    dst_crs=base.crs,\n",
    "                    resampling=Resampling.bilinear\n",
    "                )\n",
    "                kwargs = base_meta\n",
    "                with rasterio.open(out_raster_path, 'w', **kwargs) as dst:\n",
    "                    dst.write(reprojected, 1)\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"❌ Error: Could not open raster files for alignment. Please check the file paths. Error: {e}\")\n",
    "        exit()\n",
    "    return out_raster_path\n",
    "\n",
    "align_rasters(dem_path, ndwi_path, aligned_ndwi_path)\n",
    "\n",
    "print(\"  - Loading DEM and NDWI...\")\n",
    "with rasterio.open(dem_path) as dem_src:\n",
    "    dem_transform = dem_src.transform\n",
    "    dem_crs = dem_src.crs\n",
    "    dem_resolution = dem_src.res[0]\n",
    "\n",
    "# Create samples_gdf from the loaded points\n",
    "samples_gdf = gpd.GeoDataFrame(river_df, geometry=gpd.points_from_xy(river_df.Long, river_df.Lat), crs=\"EPSG:4326\")\n",
    "samples_gdf = samples_gdf.to_crs(dem_crs)\n",
    "\n",
    "print(\"  - Reading vector files (brickfields and industries)...\")\n",
    "try:\n",
    "    brickfields_path = \"brick_field_point.shp\"\n",
    "    industries_path = \"industry_point.shp\"\n",
    "    brickfields = gpd.read_file(brickfields_path).to_crs(dem_crs)\n",
    "    industries = gpd.read_file(industries_path).to_crs(dem_crs)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: Could not read shapefiles. Please check the file paths and ensure they are valid. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "samples_gdf[\"geometry\"] = samples_gdf.geometry.centroid\n",
    "brickfields[\"geometry\"] = brickfields.geometry.centroid\n",
    "industries[\"geometry\"] = industries.geometry.centroid\n",
    "\n",
    "# --- ADDED: Calculate number of brickfields and industries within a buffer ---\n",
    "print(\"  - Counting nearby brickfields and industries...\")\n",
    "buffer_distance = 1000 # 1000 meters\n",
    "samples_gdf['num_brick_field'] = 0\n",
    "samples_gdf['num_industry'] = 0\n",
    "\n",
    "samples_buffered = samples_gdf.copy()\n",
    "samples_buffered['geometry'] = samples_buffered.geometry.buffer(buffer_distance)\n",
    "\n",
    "# Spatial join to count brickfields\n",
    "sjoin_brick = gpd.sjoin(samples_buffered, brickfields, how=\"left\", predicate=\"intersects\")\n",
    "brick_counts = sjoin_brick.groupby(sjoin_brick.index).size()\n",
    "samples_gdf['num_brick_field'] = samples_gdf.index.map(brick_counts).fillna(0).astype(int)\n",
    "\n",
    "# Spatial join to count industries\n",
    "sjoin_ind = gpd.sjoin(samples_buffered, industries, how=\"left\", predicate=\"intersects\")\n",
    "ind_counts = sjoin_ind.groupby(sjoin_ind.index).size()\n",
    "samples_gdf['num_industry'] = samples_gdf.index.map(ind_counts).fillna(0).astype(int)\n",
    "\n",
    "print(\"  - Counting complete.\")\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def world_to_pixel(transform, x, y):\n",
    "    col, row = ~transform * (x, y)\n",
    "    return int(row), int(col)\n",
    "\n",
    "def compute_distances_euclidean(points_gdf, targets_gdf, transform, resolution):\n",
    "    target_pixels = np.array([world_to_pixel(transform, x, y) for x, y in zip(targets_gdf.geometry.x, targets_gdf.geometry.y)])\n",
    "    tree = cKDTree(target_pixels)\n",
    "    distances = []\n",
    "    for px, py in zip(points_gdf.geometry.x, points_gdf.geometry.y):\n",
    "        start = world_to_pixel(transform, px, py)\n",
    "        dist_pixels, _ = tree.query(start)\n",
    "        dist_meters = dist_pixels * resolution\n",
    "        distances.append(dist_meters)\n",
    "    return np.array(distances)\n",
    "\n",
    "print(\"  - Calculating Euclidean-based flow-path distances...\")\n",
    "samples_gdf[\"hydro_dist_brick\"] = compute_distances_euclidean(samples_gdf, brickfields, dem_transform, dem_resolution)\n",
    "samples_gdf[\"hydro_dist_ind\"] = compute_distances_euclidean(samples_gdf, industries, dem_transform, dem_resolution)\n",
    "\n",
    "print(\"  - Starting LULC extraction loop...\")\n",
    "lulc_dir = \"LULCMerged\"\n",
    "years = [2017, 2018, 2019, 2020, 2021, 2022]\n",
    "for y in years:\n",
    "    lulc_path = os.path.join(lulc_dir, f\"LULC{y}.tif\")\n",
    "    print(f\"    - Processing LULC for year {y}...\")\n",
    "    try:\n",
    "        with rasterio.open(lulc_path) as lulc_src:\n",
    "            # Corrected line: convert GeoDataFrame geometry to a list of (x, y) tuples\n",
    "            lulc_values = [x[0] for x in lulc_src.sample([(p.x, p.y) for p in samples_gdf.geometry])]\n",
    "            samples_gdf[f\"LULC_{y}\"] = lulc_values\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"❌ Error processing {lulc_path}: {e}\")\n",
    "        # Add a placeholder column with NaNs if the file can't be read\n",
    "        samples_gdf[f\"LULC_{y}\"] = np.nan\n",
    "        continue\n",
    "print(\"  - LULC extraction loop complete.\")\n",
    "\n",
    "# --- UPDATED: Calculate LULC Change using Change Vector Analysis (CVA) ---\n",
    "print(\"  - Calculating LULC changes using Change Vector Analysis (CVA)...\")\n",
    "if \"LULC_2017\" in samples_gdf.columns and \"LULC_2022\" in samples_gdf.columns:\n",
    "    lulc_change_vector = samples_gdf[\"LULC_2022\"].astype(float) - samples_gdf[\"LULC_2017\"].astype(float)\n",
    "    samples_gdf[\"CVA_Magnitude\"] = np.abs(lulc_change_vector)\n",
    "    samples_gdf[\"CVA_Direction\"] = lulc_change_vector\n",
    "else:\n",
    "    print(\"❌ Error: LULC_2017 and LULC_2022 columns not found. Cannot perform CVA.\")\n",
    "print(\"Step 3 Complete.\\n\")\n",
    "\n",
    "# ===================== 4. Save Final Output ===================== #\n",
    "print(\"Step 4: Saving final output...\")\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "output_name = f\"Samples_{num_samples}\"\n",
    "samples_gdf.to_file(f\"data/{output_name}.shp\")\n",
    "samples_gdf.drop(columns=\"geometry\").to_csv(f\"data/{output_name}.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Final dataset with {num_samples} sample(s) saved.\")\n",
    "\n",
    "# ===================== 5. Calculate and Print Mean LULC Changes ===================== #\n",
    "print(\"\\nStep 5: Calculating and printing mean LULC changes...\")\n",
    "cva_columns = [\"CVA_Magnitude\", \"CVA_Direction\"]\n",
    "for col in cva_columns:\n",
    "    if col in samples_gdf.columns:\n",
    "        mean_change = samples_gdf[col].mean()\n",
    "        print(f\"  - Mean value for '{col}': {mean_change:.4f}\")\n",
    "print(\"Step 5 Complete.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbdf2bf7-2c70-4edb-9527-ea35e190170b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading 200 river points from '200Sampling.shp'...\n",
      "✅ Successfully loaded 200 points from the shapefile.\n",
      "Step 1 Complete.\n",
      "\n",
      "Step 2: Interpolating initial features...\n",
      "Step 2 Complete.\n",
      "\n",
      "Step 3: Calculating hydrological and LULC features...\n",
      "  - Aligning rasters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_3420/2706620739.py:25: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river_sample_gdf[\"geometry\"] = river_sample_gdf.geometry.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Loading DEM and NDWI...\n",
      "  - Reading vector files (brickfields and industries)...\n",
      "  - Counting nearby brickfields and industries...\n",
      "  - Counting complete.\n",
      "  - Calculating Euclidean-based flow-path distances...\n",
      "  - Starting LULC extraction loop...\n",
      "    - Processing LULC for year 2017...\n",
      "    - Processing LULC for year 2018...\n",
      "    - Processing LULC for year 2019...\n",
      "    - Processing LULC for year 2020...\n",
      "    - Processing LULC for year 2021...\n",
      "    - Processing LULC for year 2022...\n",
      "  - LULC extraction loop complete.\n",
      "  - Calculating LULC changes using Change Vector Analysis (CVA)...\n",
      "Step 3 Complete.\n",
      "\n",
      "Step 4: Saving final output...\n",
      "✅ Final dataset with 200 sample(s) saved.\n",
      "\n",
      "Step 5: Calculating and printing mean LULC changes...\n",
      "  - Mean value for 'CVA_Magnitude': 0.5950\n",
      "  - Mean value for 'CVA_Direction': 0.1850\n",
      "Step 5 Complete.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_3420/2706620739.py:198: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  samples_gdf.to_file(f\"data/{output_name}.shp\")\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'num_brick_field' to 'num_brick_'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'num_industry' to 'num_indust'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'hydro_dist_brick' to 'hydro_dist'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'hydro_dist_ind' to 'hydro_di_1'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'CVA_Magnitude' to 'CVA_Magnit'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'CVA_Direction' to 'CVA_Direct'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from rasterio.features import shapes\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from shapely.geometry import shape, Point\n",
    "from scipy.spatial import cKDTree\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "# ===================== 1. Load 200 River Points from File ===================== #\n",
    "print(\"Step 1: Loading 200 river points from '200Sampling.shp'...\")\n",
    "sampling_file_path = \"200Sampling.shp\"\n",
    "try:\n",
    "    # Load the user's pre-created shapefile\n",
    "    river_sample_gdf = gpd.read_file(sampling_file_path)\n",
    "    num_samples = len(river_sample_gdf)\n",
    "    \n",
    "    # Ensure the GeoDataFrame has the correct CRS for further processing\n",
    "    if river_sample_gdf.crs != \"EPSG:4326\":\n",
    "        river_sample_gdf = river_sample_gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Ensure all geometries are points\n",
    "    river_sample_gdf[\"geometry\"] = river_sample_gdf.geometry.centroid\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file '{sampling_file_path}' was not found. Please ensure it is in the correct directory.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: Could not read '{sampling_file_path}'. Please check the file's integrity and format. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "river_coords = np.array([[p.x, p.y] for p in river_sample_gdf.geometry])\n",
    "print(f\"✅ Successfully loaded {num_samples} points from the shapefile.\")\n",
    "print(\"Step 1 Complete.\\n\")\n",
    "\n",
    "# ===================== 2. Interpolate Initial Features ===================== #\n",
    "print(\"Step 2: Interpolating initial features...\")\n",
    "try:\n",
    "    data = pd.read_csv(\"../data/RainySeason.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: RainySeason.csv not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "coords = data[['Long', 'Lat']].values\n",
    "features_to_interpolate = ['CrR', 'NiR', 'CuR', 'AsR', 'CdR', 'PbR', 'MR', 'SandR', 'SiltR', 'ClayR', 'FeR']\n",
    "numeric_features = data[features_to_interpolate]\n",
    "\n",
    "def idw_interpolation(known_coords, known_values, query_coords, power=2):\n",
    "    tree = cKDTree(known_coords)\n",
    "    dists, idxs = tree.query(query_coords, k=4)\n",
    "    dists[dists == 0] = 1e-10\n",
    "    weights = 1 / (dists ** power)\n",
    "    weights /= weights.sum(axis=1)[:, None]\n",
    "    return np.sum(weights * known_values[idxs], axis=1)\n",
    "\n",
    "interpolated_features = np.zeros((len(river_coords), numeric_features.shape[1]))\n",
    "for i, col in enumerate(numeric_features.columns):\n",
    "    interpolated_features[:, i] = idw_interpolation(coords, numeric_features[col].values, river_coords)\n",
    "\n",
    "river_df = pd.DataFrame(interpolated_features, columns=numeric_features.columns)\n",
    "river_df['Long'] = river_coords[:, 0]\n",
    "river_df['Lat'] = river_coords[:, 1]\n",
    "river_df['Source'] = 'River_Interpolated'\n",
    "print(\"Step 2 Complete.\\n\")\n",
    "\n",
    "# ===================== 3. Calculate Hydrological and LULC Features (Optimized) ===================== #\n",
    "print(\"Step 3: Calculating hydrological and LULC features...\")\n",
    "dem_path = \"DEMF.tif\"\n",
    "ndwi_path = \"CalIndices/ndwi.tif\"\n",
    "aligned_ndwi_path = \"ndwi_aligned.tif\"\n",
    "\n",
    "print(\"  - Aligning rasters...\")\n",
    "def align_rasters(base_raster_path, match_raster_path, out_raster_path):\n",
    "    try:\n",
    "        with rasterio.open(base_raster_path) as base:\n",
    "            base_meta = base.meta.copy()\n",
    "            with rasterio.open(match_raster_path) as match:\n",
    "                data = match.read(1)\n",
    "                reprojected = np.empty((base.height, base.width), dtype=np.float32)\n",
    "                reproject(\n",
    "                    source=data,\n",
    "                    destination=reprojected,\n",
    "                    src_transform=match.transform,\n",
    "                    src_crs=match.crs,\n",
    "                    dst_transform=base.transform,\n",
    "                    dst_crs=base.crs,\n",
    "                    resampling=Resampling.bilinear\n",
    "                )\n",
    "                kwargs = base_meta\n",
    "                with rasterio.open(out_raster_path, 'w', **kwargs) as dst:\n",
    "                    dst.write(reprojected, 1)\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"❌ Error: Could not open raster files for alignment. Please check the file paths. Error: {e}\")\n",
    "        exit()\n",
    "    return out_raster_path\n",
    "\n",
    "align_rasters(dem_path, ndwi_path, aligned_ndwi_path)\n",
    "\n",
    "print(\"  - Loading DEM and NDWI...\")\n",
    "with rasterio.open(dem_path) as dem_src:\n",
    "    dem_transform = dem_src.transform\n",
    "    dem_crs = dem_src.crs\n",
    "    dem_resolution = dem_src.res[0]\n",
    "\n",
    "# Create samples_gdf from the loaded points\n",
    "samples_gdf = gpd.GeoDataFrame(river_df, geometry=gpd.points_from_xy(river_df.Long, river_df.Lat), crs=\"EPSG:4326\")\n",
    "samples_gdf = samples_gdf.to_crs(dem_crs)\n",
    "\n",
    "print(\"  - Reading vector files (brickfields and industries)...\")\n",
    "try:\n",
    "    brickfields_path = \"brick_field_point.shp\"\n",
    "    industries_path = \"industry_point.shp\"\n",
    "    brickfields = gpd.read_file(brickfields_path).to_crs(dem_crs)\n",
    "    industries = gpd.read_file(industries_path).to_crs(dem_crs)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: Could not read shapefiles. Please check the file paths and ensure they are valid. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "samples_gdf[\"geometry\"] = samples_gdf.geometry.centroid\n",
    "brickfields[\"geometry\"] = brickfields.geometry.centroid\n",
    "industries[\"geometry\"] = industries.geometry.centroid\n",
    "\n",
    "# --- ADDED: Calculate number of brickfields and industries within a buffer ---\n",
    "print(\"  - Counting nearby brickfields and industries...\")\n",
    "buffer_distance = 2000 # 2000 meters\n",
    "samples_gdf['num_brick_field'] = 0\n",
    "samples_gdf['num_industry'] = 0\n",
    "\n",
    "samples_buffered = samples_gdf.copy()\n",
    "samples_buffered['geometry'] = samples_buffered.geometry.buffer(buffer_distance)\n",
    "\n",
    "# Spatial join to count brickfields\n",
    "sjoin_brick = gpd.sjoin(samples_buffered, brickfields, how=\"left\", predicate=\"intersects\")\n",
    "brick_counts = sjoin_brick.groupby(sjoin_brick.index).size()\n",
    "samples_gdf['num_brick_field'] = samples_gdf.index.map(brick_counts).fillna(0).astype(int)\n",
    "\n",
    "# Spatial join to count industries\n",
    "sjoin_ind = gpd.sjoin(samples_buffered, industries, how=\"left\", predicate=\"intersects\")\n",
    "ind_counts = sjoin_ind.groupby(sjoin_ind.index).size()\n",
    "samples_gdf['num_industry'] = samples_gdf.index.map(ind_counts).fillna(0).astype(int)\n",
    "\n",
    "print(\"  - Counting complete.\")\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def world_to_pixel(transform, x, y):\n",
    "    col, row = ~transform * (x, y)\n",
    "    return int(row), int(col)\n",
    "\n",
    "def compute_distances_euclidean(points_gdf, targets_gdf, transform, resolution):\n",
    "    target_pixels = np.array([world_to_pixel(transform, x, y) for x, y in zip(targets_gdf.geometry.x, targets_gdf.geometry.y)])\n",
    "    tree = cKDTree(target_pixels)\n",
    "    distances = []\n",
    "    for px, py in zip(points_gdf.geometry.x, points_gdf.geometry.y):\n",
    "        start = world_to_pixel(transform, px, py)\n",
    "        dist_pixels, _ = tree.query(start)\n",
    "        dist_meters = dist_pixels * resolution\n",
    "        distances.append(dist_meters)\n",
    "    return np.array(distances)\n",
    "\n",
    "print(\"  - Calculating Euclidean-based flow-path distances...\")\n",
    "samples_gdf[\"hydro_dist_brick\"] = compute_distances_euclidean(samples_gdf, brickfields, dem_transform, dem_resolution)\n",
    "samples_gdf[\"hydro_dist_ind\"] = compute_distances_euclidean(samples_gdf, industries, dem_transform, dem_resolution)\n",
    "\n",
    "print(\"  - Starting LULC extraction loop...\")\n",
    "lulc_dir = \"LULCMerged\"\n",
    "years = [2017, 2018, 2019, 2020, 2021, 2022]\n",
    "for y in years:\n",
    "    lulc_path = os.path.join(lulc_dir, f\"LULC{y}.tif\")\n",
    "    print(f\"    - Processing LULC for year {y}...\")\n",
    "    try:\n",
    "        with rasterio.open(lulc_path) as lulc_src:\n",
    "            # Corrected line: convert GeoDataFrame geometry to a list of (x, y) tuples\n",
    "            lulc_values = [x[0] for x in lulc_src.sample([(p.x, p.y) for p in samples_gdf.geometry])]\n",
    "            samples_gdf[f\"LULC_{y}\"] = lulc_values\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"❌ Error processing {lulc_path}: {e}\")\n",
    "        # Add a placeholder column with NaNs if the file can't be read\n",
    "        samples_gdf[f\"LULC_{y}\"] = np.nan\n",
    "        continue\n",
    "print(\"  - LULC extraction loop complete.\")\n",
    "\n",
    "# --- UPDATED: Calculate LULC Change using Change Vector Analysis (CVA) ---\n",
    "print(\"  - Calculating LULC changes using Change Vector Analysis (CVA)...\")\n",
    "if \"LULC_2017\" in samples_gdf.columns and \"LULC_2022\" in samples_gdf.columns:\n",
    "    lulc_change_vector = samples_gdf[\"LULC_2022\"].astype(float) - samples_gdf[\"LULC_2017\"].astype(float)\n",
    "    samples_gdf[\"CVA_Magnitude\"] = np.abs(lulc_change_vector)\n",
    "    samples_gdf[\"CVA_Direction\"] = lulc_change_vector\n",
    "else:\n",
    "    print(\"❌ Error: LULC_2017 and LULC_2022 columns not found. Cannot perform CVA.\")\n",
    "print(\"Step 3 Complete.\\n\")\n",
    "\n",
    "# ===================== 4. Save Final Output ===================== #\n",
    "print(\"Step 4: Saving final output...\")\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "output_name = f\"Samples_{num_samples}\"\n",
    "samples_gdf.to_file(f\"data/{output_name}.shp\")\n",
    "samples_gdf.drop(columns=\"geometry\").to_csv(f\"data/{output_name}.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Final dataset with {num_samples} sample(s) saved.\")\n",
    "\n",
    "# ===================== 5. Calculate and Print Mean LULC Changes ===================== #\n",
    "print(\"\\nStep 5: Calculating and printing mean LULC changes...\")\n",
    "cva_columns = [\"CVA_Magnitude\", \"CVA_Direction\"]\n",
    "for col in cva_columns:\n",
    "    if col in samples_gdf.columns:\n",
    "        mean_change = samples_gdf[col].mean()\n",
    "        print(f\"  - Mean value for '{col}': {mean_change:.4f}\")\n",
    "print(\"Step 5 Complete.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f188e3-58fc-44c4-8b3a-bca2ba28b17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading 200 river points from '200Sampling.shp'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_2589/1968649024.py:25: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river_sample_gdf[\"geometry\"] = river_sample_gdf.geometry.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded 200 points from the shapefile.\n",
      "Step 1 Complete.\n",
      "\n",
      "Step 2: Interpolating initial features...\n",
      "Step 2 Complete.\n",
      "\n",
      "Step 3: Calculating hydrological and LULC features...\n",
      "  - Aligning rasters...\n",
      "  - Loading DEM and NDWI...\n",
      "  - Reading vector files (brickfields and industries)...\n",
      "  - Counting nearby brickfields and industries...\n",
      "  - Counting complete.\n",
      "  - Calculating Euclidean-based flow-path distances...\n",
      "  - Starting LULC extraction loop...\n",
      "    - Processing LULC for year 2017...\n",
      "    - Processing LULC for year 2018...\n",
      "    - Processing LULC for year 2019...\n",
      "    - Processing LULC for year 2020...\n",
      "    - Processing LULC for year 2021...\n",
      "    - Processing LULC for year 2022...\n",
      "  - LULC extraction loop complete.\n",
      "  - Calculating LULC changes using Change Vector Analysis (CVA)...\n",
      "Step 3 Complete.\n",
      "\n",
      "Step 4: Saving final output...\n",
      "✅ Final dataset with 200 sample(s) saved.\n",
      "\n",
      "Step 5: Calculating and printing mean LULC changes...\n",
      "  - Mean value for 'CVA_Magnitude': 0.5950\n",
      "  - Mean value for 'CVA_Direction': 0.1850\n",
      "Step 5 Complete.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_2589/1968649024.py:198: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  samples_gdf.to_file(f\"data/{output_name}.shp\")\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'num_brick_field' to 'num_brick_'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'num_industry' to 'num_indust'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'hydro_dist_brick' to 'hydro_dist'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'hydro_dist_ind' to 'hydro_di_1'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'CVA_Magnitude' to 'CVA_Magnit'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'CVA_Direction' to 'CVA_Direct'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from rasterio.features import shapes\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from shapely.geometry import shape, Point\n",
    "from scipy.spatial import cKDTree\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "# ===================== 1. Load 200 River Points from File ===================== #\n",
    "print(\"Step 1: Loading 200 river points from '200Sampling.shp'...\")\n",
    "sampling_file_path = \"200Sampling.shp\"\n",
    "try:\n",
    "    # Load the user's pre-created shapefile\n",
    "    river_sample_gdf = gpd.read_file(sampling_file_path)\n",
    "    num_samples = len(river_sample_gdf)\n",
    "    \n",
    "    # Ensure the GeoDataFrame has the correct CRS for further processing\n",
    "    if river_sample_gdf.crs != \"EPSG:4326\":\n",
    "        river_sample_gdf = river_sample_gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Ensure all geometries are points\n",
    "    river_sample_gdf[\"geometry\"] = river_sample_gdf.geometry.centroid\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file '{sampling_file_path}' was not found. Please ensure it is in the correct directory.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: Could not read '{sampling_file_path}'. Please check the file's integrity and format. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "river_coords = np.array([[p.x, p.y] for p in river_sample_gdf.geometry])\n",
    "print(f\"✅ Successfully loaded {num_samples} points from the shapefile.\")\n",
    "print(\"Step 1 Complete.\\n\")\n",
    "\n",
    "# ===================== 2. Interpolate Initial Features ===================== #\n",
    "print(\"Step 2: Interpolating initial features...\")\n",
    "try:\n",
    "    data = pd.read_csv(\"../data/RainySeason.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: RainySeason.csv not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "coords = data[['Long', 'Lat']].values\n",
    "features_to_interpolate = ['CrR', 'NiR', 'CuR', 'AsR', 'CdR', 'PbR', 'MR', 'SandR', 'SiltR', 'ClayR', 'FeR', \"RI\"]\n",
    "numeric_features = data[features_to_interpolate]\n",
    "\n",
    "def idw_interpolation(known_coords, known_values, query_coords, power=2):\n",
    "    tree = cKDTree(known_coords)\n",
    "    dists, idxs = tree.query(query_coords, k=4)\n",
    "    dists[dists == 0] = 1e-10\n",
    "    weights = 1 / (dists ** power)\n",
    "    weights /= weights.sum(axis=1)[:, None]\n",
    "    return np.sum(weights * known_values[idxs], axis=1)\n",
    "\n",
    "interpolated_features = np.zeros((len(river_coords), numeric_features.shape[1]))\n",
    "for i, col in enumerate(numeric_features.columns):\n",
    "    interpolated_features[:, i] = idw_interpolation(coords, numeric_features[col].values, river_coords)\n",
    "\n",
    "river_df = pd.DataFrame(interpolated_features, columns=numeric_features.columns)\n",
    "river_df['Long'] = river_coords[:, 0]\n",
    "river_df['Lat'] = river_coords[:, 1]\n",
    "river_df['Source'] = 'River_Interpolated'\n",
    "print(\"Step 2 Complete.\\n\")\n",
    "\n",
    "# ===================== 3. Calculate Hydrological and LULC Features (Optimized) ===================== #\n",
    "print(\"Step 3: Calculating hydrological and LULC features...\")\n",
    "dem_path = \"DEMF.tif\"\n",
    "ndwi_path = \"CalIndices/ndwi.tif\"\n",
    "aligned_ndwi_path = \"ndwi_aligned.tif\"\n",
    "\n",
    "print(\"  - Aligning rasters...\")\n",
    "def align_rasters(base_raster_path, match_raster_path, out_raster_path):\n",
    "    try:\n",
    "        with rasterio.open(base_raster_path) as base:\n",
    "            base_meta = base.meta.copy()\n",
    "            with rasterio.open(match_raster_path) as match:\n",
    "                data = match.read(1)\n",
    "                reprojected = np.empty((base.height, base.width), dtype=np.float32)\n",
    "                reproject(\n",
    "                    source=data,\n",
    "                    destination=reprojected,\n",
    "                    src_transform=match.transform,\n",
    "                    src_crs=match.crs,\n",
    "                    dst_transform=base.transform,\n",
    "                    dst_crs=base.crs,\n",
    "                    resampling=Resampling.bilinear\n",
    "                )\n",
    "                kwargs = base_meta\n",
    "                with rasterio.open(out_raster_path, 'w', **kwargs) as dst:\n",
    "                    dst.write(reprojected, 1)\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"❌ Error: Could not open raster files for alignment. Please check the file paths. Error: {e}\")\n",
    "        exit()\n",
    "    return out_raster_path\n",
    "\n",
    "align_rasters(dem_path, ndwi_path, aligned_ndwi_path)\n",
    "\n",
    "print(\"  - Loading DEM and NDWI...\")\n",
    "with rasterio.open(dem_path) as dem_src:\n",
    "    dem_transform = dem_src.transform\n",
    "    dem_crs = dem_src.crs\n",
    "    dem_resolution = dem_src.res[0]\n",
    "\n",
    "# Create samples_gdf from the loaded points\n",
    "samples_gdf = gpd.GeoDataFrame(river_df, geometry=gpd.points_from_xy(river_df.Long, river_df.Lat), crs=\"EPSG:4326\")\n",
    "samples_gdf = samples_gdf.to_crs(dem_crs)\n",
    "\n",
    "print(\"  - Reading vector files (brickfields and industries)...\")\n",
    "try:\n",
    "    brickfields_path = \"brick_field_point.shp\"\n",
    "    industries_path = \"industry_point.shp\"\n",
    "    brickfields = gpd.read_file(brickfields_path).to_crs(dem_crs)\n",
    "    industries = gpd.read_file(industries_path).to_crs(dem_crs)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: Could not read shapefiles. Please check the file paths and ensure they are valid. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "samples_gdf[\"geometry\"] = samples_gdf.geometry.centroid\n",
    "brickfields[\"geometry\"] = brickfields.geometry.centroid\n",
    "industries[\"geometry\"] = industries.geometry.centroid\n",
    "\n",
    "# --- ADDED: Calculate number of brickfields and industries within a buffer ---\n",
    "print(\"  - Counting nearby brickfields and industries...\")\n",
    "buffer_distance = 1000 # 1000 meters\n",
    "samples_gdf['num_brick_field'] = 0\n",
    "samples_gdf['num_industry'] = 0\n",
    "\n",
    "samples_buffered = samples_gdf.copy()\n",
    "samples_buffered['geometry'] = samples_buffered.geometry.buffer(buffer_distance)\n",
    "\n",
    "# Spatial join to count brickfields\n",
    "sjoin_brick = gpd.sjoin(samples_buffered, brickfields, how=\"left\", predicate=\"intersects\")\n",
    "brick_counts = sjoin_brick.groupby(sjoin_brick.index).size()\n",
    "samples_gdf['num_brick_field'] = samples_gdf.index.map(brick_counts).fillna(0).astype(int)\n",
    "\n",
    "# Spatial join to count industries\n",
    "sjoin_ind = gpd.sjoin(samples_buffered, industries, how=\"left\", predicate=\"intersects\")\n",
    "ind_counts = sjoin_ind.groupby(sjoin_ind.index).size()\n",
    "samples_gdf['num_industry'] = samples_gdf.index.map(ind_counts).fillna(0).astype(int)\n",
    "\n",
    "print(\"  - Counting complete.\")\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def world_to_pixel(transform, x, y):\n",
    "    col, row = ~transform * (x, y)\n",
    "    return int(row), int(col)\n",
    "\n",
    "def compute_distances_euclidean(points_gdf, targets_gdf, transform, resolution):\n",
    "    target_pixels = np.array([world_to_pixel(transform, x, y) for x, y in zip(targets_gdf.geometry.x, targets_gdf.geometry.y)])\n",
    "    tree = cKDTree(target_pixels)\n",
    "    distances = []\n",
    "    for px, py in zip(points_gdf.geometry.x, points_gdf.geometry.y):\n",
    "        start = world_to_pixel(transform, px, py)\n",
    "        dist_pixels, _ = tree.query(start)\n",
    "        dist_meters = dist_pixels * resolution\n",
    "        distances.append(dist_meters)\n",
    "    return np.array(distances)\n",
    "\n",
    "print(\"  - Calculating Euclidean-based flow-path distances...\")\n",
    "samples_gdf[\"hydro_dist_brick\"] = compute_distances_euclidean(samples_gdf, brickfields, dem_transform, dem_resolution)\n",
    "samples_gdf[\"hydro_dist_ind\"] = compute_distances_euclidean(samples_gdf, industries, dem_transform, dem_resolution)\n",
    "\n",
    "print(\"  - Starting LULC extraction loop...\")\n",
    "lulc_dir = \"LULCMerged\"\n",
    "years = [2017, 2018, 2019, 2020, 2021, 2022]\n",
    "for y in years:\n",
    "    lulc_path = os.path.join(lulc_dir, f\"LULC{y}.tif\")\n",
    "    print(f\"    - Processing LULC for year {y}...\")\n",
    "    try:\n",
    "        with rasterio.open(lulc_path) as lulc_src:\n",
    "            # Corrected line: convert GeoDataFrame geometry to a list of (x, y) tuples\n",
    "            lulc_values = [x[0] for x in lulc_src.sample([(p.x, p.y) for p in samples_gdf.geometry])]\n",
    "            samples_gdf[f\"LULC_{y}\"] = lulc_values\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"❌ Error processing {lulc_path}: {e}\")\n",
    "        # Add a placeholder column with NaNs if the file can't be read\n",
    "        samples_gdf[f\"LULC_{y}\"] = np.nan\n",
    "        continue\n",
    "print(\"  - LULC extraction loop complete.\")\n",
    "\n",
    "# --- UPDATED: Calculate LULC Change using Change Vector Analysis (CVA) ---\n",
    "print(\"  - Calculating LULC changes using Change Vector Analysis (CVA)...\")\n",
    "if \"LULC_2017\" in samples_gdf.columns and \"LULC_2022\" in samples_gdf.columns:\n",
    "    lulc_change_vector = samples_gdf[\"LULC_2022\"].astype(float) - samples_gdf[\"LULC_2017\"].astype(float)\n",
    "    samples_gdf[\"CVA_Magnitude\"] = np.abs(lulc_change_vector)\n",
    "    samples_gdf[\"CVA_Direction\"] = lulc_change_vector\n",
    "else:\n",
    "    print(\"❌ Error: LULC_2017 and LULC_2022 columns not found. Cannot perform CVA.\")\n",
    "print(\"Step 3 Complete.\\n\")\n",
    "\n",
    "# ===================== 4. Save Final Output ===================== #\n",
    "print(\"Step 4: Saving final output...\")\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "output_name = f\"Samples_{num_samples}\"\n",
    "samples_gdf.to_file(f\"data/{output_name}.shp\")\n",
    "samples_gdf.drop(columns=\"geometry\").to_csv(f\"data/{output_name}test.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Final dataset with {num_samples} sample(s) saved.\")\n",
    "\n",
    "# ===================== 5. Calculate and Print Mean LULC Changes ===================== #\n",
    "print(\"\\nStep 5: Calculating and printing mean LULC changes...\")\n",
    "cva_columns = [\"CVA_Magnitude\", \"CVA_Direction\"]\n",
    "for col in cva_columns:\n",
    "    if col in samples_gdf.columns:\n",
    "        mean_change = samples_gdf[col].mean()\n",
    "        print(f\"  - Mean value for '{col}': {mean_change:.4f}\")\n",
    "print(\"Step 5 Complete.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa28cee7-f609-4611-ae8a-67973bc7368e",
   "metadata": {},
   "source": [
    "# 100 Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdaede5e-094d-450c-9af6-122333609920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading 200 river points from '100Sampling.shp'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_3018/1708610200.py:25: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river_sample_gdf[\"geometry\"] = river_sample_gdf.geometry.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded 100 points from the shapefile.\n",
      "Step 1 Complete.\n",
      "\n",
      "Step 2: Interpolating initial features...\n",
      "Step 2 Complete.\n",
      "\n",
      "Step 3: Calculating hydrological and LULC features...\n",
      "  - Aligning rasters...\n",
      "  - Loading DEM and NDWI...\n",
      "  - Reading vector files (brickfields and industries)...\n",
      "  - Counting nearby brickfields and industries...\n",
      "  - Counting complete.\n",
      "  - Calculating Euclidean-based flow-path distances...\n",
      "  - Starting LULC extraction loop...\n",
      "    - Processing LULC for year 2017...\n",
      "    - Processing LULC for year 2018...\n",
      "    - Processing LULC for year 2019...\n",
      "    - Processing LULC for year 2020...\n",
      "    - Processing LULC for year 2021...\n",
      "    - Processing LULC for year 2022...\n",
      "  - LULC extraction loop complete.\n",
      "  - Calculating LULC changes using Change Vector Analysis (CVA)...\n",
      "Step 3 Complete.\n",
      "\n",
      "Step 4: Saving final output...\n",
      "✅ Final dataset with 100 sample(s) saved.\n",
      "\n",
      "Step 5: Calculating and printing mean LULC changes...\n",
      "  - Mean value for 'CVA_Magnitude': 0.0400\n",
      "  - Mean value for 'CVA_Direction': -0.0400\n",
      "Step 5 Complete.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_3018/1708610200.py:198: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  samples_gdf.to_file(f\"data/{output_name}.shp\")\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'num_brick_field' to 'num_brick_'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'num_industry' to 'num_indust'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'hydro_dist_brick' to 'hydro_dist'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'hydro_dist_ind' to 'hydro_di_1'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'CVA_Magnitude' to 'CVA_Magnit'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'CVA_Direction' to 'CVA_Direct'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from rasterio.features import shapes\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from shapely.geometry import shape, Point\n",
    "from scipy.spatial import cKDTree\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "# ===================== 1. Load 200 River Points from File ===================== #\n",
    "print(\"Step 1: Loading 200 river points from '100Sampling.shp'...\")\n",
    "sampling_file_path = \"100Sampling.shp\"\n",
    "try:\n",
    "    # Load the user's pre-created shapefile\n",
    "    river_sample_gdf = gpd.read_file(sampling_file_path)\n",
    "    num_samples = len(river_sample_gdf)\n",
    "    \n",
    "    # Ensure the GeoDataFrame has the correct CRS for further processing\n",
    "    if river_sample_gdf.crs != \"EPSG:4326\":\n",
    "        river_sample_gdf = river_sample_gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Ensure all geometries are points\n",
    "    river_sample_gdf[\"geometry\"] = river_sample_gdf.geometry.centroid\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file '{sampling_file_path}' was not found. Please ensure it is in the correct directory.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: Could not read '{sampling_file_path}'. Please check the file's integrity and format. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "river_coords = np.array([[p.x, p.y] for p in river_sample_gdf.geometry])\n",
    "print(f\"✅ Successfully loaded {num_samples} points from the shapefile.\")\n",
    "print(\"Step 1 Complete.\\n\")\n",
    "\n",
    "# ===================== 2. Interpolate Initial Features ===================== #\n",
    "print(\"Step 2: Interpolating initial features...\")\n",
    "try:\n",
    "    data = pd.read_csv(\"../data/RainySeason.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: RainySeason.csv not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "coords = data[['Long', 'Lat']].values\n",
    "features_to_interpolate = ['CrR', 'NiR', 'CuR', 'AsR', 'CdR', 'PbR', 'MR', 'SandR', 'SiltR', 'ClayR', 'FeR', \"RI\"]\n",
    "numeric_features = data[features_to_interpolate]\n",
    "\n",
    "def idw_interpolation(known_coords, known_values, query_coords, power=2):\n",
    "    tree = cKDTree(known_coords)\n",
    "    dists, idxs = tree.query(query_coords, k=4)\n",
    "    dists[dists == 0] = 1e-10\n",
    "    weights = 1 / (dists ** power)\n",
    "    weights /= weights.sum(axis=1)[:, None]\n",
    "    return np.sum(weights * known_values[idxs], axis=1)\n",
    "\n",
    "interpolated_features = np.zeros((len(river_coords), numeric_features.shape[1]))\n",
    "for i, col in enumerate(numeric_features.columns):\n",
    "    interpolated_features[:, i] = idw_interpolation(coords, numeric_features[col].values, river_coords)\n",
    "\n",
    "river_df = pd.DataFrame(interpolated_features, columns=numeric_features.columns)\n",
    "river_df['Long'] = river_coords[:, 0]\n",
    "river_df['Lat'] = river_coords[:, 1]\n",
    "river_df['Source'] = 'River_Interpolated'\n",
    "print(\"Step 2 Complete.\\n\")\n",
    "\n",
    "# ===================== 3. Calculate Hydrological and LULC Features (Optimized) ===================== #\n",
    "print(\"Step 3: Calculating hydrological and LULC features...\")\n",
    "dem_path = \"DEMF.tif\"\n",
    "ndwi_path = \"CalIndices/ndwi.tif\"\n",
    "aligned_ndwi_path = \"ndwi_aligned.tif\"\n",
    "\n",
    "print(\"  - Aligning rasters...\")\n",
    "def align_rasters(base_raster_path, match_raster_path, out_raster_path):\n",
    "    try:\n",
    "        with rasterio.open(base_raster_path) as base:\n",
    "            base_meta = base.meta.copy()\n",
    "            with rasterio.open(match_raster_path) as match:\n",
    "                data = match.read(1)\n",
    "                reprojected = np.empty((base.height, base.width), dtype=np.float32)\n",
    "                reproject(\n",
    "                    source=data,\n",
    "                    destination=reprojected,\n",
    "                    src_transform=match.transform,\n",
    "                    src_crs=match.crs,\n",
    "                    dst_transform=base.transform,\n",
    "                    dst_crs=base.crs,\n",
    "                    resampling=Resampling.bilinear\n",
    "                )\n",
    "                kwargs = base_meta\n",
    "                with rasterio.open(out_raster_path, 'w', **kwargs) as dst:\n",
    "                    dst.write(reprojected, 1)\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"❌ Error: Could not open raster files for alignment. Please check the file paths. Error: {e}\")\n",
    "        exit()\n",
    "    return out_raster_path\n",
    "\n",
    "align_rasters(dem_path, ndwi_path, aligned_ndwi_path)\n",
    "\n",
    "print(\"  - Loading DEM and NDWI...\")\n",
    "with rasterio.open(dem_path) as dem_src:\n",
    "    dem_transform = dem_src.transform\n",
    "    dem_crs = dem_src.crs\n",
    "    dem_resolution = dem_src.res[0]\n",
    "\n",
    "# Create samples_gdf from the loaded points\n",
    "samples_gdf = gpd.GeoDataFrame(river_df, geometry=gpd.points_from_xy(river_df.Long, river_df.Lat), crs=\"EPSG:4326\")\n",
    "samples_gdf = samples_gdf.to_crs(dem_crs)\n",
    "\n",
    "print(\"  - Reading vector files (brickfields and industries)...\")\n",
    "try:\n",
    "    brickfields_path = \"brick_field_point.shp\"\n",
    "    industries_path = \"industry_point.shp\"\n",
    "    brickfields = gpd.read_file(brickfields_path).to_crs(dem_crs)\n",
    "    industries = gpd.read_file(industries_path).to_crs(dem_crs)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: Could not read shapefiles. Please check the file paths and ensure they are valid. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "samples_gdf[\"geometry\"] = samples_gdf.geometry.centroid\n",
    "brickfields[\"geometry\"] = brickfields.geometry.centroid\n",
    "industries[\"geometry\"] = industries.geometry.centroid\n",
    "\n",
    "# --- ADDED: Calculate number of brickfields and industries within a buffer ---\n",
    "print(\"  - Counting nearby brickfields and industries...\")\n",
    "buffer_distance = 500 # 500 meters\n",
    "samples_gdf['num_brick_field'] = 0\n",
    "samples_gdf['num_industry'] = 0\n",
    "\n",
    "samples_buffered = samples_gdf.copy()\n",
    "samples_buffered['geometry'] = samples_buffered.geometry.buffer(buffer_distance)\n",
    "\n",
    "# Spatial join to count brickfields\n",
    "sjoin_brick = gpd.sjoin(samples_buffered, brickfields, how=\"left\", predicate=\"intersects\")\n",
    "brick_counts = sjoin_brick.groupby(sjoin_brick.index).size()\n",
    "samples_gdf['num_brick_field'] = samples_gdf.index.map(brick_counts).fillna(0).astype(int)\n",
    "\n",
    "# Spatial join to count industries\n",
    "sjoin_ind = gpd.sjoin(samples_buffered, industries, how=\"left\", predicate=\"intersects\")\n",
    "ind_counts = sjoin_ind.groupby(sjoin_ind.index).size()\n",
    "samples_gdf['num_industry'] = samples_gdf.index.map(ind_counts).fillna(0).astype(int)\n",
    "\n",
    "print(\"  - Counting complete.\")\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def world_to_pixel(transform, x, y):\n",
    "    col, row = ~transform * (x, y)\n",
    "    return int(row), int(col)\n",
    "\n",
    "def compute_distances_euclidean(points_gdf, targets_gdf, transform, resolution):\n",
    "    target_pixels = np.array([world_to_pixel(transform, x, y) for x, y in zip(targets_gdf.geometry.x, targets_gdf.geometry.y)])\n",
    "    tree = cKDTree(target_pixels)\n",
    "    distances = []\n",
    "    for px, py in zip(points_gdf.geometry.x, points_gdf.geometry.y):\n",
    "        start = world_to_pixel(transform, px, py)\n",
    "        dist_pixels, _ = tree.query(start)\n",
    "        dist_meters = dist_pixels * resolution\n",
    "        distances.append(dist_meters)\n",
    "    return np.array(distances)\n",
    "\n",
    "print(\"  - Calculating Euclidean-based flow-path distances...\")\n",
    "samples_gdf[\"hydro_dist_brick\"] = compute_distances_euclidean(samples_gdf, brickfields, dem_transform, dem_resolution)\n",
    "samples_gdf[\"hydro_dist_ind\"] = compute_distances_euclidean(samples_gdf, industries, dem_transform, dem_resolution)\n",
    "\n",
    "print(\"  - Starting LULC extraction loop...\")\n",
    "lulc_dir = \"LULCMerged\"\n",
    "years = [2017, 2018, 2019, 2020, 2021, 2022]\n",
    "for y in years:\n",
    "    lulc_path = os.path.join(lulc_dir, f\"LULC{y}.tif\")\n",
    "    print(f\"    - Processing LULC for year {y}...\")\n",
    "    try:\n",
    "        with rasterio.open(lulc_path) as lulc_src:\n",
    "            # Corrected line: convert GeoDataFrame geometry to a list of (x, y) tuples\n",
    "            lulc_values = [x[0] for x in lulc_src.sample([(p.x, p.y) for p in samples_gdf.geometry])]\n",
    "            samples_gdf[f\"LULC_{y}\"] = lulc_values\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"❌ Error processing {lulc_path}: {e}\")\n",
    "        # Add a placeholder column with NaNs if the file can't be read\n",
    "        samples_gdf[f\"LULC_{y}\"] = np.nan\n",
    "        continue\n",
    "print(\"  - LULC extraction loop complete.\")\n",
    "\n",
    "# --- UPDATED: Calculate LULC Change using Change Vector Analysis (CVA) ---\n",
    "print(\"  - Calculating LULC changes using Change Vector Analysis (CVA)...\")\n",
    "if \"LULC_2017\" in samples_gdf.columns and \"LULC_2022\" in samples_gdf.columns:\n",
    "    lulc_change_vector = samples_gdf[\"LULC_2022\"].astype(float) - samples_gdf[\"LULC_2017\"].astype(float)\n",
    "    samples_gdf[\"CVA_Magnitude\"] = np.abs(lulc_change_vector)\n",
    "    samples_gdf[\"CVA_Direction\"] = lulc_change_vector\n",
    "else:\n",
    "    print(\"❌ Error: LULC_2017 and LULC_2022 columns not found. Cannot perform CVA.\")\n",
    "print(\"Step 3 Complete.\\n\")\n",
    "\n",
    "# ===================== 4. Save Final Output ===================== #\n",
    "print(\"Step 4: Saving final output...\")\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "output_name = f\"Samples_{num_samples}\"\n",
    "samples_gdf.to_file(f\"data/{output_name}.shp\")\n",
    "samples_gdf = samples_gdf.round(2)\n",
    "samples_gdf.drop(columns=\"geometry\").to_csv(f\"data/{output_name}.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Final dataset with {num_samples} sample(s) saved.\")\n",
    "\n",
    "# ===================== 5. Calculate and Print Mean LULC Changes ===================== #\n",
    "print(\"\\nStep 5: Calculating and printing mean LULC changes...\")\n",
    "cva_columns = [\"CVA_Magnitude\", \"CVA_Direction\"]\n",
    "for col in cva_columns:\n",
    "    if col in samples_gdf.columns:\n",
    "        mean_change = samples_gdf[col].mean()\n",
    "        print(f\"  - Mean value for '{col}': {mean_change:.4f}\")\n",
    "print(\"Step 5 Complete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21f8bfe9-2cab-43b2-8a1d-9d7065cb9d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Calculating hydrological and LULC features...\n",
      "  - Aligning rasters...\n",
      "  - Loading DEM and NDWI...\n",
      "  - Reading vector files (brickfields and industries)...\n",
      "  - Counting nearby brickfields and industries...\n",
      "  - Counting complete.\n",
      "  - Calculating Euclidean-based flow-path distances...\n",
      "  - Starting LULC extraction loop...\n",
      "    - Processing LULC for year 2017...\n",
      "    - Processing LULC for year 2018...\n",
      "    - Processing LULC for year 2019...\n",
      "    - Processing LULC for year 2020...\n",
      "    - Processing LULC for year 2021...\n",
      "    - Processing LULC for year 2022...\n",
      "  - LULC extraction loop complete.\n",
      "  - Calculating LULC changes using a Transition Matrix...\n",
      "✅ LULC Transition Matrix:\n",
      "LULC 2022   1  4  7\n",
      "LULC 2017          \n",
      "1          97  0  0\n",
      "4           0  1  0\n",
      "5           1  0  0\n",
      "7           0  0  1\n",
      "Step 3 Complete.\n",
      "\n",
      "Step 4: Saving final output...\n",
      "✅ LULC transition matrix saved to 'data/LULC_Transition_Matrix.csv'.\n",
      "✅ Final dataset with 100 sample(s) saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_3420/3399716703.py:138: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  samples_gdf.to_file(f\"data/{output_name}.shp\")\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'num_brick_field' to 'num_brick_'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'num_industry' to 'num_indust'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'hydro_dist_brick' to 'hydro_dist'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'hydro_dist_ind' to 'hydro_di_1'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "# ===================== 3. Calculate Hydrological and LULC Features (Optimized) ===================== #\n",
    "print(\"Step 3: Calculating hydrological and LULC features...\")\n",
    "dem_path = \"DEMF.tif\"\n",
    "ndwi_path = \"CalIndices/ndwi.tif\"\n",
    "aligned_ndwi_path = \"ndwi_aligned.tif\"\n",
    "\n",
    "print(\"  - Aligning rasters...\")\n",
    "def align_rasters(base_raster_path, match_raster_path, out_raster_path):\n",
    "    try:\n",
    "        with rasterio.open(base_raster_path) as base:\n",
    "            base_meta = base.meta.copy()\n",
    "            with rasterio.open(match_raster_path) as match:\n",
    "                data = match.read(1)\n",
    "                reprojected = np.empty((base.height, base.width), dtype=np.float32)\n",
    "                reproject(\n",
    "                    source=data,\n",
    "                    destination=reprojected,\n",
    "                    src_transform=match.transform,\n",
    "                    src_crs=match.crs,\n",
    "                    dst_transform=base.transform,\n",
    "                    dst_crs=base.crs,\n",
    "                    resampling=Resampling.bilinear\n",
    "                )\n",
    "                kwargs = base_meta\n",
    "                with rasterio.open(out_raster_path, 'w', **kwargs) as dst:\n",
    "                    dst.write(reprojected, 1)\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"❌ Error: Could not open raster files for alignment. Please check the file paths. Error: {e}\")\n",
    "        exit()\n",
    "    return out_raster_path\n",
    "\n",
    "align_rasters(dem_path, ndwi_path, aligned_ndwi_path)\n",
    "\n",
    "print(\"  - Loading DEM and NDWI...\")\n",
    "with rasterio.open(dem_path) as dem_src:\n",
    "    dem_transform = dem_src.transform\n",
    "    dem_crs = dem_src.crs\n",
    "    dem_resolution = dem_src.res[0]\n",
    "\n",
    "# Create samples_gdf from the loaded points\n",
    "samples_gdf = gpd.GeoDataFrame(river_df, geometry=gpd.points_from_xy(river_df.Long, river_df.Lat), crs=\"EPSG:4326\")\n",
    "samples_gdf = samples_gdf.to_crs(dem_crs)\n",
    "\n",
    "print(\"  - Reading vector files (brickfields and industries)...\")\n",
    "try:\n",
    "    brickfields_path = \"brick_field_point.shp\"\n",
    "    industries_path = \"industry_point.shp\"\n",
    "    brickfields = gpd.read_file(brickfields_path).to_crs(dem_crs)\n",
    "    industries = gpd.read_file(industries_path).to_crs(dem_crs)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: Could not read shapefiles. Please check the file paths and ensure they are valid. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "samples_gdf[\"geometry\"] = samples_gdf.geometry.centroid\n",
    "brickfields[\"geometry\"] = brickfields.geometry.centroid\n",
    "industries[\"geometry\"] = industries.geometry.centroid\n",
    "\n",
    "# --- ADDED: Calculate number of brickfields and industries within a buffer ---\n",
    "print(\"  - Counting nearby brickfields and industries...\")\n",
    "buffer_distance = 1000 # 1000 meters\n",
    "samples_gdf['num_brick_field'] = 0\n",
    "samples_gdf['num_industry'] = 0\n",
    "\n",
    "samples_buffered = samples_gdf.copy()\n",
    "samples_buffered['geometry'] = samples_buffered.geometry.buffer(buffer_distance)\n",
    "\n",
    "# Spatial join to count brickfields\n",
    "sjoin_brick = gpd.sjoin(samples_buffered, brickfields, how=\"left\", predicate=\"intersects\")\n",
    "brick_counts = sjoin_brick.groupby(sjoin_brick.index).size()\n",
    "samples_gdf['num_brick_field'] = samples_gdf.index.map(brick_counts).fillna(0).astype(int)\n",
    "\n",
    "# Spatial join to count industries\n",
    "sjoin_ind = gpd.sjoin(samples_buffered, industries, how=\"left\", predicate=\"intersects\")\n",
    "ind_counts = sjoin_ind.groupby(sjoin_ind.index).size()\n",
    "samples_gdf['num_industry'] = samples_gdf.index.map(ind_counts).fillna(0).astype(int)\n",
    "\n",
    "print(\"  - Counting complete.\")\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def world_to_pixel(transform, x, y):\n",
    "    col, row = ~transform * (x, y)\n",
    "    return int(row), int(col)\n",
    "\n",
    "def compute_distances_euclidean(points_gdf, targets_gdf, transform, resolution):\n",
    "    target_pixels = np.array([world_to_pixel(transform, x, y) for x, y in zip(targets_gdf.geometry.x, targets_gdf.geometry.y)])\n",
    "    tree = cKDTree(target_pixels)\n",
    "    distances = []\n",
    "    for px, py in zip(points_gdf.geometry.x, points_gdf.geometry.y):\n",
    "        start = world_to_pixel(transform, px, py)\n",
    "        dist_pixels, _ = tree.query(start)\n",
    "        dist_meters = dist_pixels * resolution\n",
    "        distances.append(dist_meters)\n",
    "    return np.array(distances)\n",
    "\n",
    "print(\"  - Calculating Euclidean-based flow-path distances...\")\n",
    "samples_gdf[\"hydro_dist_brick\"] = compute_distances_euclidean(samples_gdf, brickfields, dem_transform, dem_resolution)\n",
    "samples_gdf[\"hydro_dist_ind\"] = compute_distances_euclidean(samples_gdf, industries, dem_transform, dem_resolution)\n",
    "\n",
    "print(\"  - Starting LULC extraction loop...\")\n",
    "lulc_dir = \"LULCMerged\"\n",
    "years = [2017, 2018, 2019, 2020, 2021, 2022]\n",
    "for y in years:\n",
    "    lulc_path = os.path.join(lulc_dir, f\"LULC{y}.tif\")\n",
    "    print(f\"    - Processing LULC for year {y}...\")\n",
    "    try:\n",
    "        with rasterio.open(lulc_path) as lulc_src:\n",
    "            # Corrected line: convert GeoDataFrame geometry to a list of (x, y) tuples\n",
    "            lulc_values = [x[0] for x in lulc_src.sample([(p.x, p.y) for p in samples_gdf.geometry])]\n",
    "            samples_gdf[f\"LULC_{y}\"] = lulc_values\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"❌ Error processing {lulc_path}: {e}\")\n",
    "        # Add a placeholder column with NaNs if the file can't be read\n",
    "        samples_gdf[f\"LULC_{y}\"] = np.nan\n",
    "        continue\n",
    "print(\"  - LULC extraction loop complete.\")\n",
    "\n",
    "\n",
    "# --- UPDATED: Calculate LULC Change using a Transition Matrix ---\n",
    "print(\"  - Calculating LULC changes using a Transition Matrix...\")\n",
    "if \"LULC_2017\" in samples_gdf.columns and \"LULC_2022\" in samples_gdf.columns:\n",
    "    # Create a transition matrix using pandas crosstab\n",
    "    lulc_transition_matrix = pd.crosstab(\n",
    "        samples_gdf[\"LULC_2017\"],\n",
    "        samples_gdf[\"LULC_2022\"],\n",
    "        rownames=['LULC 2017'],\n",
    "        colnames=['LULC 2022']\n",
    "    )\n",
    "    print(\"✅ LULC Transition Matrix:\")\n",
    "    print(lulc_transition_matrix)\n",
    "else:\n",
    "    print(\"❌ Error: LULC_2017 and LULC_2022 columns not found. Cannot perform Transition Matrix analysis.\")\n",
    "print(\"Step 3 Complete.\\n\")\n",
    "\n",
    "# ===================== 4. Save Final Output ===================== #\n",
    "print(\"Step 4: Saving final output...\")\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "output_name = f\"Samples_{num_samples}\"\n",
    "samples_gdf.to_file(f\"data/{output_name}.shp\")\n",
    "samples_gdf.drop(columns=\"geometry\").to_csv(f\"data/{output_name}1.csv\", index=False)\n",
    "\n",
    "# --- ADDED: Save the transition matrix to a separate CSV file ---\n",
    "if 'lulc_transition_matrix' in locals():\n",
    "    lulc_transition_matrix.to_csv(\"data/LULC_Transition_Matrix.csv\")\n",
    "    print(\"✅ LULC transition matrix saved to 'data/LULC_Transition_Matrix.csv'.\")\n",
    "else:\n",
    "    print(\"❌ LULC transition matrix was not created, so it cannot be saved.\")\n",
    "print(f\"✅ Final dataset with {num_samples} sample(s) saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59ce42-4a06-42fe-b14e-83261d74be0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06b3ede-d1ee-4e07-b41e-6298eb3e0fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd84ce3-3cd1-49f4-b413-53ab16f70a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7558716-561f-4c5d-8971-baaa2036b367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CrR</th>\n",
       "      <th>NiR</th>\n",
       "      <th>CuR</th>\n",
       "      <th>AsR</th>\n",
       "      <th>CdR</th>\n",
       "      <th>PbR</th>\n",
       "      <th>MR</th>\n",
       "      <th>SandR</th>\n",
       "      <th>SiltR</th>\n",
       "      <th>ClayR</th>\n",
       "      <th>...</th>\n",
       "      <th>hydro_dist_brick</th>\n",
       "      <th>hydro_dist_ind</th>\n",
       "      <th>LULC_2017</th>\n",
       "      <th>LULC_2018</th>\n",
       "      <th>LULC_2019</th>\n",
       "      <th>LULC_2020</th>\n",
       "      <th>LULC_2021</th>\n",
       "      <th>LULC_2022</th>\n",
       "      <th>CVA_Magnitude</th>\n",
       "      <th>CVA_Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.18</td>\n",
       "      <td>21.78</td>\n",
       "      <td>43.81</td>\n",
       "      <td>9.03</td>\n",
       "      <td>2.63</td>\n",
       "      <td>51.47</td>\n",
       "      <td>32.24</td>\n",
       "      <td>21.32</td>\n",
       "      <td>45.71</td>\n",
       "      <td>32.47</td>\n",
       "      <td>...</td>\n",
       "      <td>1622.59</td>\n",
       "      <td>2350.53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.05</td>\n",
       "      <td>21.19</td>\n",
       "      <td>43.03</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.64</td>\n",
       "      <td>51.08</td>\n",
       "      <td>32.25</td>\n",
       "      <td>21.37</td>\n",
       "      <td>45.32</td>\n",
       "      <td>32.90</td>\n",
       "      <td>...</td>\n",
       "      <td>770.25</td>\n",
       "      <td>1351.45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.61</td>\n",
       "      <td>20.39</td>\n",
       "      <td>41.98</td>\n",
       "      <td>8.53</td>\n",
       "      <td>2.65</td>\n",
       "      <td>50.63</td>\n",
       "      <td>32.29</td>\n",
       "      <td>21.52</td>\n",
       "      <td>44.66</td>\n",
       "      <td>33.55</td>\n",
       "      <td>...</td>\n",
       "      <td>1248.12</td>\n",
       "      <td>503.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.62</td>\n",
       "      <td>19.78</td>\n",
       "      <td>41.16</td>\n",
       "      <td>8.28</td>\n",
       "      <td>2.66</td>\n",
       "      <td>50.47</td>\n",
       "      <td>32.35</td>\n",
       "      <td>21.69</td>\n",
       "      <td>44.00</td>\n",
       "      <td>34.15</td>\n",
       "      <td>...</td>\n",
       "      <td>2003.28</td>\n",
       "      <td>1400.06</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.63</td>\n",
       "      <td>20.08</td>\n",
       "      <td>41.54</td>\n",
       "      <td>8.48</td>\n",
       "      <td>2.66</td>\n",
       "      <td>50.03</td>\n",
       "      <td>32.24</td>\n",
       "      <td>21.54</td>\n",
       "      <td>44.60</td>\n",
       "      <td>33.60</td>\n",
       "      <td>...</td>\n",
       "      <td>1662.69</td>\n",
       "      <td>1915.44</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     CrR    NiR    CuR   AsR   CdR    PbR     MR  SandR  SiltR  ClayR  ...  \\\n",
       "0  84.18  21.78  43.81  9.03  2.63  51.47  32.24  21.32  45.71  32.47  ...   \n",
       "1  86.05  21.19  43.03  8.82  2.64  51.08  32.25  21.37  45.32  32.90  ...   \n",
       "2  88.61  20.39  41.98  8.53  2.65  50.63  32.29  21.52  44.66  33.55  ...   \n",
       "3  90.62  19.78  41.16  8.28  2.66  50.47  32.35  21.69  44.00  34.15  ...   \n",
       "4  89.63  20.08  41.54  8.48  2.66  50.03  32.24  21.54  44.60  33.60  ...   \n",
       "\n",
       "   hydro_dist_brick  hydro_dist_ind  LULC_2017  LULC_2018 LULC_2019  \\\n",
       "0           1622.59         2350.53          1          1         1   \n",
       "1            770.25         1351.45          1          1         1   \n",
       "2           1248.12          503.12          1          1         1   \n",
       "3           2003.28         1400.06          1          1         1   \n",
       "4           1662.69         1915.44          7          7         7   \n",
       "\n",
       "   LULC_2020  LULC_2021  LULC_2022  CVA_Magnitude  CVA_Direction  \n",
       "0          1          1          1            0.0            0.0  \n",
       "1          1          1          1            0.0            0.0  \n",
       "2          1          1          1            0.0            0.0  \n",
       "3          1          1          1            0.0            0.0  \n",
       "4          7          7          7            0.0            0.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/Samples_100.csv\", index_col=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccd29c4b-d553-408c-8fca-ad9c245a3576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CrR</th>\n",
       "      <th>NiR</th>\n",
       "      <th>CuR</th>\n",
       "      <th>AsR</th>\n",
       "      <th>CdR</th>\n",
       "      <th>PbR</th>\n",
       "      <th>MR</th>\n",
       "      <th>SandR</th>\n",
       "      <th>SiltR</th>\n",
       "      <th>ClayR</th>\n",
       "      <th>...</th>\n",
       "      <th>hydro_dist_brick</th>\n",
       "      <th>hydro_dist_ind</th>\n",
       "      <th>LULC_2017</th>\n",
       "      <th>LULC_2018</th>\n",
       "      <th>LULC_2019</th>\n",
       "      <th>LULC_2020</th>\n",
       "      <th>LULC_2021</th>\n",
       "      <th>LULC_2022</th>\n",
       "      <th>CVA_Magnitude</th>\n",
       "      <th>CVA_Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>24.59</td>\n",
       "      <td>41.34</td>\n",
       "      <td>76.67</td>\n",
       "      <td>16.08</td>\n",
       "      <td>2.81</td>\n",
       "      <td>107.79</td>\n",
       "      <td>32.37</td>\n",
       "      <td>23.55</td>\n",
       "      <td>41.39</td>\n",
       "      <td>35.95</td>\n",
       "      <td>...</td>\n",
       "      <td>344.83</td>\n",
       "      <td>3440.29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>16.93</td>\n",
       "      <td>43.33</td>\n",
       "      <td>79.61</td>\n",
       "      <td>15.51</td>\n",
       "      <td>2.95</td>\n",
       "      <td>121.00</td>\n",
       "      <td>32.82</td>\n",
       "      <td>17.04</td>\n",
       "      <td>44.98</td>\n",
       "      <td>38.81</td>\n",
       "      <td>...</td>\n",
       "      <td>260.11</td>\n",
       "      <td>3112.60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>14.04</td>\n",
       "      <td>44.79</td>\n",
       "      <td>80.52</td>\n",
       "      <td>16.79</td>\n",
       "      <td>2.91</td>\n",
       "      <td>117.86</td>\n",
       "      <td>32.97</td>\n",
       "      <td>14.37</td>\n",
       "      <td>49.67</td>\n",
       "      <td>37.68</td>\n",
       "      <td>...</td>\n",
       "      <td>316.23</td>\n",
       "      <td>2152.32</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>19.89</td>\n",
       "      <td>43.18</td>\n",
       "      <td>78.25</td>\n",
       "      <td>17.24</td>\n",
       "      <td>2.78</td>\n",
       "      <td>107.00</td>\n",
       "      <td>32.67</td>\n",
       "      <td>19.43</td>\n",
       "      <td>47.15</td>\n",
       "      <td>35.23</td>\n",
       "      <td>...</td>\n",
       "      <td>411.17</td>\n",
       "      <td>1775.18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>14.40</td>\n",
       "      <td>41.37</td>\n",
       "      <td>83.48</td>\n",
       "      <td>13.05</td>\n",
       "      <td>3.26</td>\n",
       "      <td>145.71</td>\n",
       "      <td>32.64</td>\n",
       "      <td>17.71</td>\n",
       "      <td>37.40</td>\n",
       "      <td>44.80</td>\n",
       "      <td>...</td>\n",
       "      <td>1686.90</td>\n",
       "      <td>3080.91</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CrR    NiR    CuR    AsR   CdR     PbR     MR  SandR  SiltR  ClayR  ...  \\\n",
       "95  24.59  41.34  76.67  16.08  2.81  107.79  32.37  23.55  41.39  35.95  ...   \n",
       "96  16.93  43.33  79.61  15.51  2.95  121.00  32.82  17.04  44.98  38.81  ...   \n",
       "97  14.04  44.79  80.52  16.79  2.91  117.86  32.97  14.37  49.67  37.68  ...   \n",
       "98  19.89  43.18  78.25  17.24  2.78  107.00  32.67  19.43  47.15  35.23  ...   \n",
       "99  14.40  41.37  83.48  13.05  3.26  145.71  32.64  17.71  37.40  44.80  ...   \n",
       "\n",
       "    hydro_dist_brick  hydro_dist_ind  LULC_2017  LULC_2018 LULC_2019  \\\n",
       "95            344.83         3440.29          1          1         1   \n",
       "96            260.11         3112.60          1          1         1   \n",
       "97            316.23         2152.32          1          1         1   \n",
       "98            411.17         1775.18          1          1         1   \n",
       "99           1686.90         3080.91          4          5         5   \n",
       "\n",
       "    LULC_2020  LULC_2021  LULC_2022  CVA_Magnitude  CVA_Direction  \n",
       "95          1          1          1            0.0            0.0  \n",
       "96          1          1          1            0.0            0.0  \n",
       "97          1          1          1            0.0            0.0  \n",
       "98          1          1          1            0.0            0.0  \n",
       "99          4          5          4            0.0            0.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd433fa9-3d19-45b2-b48c-09097c2870cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading 200 river points from '100Sampling.shp'...\n",
      "✅ Successfully loaded 100 points from the shapefile.\n",
      "Step 1 Complete.\n",
      "\n",
      "Step 2: Interpolating initial features...\n",
      "Step 2 Complete.\n",
      "\n",
      "Step 3: Calculating hydrological and LULC features...\n",
      "    - Aligning rasters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_3225/1019984041.py:25: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river_sample_gdf[\"geometry\"] = river_sample_gdf.geometry.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - Loading DEM and NDWI...\n",
      "    - Reading vector files (brickfields and industries)...\n",
      "    - Counting nearby brickfields and industries...\n",
      "    - Counting complete.\n",
      "    - Calculating Euclidean-based flow-path distances...\n",
      "    - Starting LULC extraction loop...\n",
      "      - Processing LULC for year 2017...\n",
      "      - Processing LULC for year 2018...\n",
      "      - Processing LULC for year 2019...\n",
      "      - Processing LULC for year 2020...\n",
      "      - Processing LULC for year 2021...\n",
      "      - Processing LULC for year 2022...\n",
      "    - LULC extraction loop complete.\n",
      "    - Calculating LULC changes using Change Vector Analysis (CVA)...\n",
      "Step 3 Complete.\n",
      "\n",
      "Step 4: Saving final output...\n",
      "✅ Final dataset with 100 sample(s) saved.\n",
      "\n",
      "Step 5: Calculating and printing mean LULC changes...\n",
      "    - Mean value for 'CVA_Magnitude': 0.0400\n",
      "    - Mean value for 'CVA_Direction': -0.0400\n",
      "Step 5 Complete.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_3225/1019984041.py:199: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  samples_gdf.to_file(f\"data/{output_name}W.shp\")\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'num_brick_field' to 'num_brick_'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'num_industry' to 'num_indust'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'hydro_dist_brick' to 'hydro_dist'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'hydro_dist_ind' to 'hydro_di_1'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'CVA_Magnitude' to 'CVA_Magnit'\n",
      "  ogr_write(\n",
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'CVA_Direction' to 'CVA_Direct'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from rasterio.features import shapes\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from shapely.geometry import shape, Point\n",
    "from scipy.spatial import cKDTree\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "# ===================== 1. Load 200 River Points from File ===================== #\n",
    "print(\"Step 1: Loading 200 river points from '100Sampling.shp'...\")\n",
    "sampling_file_path = \"100Sampling.shp\"\n",
    "try:\n",
    "    # Load the user's pre-created shapefile\n",
    "    river_sample_gdf = gpd.read_file(sampling_file_path)\n",
    "    num_samples = len(river_sample_gdf)\n",
    "    \n",
    "    # Ensure the GeoDataFrame has the correct CRS for further processing\n",
    "    if river_sample_gdf.crs != \"EPSG:4326\":\n",
    "        river_sample_gdf = river_sample_gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Ensure all geometries are points\n",
    "    river_sample_gdf[\"geometry\"] = river_sample_gdf.geometry.centroid\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file '{sampling_file_path}' was not found. Please ensure it is in the correct directory.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: Could not read '{sampling_file_path}'. Please check the file's integrity and format. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "river_coords = np.array([[p.x, p.y] for p in river_sample_gdf.geometry])\n",
    "print(f\"✅ Successfully loaded {num_samples} points from the shapefile.\")\n",
    "print(\"Step 1 Complete.\\n\")\n",
    "\n",
    "# ===================== 2. Interpolate Initial Features ===================== #\n",
    "print(\"Step 2: Interpolating initial features...\")\n",
    "try:\n",
    "    # Fix: Specify the correct encoding for the CSV file\n",
    "    data = pd.read_csv(\"../data/WinterSeason1.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: RainySeason.csv not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "coords = data[['Long', 'Lat']].values\n",
    "features_to_interpolate = ['CrW', 'NiW', 'CuW', 'AsW', 'CdW', 'PbW', 'MW', 'SandW', 'SiltW', 'ClayW', 'FeW', \"RI\"]\n",
    "numeric_features = data[features_to_interpolate]\n",
    "\n",
    "def idw_interpolation(known_coords, known_values, query_coords, power=2):\n",
    "    tree = cKDTree(known_coords)\n",
    "    dists, idxs = tree.query(query_coords, k=4)\n",
    "    dists[dists == 0] = 1e-10\n",
    "    weights = 1 / (dists ** power)\n",
    "    weights /= weights.sum(axis=1)[:, None]\n",
    "    return np.sum(weights * known_values[idxs], axis=1)\n",
    "\n",
    "interpolated_features = np.zeros((len(river_coords), numeric_features.shape[1]))\n",
    "for i, col in enumerate(numeric_features.columns):\n",
    "    interpolated_features[:, i] = idw_interpolation(coords, numeric_features[col].values, river_coords)\n",
    "\n",
    "river_df = pd.DataFrame(interpolated_features, columns=numeric_features.columns)\n",
    "river_df['Long'] = river_coords[:, 0]\n",
    "river_df['Lat'] = river_coords[:, 1]\n",
    "river_df['Source'] = 'River_Interpolated'\n",
    "print(\"Step 2 Complete.\\n\")\n",
    "\n",
    "# ===================== 3. Calculate Hydrological and LULC Features (Optimized) ===================== #\n",
    "print(\"Step 3: Calculating hydrological and LULC features...\")\n",
    "dem_path = \"DEMF.tif\"\n",
    "ndwi_path = \"CalIndices/ndwi.tif\"\n",
    "aligned_ndwi_path = \"ndwi_aligned.tif\"\n",
    "\n",
    "print(\"    - Aligning rasters...\")\n",
    "def align_rasters(base_raster_path, match_raster_path, out_raster_path):\n",
    "    try:\n",
    "        with rasterio.open(base_raster_path) as base:\n",
    "            base_meta = base.meta.copy()\n",
    "            with rasterio.open(match_raster_path) as match:\n",
    "                data = match.read(1)\n",
    "                reprojected = np.empty((base.height, base.width), dtype=np.float32)\n",
    "                reproject(\n",
    "                    source=data,\n",
    "                    destination=reprojected,\n",
    "                    src_transform=match.transform,\n",
    "                    src_crs=match.crs,\n",
    "                    dst_transform=base.transform,\n",
    "                    dst_crs=base.crs,\n",
    "                    resampling=Resampling.bilinear\n",
    "                )\n",
    "                kwargs = base_meta\n",
    "                with rasterio.open(out_raster_path, 'w', **kwargs) as dst:\n",
    "                    dst.write(reprojected, 1)\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"❌ Error: Could not open raster files for alignment. Please check the file paths. Error: {e}\")\n",
    "        exit()\n",
    "    return out_raster_path\n",
    "\n",
    "align_rasters(dem_path, ndwi_path, aligned_ndwi_path)\n",
    "\n",
    "print(\"    - Loading DEM and NDWI...\")\n",
    "with rasterio.open(dem_path) as dem_src:\n",
    "    dem_transform = dem_src.transform\n",
    "    dem_crs = dem_src.crs\n",
    "    dem_resolution = dem_src.res[0]\n",
    "\n",
    "# Create samples_gdf from the loaded points\n",
    "samples_gdf = gpd.GeoDataFrame(river_df, geometry=gpd.points_from_xy(river_df.Long, river_df.Lat), crs=\"EPSG:4326\")\n",
    "samples_gdf = samples_gdf.to_crs(dem_crs)\n",
    "\n",
    "print(\"    - Reading vector files (brickfields and industries)...\")\n",
    "try:\n",
    "    brickfields_path = \"brick_field_point.shp\"\n",
    "    industries_path = \"industry_point.shp\"\n",
    "    brickfields = gpd.read_file(brickfields_path).to_crs(dem_crs)\n",
    "    industries = gpd.read_file(industries_path).to_crs(dem_crs)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: Could not read shapefiles. Please check the file paths and ensure they are valid. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "samples_gdf[\"geometry\"] = samples_gdf.geometry.centroid\n",
    "brickfields[\"geometry\"] = brickfields.geometry.centroid\n",
    "industries[\"geometry\"] = industries.geometry.centroid\n",
    "\n",
    "# --- ADDED: Calculate number of brickfields and industries within a buffer ---\n",
    "print(\"    - Counting nearby brickfields and industries...\")\n",
    "buffer_distance = 500 # 500 meters\n",
    "samples_gdf['num_brick_field'] = 0\n",
    "samples_gdf['num_industry'] = 0\n",
    "\n",
    "samples_buffered = samples_gdf.copy()\n",
    "samples_buffered['geometry'] = samples_buffered.geometry.buffer(buffer_distance)\n",
    "\n",
    "# Spatial join to count brickfields\n",
    "sjoin_brick = gpd.sjoin(samples_buffered, brickfields, how=\"left\", predicate=\"intersects\")\n",
    "brick_counts = sjoin_brick.groupby(sjoin_brick.index).size()\n",
    "samples_gdf['num_brick_field'] = samples_gdf.index.map(brick_counts).fillna(0).astype(int)\n",
    "\n",
    "# Spatial join to count industries\n",
    "sjoin_ind = gpd.sjoin(samples_buffered, industries, how=\"left\", predicate=\"intersects\")\n",
    "ind_counts = sjoin_ind.groupby(sjoin_ind.index).size()\n",
    "samples_gdf['num_industry'] = samples_gdf.index.map(ind_counts).fillna(0).astype(int)\n",
    "\n",
    "print(\"    - Counting complete.\")\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def world_to_pixel(transform, x, y):\n",
    "    col, row = ~transform * (x, y)\n",
    "    return int(row), int(col)\n",
    "\n",
    "def compute_distances_euclidean(points_gdf, targets_gdf, transform, resolution):\n",
    "    target_pixels = np.array([world_to_pixel(transform, x, y) for x, y in zip(targets_gdf.geometry.x, targets_gdf.geometry.y)])\n",
    "    tree = cKDTree(target_pixels)\n",
    "    distances = []\n",
    "    for px, py in zip(points_gdf.geometry.x, points_gdf.geometry.y):\n",
    "        start = world_to_pixel(transform, px, py)\n",
    "        dist_pixels, _ = tree.query(start)\n",
    "        dist_meters = dist_pixels * resolution\n",
    "        distances.append(dist_meters)\n",
    "    return np.array(distances)\n",
    "\n",
    "print(\"    - Calculating Euclidean-based flow-path distances...\")\n",
    "samples_gdf[\"hydro_dist_brick\"] = compute_distances_euclidean(samples_gdf, brickfields, dem_transform, dem_resolution)\n",
    "samples_gdf[\"hydro_dist_ind\"] = compute_distances_euclidean(samples_gdf, industries, dem_transform, dem_resolution)\n",
    "\n",
    "print(\"    - Starting LULC extraction loop...\")\n",
    "lulc_dir = \"LULCMerged\"\n",
    "years = [2017, 2018, 2019, 2020, 2021, 2022]\n",
    "for y in years:\n",
    "    lulc_path = os.path.join(lulc_dir, f\"LULC{y}.tif\")\n",
    "    print(f\"      - Processing LULC for year {y}...\")\n",
    "    try:\n",
    "        with rasterio.open(lulc_path) as lulc_src:\n",
    "            # Corrected line: convert GeoDataFrame geometry to a list of (x, y) tuples\n",
    "            lulc_values = [x[0] for x in lulc_src.sample([(p.x, p.y) for p in samples_gdf.geometry])]\n",
    "            samples_gdf[f\"LULC_{y}\"] = lulc_values\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"❌ Error processing {lulc_path}: {e}\")\n",
    "        # Add a placeholder column with NaNs if the file can't be read\n",
    "        samples_gdf[f\"LULC_{y}\"] = np.nan\n",
    "        continue\n",
    "print(\"    - LULC extraction loop complete.\")\n",
    "\n",
    "# --- UPDATED: Calculate LULC Change using Change Vector Analysis (CVA) ---\n",
    "print(\"    - Calculating LULC changes using Change Vector Analysis (CVA)...\")\n",
    "if \"LULC_2017\" in samples_gdf.columns and \"LULC_2022\" in samples_gdf.columns:\n",
    "    lulc_change_vector = samples_gdf[\"LULC_2022\"].astype(float) - samples_gdf[\"LULC_2017\"].astype(float)\n",
    "    samples_gdf[\"CVA_Magnitude\"] = np.abs(lulc_change_vector)\n",
    "    samples_gdf[\"CVA_Direction\"] = lulc_change_vector\n",
    "else:\n",
    "    print(\"❌ Error: LULC_2017 and LULC_2022 columns not found. Cannot perform CVA.\")\n",
    "print(\"Step 3 Complete.\\n\")\n",
    "\n",
    "# ===================== 4. Save Final Output ===================== #\n",
    "print(\"Step 4: Saving final output...\")\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "output_name = f\"Samples_{num_samples}W\"\n",
    "samples_gdf.to_file(f\"data/{output_name}W.shp\")\n",
    "samples_gdf = samples_gdf.round(2)\n",
    "samples_gdf.drop(columns=\"geometry\").to_csv(f\"data/{output_name}.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Final dataset with {num_samples} sample(s) saved.\")\n",
    "\n",
    "# ===================== 5. Calculate and Print Mean LULC Changes ===================== #\n",
    "print(\"\\nStep 5: Calculating and printing mean LULC changes...\")\n",
    "cva_columns = [\"CVA_Magnitude\", \"CVA_Direction\"]\n",
    "for col in cva_columns:\n",
    "    if col in samples_gdf.columns:\n",
    "        mean_change = samples_gdf[col].mean()\n",
    "        print(f\"    - Mean value for '{col}': {mean_change:.4f}\")\n",
    "print(\"Step 5 Complete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bedce7-e1e3-4ed2-87a1-38cfe2361008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
