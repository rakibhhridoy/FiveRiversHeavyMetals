{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20bda13-c04f-466c-81b8-edce76807b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout, Layer, MultiHeadAttention, LayerNormalization, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import sys\n",
    "from io import StringIO\n",
    "import pickle # Import the pickle library for saving objects\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# Train-test split\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "# We are not using rasters in this GNN-MLP model, but the paths are still\n",
    "# defined for consistency with previous versions.\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(\"Note: Raster data is not used in this GNN-MLP model.\")\n",
    "\n",
    "# ==================== 3. Create a Custom Data Generator ==================== #\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, mlp_data, gnn_data, y, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "        batch_y = self.y[batch_indices]\n",
    "        \n",
    "        return (batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #\n",
    "coords_train = train_combined[['Long','Lat']].values\n",
    "coords_test = test_orig[['Long','Lat']].values\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_test_train = distance_matrix(coords_test, coords_train)\n",
    "gnn_test = np.exp(-dist_mat_test_train/10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mlp_train = scaler.fit_transform(train_combined[numeric_cols])\n",
    "mlp_test = scaler.transform(test_orig[numeric_cols])\n",
    "y_train = train_combined['RI'].values\n",
    "y_test = test_orig['RI'].values\n",
    "\n",
    "# ==================== 5. Define GNN-MLP Fusion Model ==================== #\n",
    "def build_gnn_mlp_model(mlp_dim, gnn_dim):\n",
    "    # Inputs for all branches\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    \n",
    "    # --- MLP Branch ---\n",
    "    mlp_embedding = Dense(128, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(64, activation=\"relu\", name=\"mlp_embedding\")(mlp_embedding)\n",
    "\n",
    "    # --- GNN Branch ---\n",
    "    gnn_embedding = Dense(128, activation=\"relu\")(gnn_input)\n",
    "    gnn_embedding = Dense(64, activation=\"relu\", name=\"gnn_embedding\")(gnn_embedding)\n",
    "\n",
    "    # --- Concatenate Embeddings ---\n",
    "    combined = Concatenate()([mlp_embedding, gnn_embedding])\n",
    "    \n",
    "    # Final dense layers for prediction\n",
    "    f = Dense(128, activation=\"relu\")(combined)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[mlp_input, gnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, mlp_test, gnn_test_matrix, y_test, return_preds=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on given data and returns R², RMSE, and predictions.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict((mlp_test, gnn_test_matrix)).flatten()\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return r2, rmse\n",
    "\n",
    "def calculate_permutation_importance(model, mlp_data, gnn_data, y_true):\n",
    "    \"\"\"\n",
    "    Calculates permutation feature importance for the MLP and GNN branches.\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting Permutation Feature Importance Analysis...\")\n",
    "    # Get baseline R² on the unshuffled data\n",
    "    baseline_r2, _ = evaluate_model(model, mlp_data, gnn_data, y_true)\n",
    "    print(f\"Baseline R² on test set: {baseline_r2:.4f}\")\n",
    "\n",
    "    importance = {}\n",
    "    \n",
    "    # Permute MLP input\n",
    "    shuffled_mlp_data = mlp_data.copy()\n",
    "    np.random.shuffle(shuffled_mlp_data)\n",
    "    shuffled_r2, _ = evaluate_model(model, shuffled_mlp_data, gnn_data, y_true)\n",
    "    importance['MLP'] = baseline_r2 - shuffled_r2\n",
    "\n",
    "    # Permute GNN input\n",
    "    shuffled_gnn_data = gnn_data.copy()\n",
    "    np.random.shuffle(shuffled_gnn_data)\n",
    "    shuffled_r2, _ = evaluate_model(model, mlp_data, shuffled_gnn_data, y_true)\n",
    "    importance['GNN'] = baseline_r2 - shuffled_r2\n",
    "\n",
    "    return importance\n",
    "        \n",
    "# ==================== Run the Analysis ==================== #\n",
    "# Redirect output to a string for later saving\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = captured_output = StringIO()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing GNN-MLP Fusion Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "mlp_input_dim = mlp_train.shape[1]\n",
    "\n",
    "model = build_gnn_mlp_model(mlp_input_dim, gnn_input_dim)\n",
    "model.summary()\n",
    "\n",
    "# ==================== 6. Create Data Generators ==================== #\n",
    "train_generator = DataGenerator(\n",
    "    mlp_data=mlp_train, gnn_data=gnn_train, y=y_train,\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "# ==================== 7. Train Model ==================== #\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=train_generator\n",
    ")\n",
    "\n",
    "# ==================== 8. Evaluate & Perform Feature Importance ==================== #\n",
    "# Predict on the training data using the generator\n",
    "y_pred_train = model.predict(train_generator).flatten()\n",
    "r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))\n",
    "\n",
    "# Evaluate on the test data using the updated function\n",
    "r2_test, rmse_test = evaluate_model(model, mlp_test, gnn_test, y_test)\n",
    "y_pred_test = evaluate_model(model, mlp_test, gnn_test, y_test, return_preds=True)\n",
    "\n",
    "print(f\"\\n GNN-MLP Fusion Model Performance:\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# Calculate and print feature importance\n",
    "feature_importance = calculate_permutation_importance(model, mlp_test, gnn_test, y_test)\n",
    "print(\"\\n--- Feature Importance (Permutation) ---\")\n",
    "sorted_importance = sorted(feature_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, score in sorted_importance:\n",
    "    print(f\"{feature}: {score:.4f}\")\n",
    "\n",
    "# ==================== 9. Save all info to a folder ==================== #\n",
    "# Restore standard output\n",
    "sys.stdout = old_stdout\n",
    "printed_output = captured_output.getvalue()\n",
    "\n",
    "output_folder = \"gnn_mlp\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(f\"\\nCreating folder: '{output_folder}' and saving results...\")\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(output_folder, \"gnn_mlp_model.keras\")\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save the predictions and true labels\n",
    "np.save(os.path.join(output_folder, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(output_folder, \"y_test.npy\"), y_test)\n",
    "np.save(os.path.join(output_folder, \"y_pred_train.npy\"), y_pred_train)\n",
    "np.save(os.path.join(output_folder, \"y_pred_test.npy\"), y_pred_test)\n",
    "print(f\"Predictions and true labels saved as .npy files.\")\n",
    "\n",
    "# Save the printed output to a text file\n",
    "output_path = os.path.join(output_folder, \"analysis_output.txt\")\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(printed_output)\n",
    "print(f\"Analysis results saved to: {output_path}\")\n",
    "\n",
    "# Save the feature importance dictionary as a .pkl file\n",
    "importance_path = os.path.join(output_folder, \"feature_importance.pkl\")\n",
    "with open(importance_path, 'wb') as f:\n",
    "    pickle.dump(feature_importance, f)\n",
    "print(f\"Feature importance results saved to: {importance_path}\")\n",
    "\n",
    "print(\"\\nAll information successfully saved.\")\n",
    "\n",
    "# Garbage collect to free up memory now that everything is saved\n",
    "del model, history, train_generator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e094891-8cc6-4f81-af2d-21583ae7e370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50e14240-b709-4ff4-aa0d-e900371685b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Analyzing GNN-MLP-Raster Fusion Model with 5-Fold Cross-Validation\n",
      "Using a uniform patch size of 100 pixels for a 500m buffer.\n",
      "================================================================================\n",
      "\n",
      "--- Starting Fold 1/5 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ raster_input        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,520</span> │ raster_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ mlp_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,032</span> │ gnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33856</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ raster_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,166,848</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mlp_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ gnn_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ raster_embedding… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ raster_input        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m26\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m98\u001b[0m,    │      \u001b[38;5;34m7,520\u001b[0m │ raster_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m49\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m47\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ mlp_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m12,032\u001b[0m │ gnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33856\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ raster_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │  \u001b[38;5;34m2,166,848\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ mlp_embedding[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ gnn_embedding[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ raster_embedding… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m24,704\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,256,481</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,256,481\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,256,481</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,256,481\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 155860.8281 - val_loss: 14426.4189\n",
      "Epoch 2/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 101962.0391 - val_loss: 3614.8186\n",
      "Epoch 3/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 37470.3086 - val_loss: 4155.1509\n",
      "Epoch 4/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 7083.7373 - val_loss: 2953.2900\n",
      "Epoch 5/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 5079.8433 - val_loss: 2475.6829\n",
      "Epoch 6/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 3330.6868 - val_loss: 2208.4880\n",
      "Epoch 7/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 4411.3809 - val_loss: 2980.4490\n",
      "Epoch 8/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 3998.7439 - val_loss: 1424.3468\n",
      "Epoch 9/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 2043.4912 - val_loss: 1370.5883\n",
      "Epoch 10/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 2501.2427 - val_loss: 1551.1763\n",
      "Epoch 11/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 2751.2683 - val_loss: 1278.0474\n",
      "Epoch 12/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 2066.7239 - val_loss: 1875.8248\n",
      "Epoch 13/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 3058.5999 - val_loss: 768.2339\n",
      "Epoch 14/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 1517.1156 - val_loss: 1223.0161\n",
      "Epoch 15/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 1641.6008 - val_loss: 1891.1133\n",
      "Epoch 16/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 1904.9873 - val_loss: 1348.3179\n",
      "Epoch 17/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 1821.3950 - val_loss: 568.5452\n",
      "Epoch 18/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 2047.7509 - val_loss: 2029.5039\n",
      "Epoch 19/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 1689.1877 - val_loss: 2258.9629\n",
      "Epoch 20/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 1986.2687 - val_loss: 1381.3824\n",
      "Epoch 21/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 1454.7866 - val_loss: 1563.5121\n",
      "Epoch 22/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1539.5540 - val_loss: 640.7040\n",
      "Epoch 23/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 2190.1926 - val_loss: 1908.8324\n",
      "Epoch 24/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 1451.6429 - val_loss: 2441.5188\n",
      "Epoch 25/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1277.1399 - val_loss: 1265.2871\n",
      "Epoch 26/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 1592.8337 - val_loss: 574.8486\n",
      "Epoch 27/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 1296.2378 - val_loss: 2757.2852\n",
      "Fold 1 Test Metrics:\n",
      "R²: 0.8894 | RMSE: 23.8442 | MAE: 18.1025 | SMAPE: 10.8519%\n",
      "\n",
      "Starting Permutation Feature Importance Analysis...\n",
      "Baseline R²: 0.8894\n",
      "Permuting MLP features...\n",
      "Permuting GNN features...\n",
      "Permuting Raster features...\n",
      "\n",
      "--- Starting Fold 2/5 ---\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 898850.8125 - val_loss: 100962.8047\n",
      "Epoch 2/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 76930.0781 - val_loss: 26016.9219\n",
      "Epoch 3/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 13245.5635 - val_loss: 10595.9238\n",
      "Epoch 4/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 10291.3584 - val_loss: 9867.9541\n",
      "Epoch 5/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 24398.7715 - val_loss: 5731.5601\n",
      "Epoch 6/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 6266.1401 - val_loss: 6293.6641\n",
      "Epoch 7/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 5192.0127 - val_loss: 5549.7563\n",
      "Epoch 8/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 11044.3740 - val_loss: 4162.9653\n",
      "Epoch 9/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 5134.1748 - val_loss: 3976.4036\n",
      "Epoch 10/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 5247.8423 - val_loss: 3744.6992\n",
      "Epoch 11/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 3933.8311 - val_loss: 3362.5637\n",
      "Epoch 12/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 9122.5977 - val_loss: 3610.5896\n",
      "Epoch 13/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 3382.9373 - val_loss: 4103.4844\n",
      "Epoch 14/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 2284.7769 - val_loss: 4810.2300\n",
      "Epoch 15/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 2228.7910 - val_loss: 3709.4309\n",
      "Epoch 16/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 2785.9180 - val_loss: 3986.9739\n",
      "Epoch 17/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 4195.9966 - val_loss: 2922.8242\n",
      "Epoch 18/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 1341.5100 - val_loss: 2750.9111\n",
      "Epoch 19/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 2146.5144 - val_loss: 2991.1531\n",
      "Epoch 20/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 1874.8706 - val_loss: 2409.8762\n",
      "Epoch 21/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 2314.7791 - val_loss: 1731.3961\n",
      "Epoch 22/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 1867.6777 - val_loss: 1901.4907\n",
      "Epoch 23/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 1550.1379 - val_loss: 1902.4164\n",
      "Epoch 24/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 1458.3658 - val_loss: 2169.6140\n",
      "Epoch 25/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 1894.1309 - val_loss: 2205.6130\n",
      "Epoch 26/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 1658.1292 - val_loss: 1885.1777\n",
      "Epoch 27/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 1955.3555 - val_loss: 2280.6504\n",
      "Epoch 28/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 1204.6937 - val_loss: 2207.4250\n",
      "Epoch 29/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 1652.4037 - val_loss: 1718.0156\n",
      "Epoch 30/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 933.2133 - val_loss: 2312.7566\n",
      "Epoch 31/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 992.0715 - val_loss: 2189.8025\n",
      "Epoch 32/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 1344.6492 - val_loss: 2327.6096\n",
      "Epoch 33/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 1001.7149 - val_loss: 1528.8895\n",
      "Epoch 34/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 1478.2952 - val_loss: 1655.1328\n",
      "Epoch 35/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 1210.4269 - val_loss: 999.8386\n",
      "Epoch 36/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 1451.0820 - val_loss: 1577.4938\n",
      "Epoch 37/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 1085.7001 - val_loss: 1100.3273\n",
      "Epoch 38/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 1164.2028 - val_loss: 981.9919\n",
      "Epoch 39/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 1056.8911 - val_loss: 1063.7144\n",
      "Epoch 40/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 1041.0302 - val_loss: 1039.4720\n",
      "Epoch 41/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 904.6397 - val_loss: 1031.2198\n",
      "Epoch 42/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 1256.2764 - val_loss: 983.0106\n",
      "Epoch 43/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 1399.4137 - val_loss: 1615.1779\n",
      "Epoch 44/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 923.5389 - val_loss: 2782.1282\n",
      "Epoch 45/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 1288.0043 - val_loss: 1039.5231\n",
      "Epoch 46/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 1228.7817 - val_loss: 978.7167\n",
      "Epoch 47/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 891.7226 - val_loss: 1356.3966\n",
      "Epoch 48/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 624.1185 - val_loss: 1066.7725\n",
      "Epoch 49/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 1296.3284 - val_loss: 936.3133\n",
      "Epoch 50/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 626.1991 - val_loss: 1110.0663\n",
      "Epoch 51/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 829.9431 - val_loss: 1331.2292\n",
      "Epoch 52/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 768.8628 - val_loss: 759.0628\n",
      "Epoch 53/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 689.0873 - val_loss: 960.7764\n",
      "Epoch 54/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 812.9826 - val_loss: 789.2448\n",
      "Epoch 55/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 869.6300 - val_loss: 632.2260\n",
      "Epoch 56/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 1012.7044 - val_loss: 836.8369\n",
      "Epoch 57/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 1198.7764 - val_loss: 1652.6572\n",
      "Epoch 58/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 625.6627 - val_loss: 1896.2861\n",
      "Epoch 59/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 535.9835 - val_loss: 1823.8112\n",
      "Epoch 60/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 373.2879 - val_loss: 2205.8884\n",
      "Epoch 61/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 977.5141 - val_loss: 1556.7637\n",
      "Epoch 62/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 591.6830 - val_loss: 1576.8066\n",
      "Epoch 63/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 720.4304 - val_loss: 725.4816\n",
      "Epoch 64/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 297.8492 - val_loss: 699.0314\n",
      "Epoch 65/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 651.8356 - val_loss: 905.3008\n",
      "Fold 2 Test Metrics:\n",
      "R²: 0.8739 | RMSE: 25.1441 | MAE: 17.6808 | SMAPE: 9.5843%\n",
      "\n",
      "Starting Permutation Feature Importance Analysis...\n",
      "Baseline R²: 0.8739\n",
      "Permuting MLP features...\n",
      "Permuting GNN features...\n",
      "Permuting Raster features...\n",
      "\n",
      "--- Starting Fold 3/5 ---\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 63941.8008 - val_loss: 31298.9121\n",
      "Epoch 2/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 75460.6484 - val_loss: 5819.8115\n",
      "Epoch 3/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 17700.4863 - val_loss: 6886.2515\n",
      "Epoch 4/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 16747.4512 - val_loss: 2777.5063\n",
      "Epoch 5/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 7624.4854 - val_loss: 2883.0220\n",
      "Epoch 6/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 5295.5073 - val_loss: 2517.1250\n",
      "Epoch 7/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 3174.8770 - val_loss: 970.1010\n",
      "Epoch 8/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 3553.7744 - val_loss: 1243.1707\n",
      "Epoch 9/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 2942.3181 - val_loss: 749.7491\n",
      "Epoch 10/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 1903.3276 - val_loss: 491.7584\n",
      "Epoch 11/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 2592.4685 - val_loss: 619.4930\n",
      "Epoch 12/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 1966.6880 - val_loss: 691.2559\n",
      "Epoch 13/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 2500.1609 - val_loss: 694.3989\n",
      "Epoch 14/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 3241.6975 - val_loss: 1611.3298\n",
      "Epoch 15/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - loss: 2485.7888 - val_loss: 534.9032\n",
      "Epoch 16/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 1927.1077 - val_loss: 553.8541\n",
      "Epoch 17/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 1040.3608 - val_loss: 522.7350\n",
      "Epoch 18/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 2509.2683 - val_loss: 1398.6788\n",
      "Epoch 19/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 2366.7114 - val_loss: 625.6306\n",
      "Epoch 20/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 1812.6396 - val_loss: 1017.8156\n",
      "Fold 3 Test Metrics:\n",
      "R²: 0.8309 | RMSE: 22.1756 | MAE: 18.0275 | SMAPE: 12.3719%\n",
      "\n",
      "Starting Permutation Feature Importance Analysis...\n",
      "Baseline R²: 0.8486\n",
      "Permuting MLP features...\n",
      "Permuting GNN features...\n",
      "Permuting Raster features...\n",
      "\n",
      "--- Starting Fold 4/5 ---\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - loss: 169258.0625 - val_loss: 59306.2578\n",
      "Epoch 2/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 259168.7500 - val_loss: 17853.1504\n",
      "Epoch 3/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 30459.6953 - val_loss: 4875.4497\n",
      "Epoch 4/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 14287.8984 - val_loss: 3428.8110\n",
      "Epoch 5/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 3365.4368 - val_loss: 5042.1865\n",
      "Epoch 6/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 3339.4985 - val_loss: 3768.7290\n",
      "Epoch 7/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 4315.3833 - val_loss: 2215.3386\n",
      "Epoch 8/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 2168.8752 - val_loss: 1417.0370\n",
      "Epoch 9/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 5751.2891 - val_loss: 3245.9644\n",
      "Epoch 10/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 6908.4878 - val_loss: 1859.5905\n",
      "Epoch 11/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 2144.3784 - val_loss: 1789.0905\n",
      "Epoch 12/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 4307.2690 - val_loss: 1192.4410\n",
      "Epoch 13/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 1959.2667 - val_loss: 1590.1343\n",
      "Epoch 14/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 2299.7783 - val_loss: 1047.5970\n",
      "Epoch 15/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 1604.2383 - val_loss: 680.6624\n",
      "Epoch 16/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 1666.4227 - val_loss: 739.2390\n",
      "Epoch 17/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 1831.1354 - val_loss: 632.3118\n",
      "Epoch 18/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 1845.0676 - val_loss: 1165.8591\n",
      "Epoch 19/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 2623.5745 - val_loss: 456.7268\n",
      "Epoch 20/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 865.1226 - val_loss: 1180.1263\n",
      "Epoch 21/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 1562.5564 - val_loss: 1103.3447\n",
      "Epoch 22/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 961.8481 - val_loss: 1035.1300\n",
      "Epoch 23/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 1200.3837 - val_loss: 798.6820\n",
      "Epoch 24/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 1284.3262 - val_loss: 1516.2156\n",
      "Epoch 25/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 1113.1459 - val_loss: 416.9909\n",
      "Epoch 26/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 1013.1179 - val_loss: 552.5344\n",
      "Epoch 27/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 973.3244 - val_loss: 562.2733\n",
      "Epoch 28/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 1176.4635 - val_loss: 365.9141\n",
      "Epoch 29/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 808.8126 - val_loss: 377.9682\n",
      "Epoch 30/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 1712.6217 - val_loss: 602.6394\n",
      "Epoch 31/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 1014.0079 - val_loss: 410.2581\n",
      "Epoch 32/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 1418.8005 - val_loss: 768.5825\n",
      "Epoch 33/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 1150.2622 - val_loss: 1525.5310\n",
      "Epoch 34/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 1391.4019 - val_loss: 341.8595\n",
      "Epoch 35/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - loss: 893.5878 - val_loss: 382.5794\n",
      "Epoch 36/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 893.5231 - val_loss: 1011.4792\n",
      "Epoch 37/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 989.0040 - val_loss: 522.2158\n",
      "Epoch 38/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 1058.9071 - val_loss: 1127.7183\n",
      "Epoch 39/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 894.2440 - val_loss: 339.6374\n",
      "Epoch 40/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - loss: 1563.1643 - val_loss: 1173.2063\n",
      "Epoch 41/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 960.2133 - val_loss: 814.2413\n",
      "Epoch 42/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 938.6692 - val_loss: 551.4003\n",
      "Epoch 43/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 826.6472 - val_loss: 1264.4282\n",
      "Epoch 44/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 588.5123 - val_loss: 318.0999\n",
      "Epoch 45/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 1047.2673 - val_loss: 1254.7142\n",
      "Epoch 46/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 1265.2770 - val_loss: 391.4153\n",
      "Epoch 47/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 896.9360 - val_loss: 486.7061\n",
      "Epoch 48/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - loss: 698.9099 - val_loss: 959.6455\n",
      "Epoch 49/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 472.9431 - val_loss: 1442.7506\n",
      "Epoch 50/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 762.6199 - val_loss: 1269.0466\n",
      "Epoch 51/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 449.7401 - val_loss: 1085.2877\n",
      "Epoch 52/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 911.6893 - val_loss: 1310.4326\n",
      "Epoch 53/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 905.8477 - val_loss: 1929.4060\n",
      "Epoch 54/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 967.9574 - val_loss: 1937.9961\n",
      "Fold 4 Test Metrics:\n",
      "R²: 0.9434 | RMSE: 17.8354 | MAE: 14.2673 | SMAPE: 8.1390%\n",
      "\n",
      "Starting Permutation Feature Importance Analysis...\n",
      "Baseline R²: 0.9226\n",
      "Permuting MLP features...\n",
      "Permuting GNN features...\n",
      "Permuting Raster features...\n",
      "\n",
      "--- Starting Fold 5/5 ---\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - loss: 395060.3750 - val_loss: 25604.2148\n",
      "Epoch 2/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 63321.7031 - val_loss: 3526.9565\n",
      "Epoch 3/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 17724.5312 - val_loss: 5470.0259\n",
      "Epoch 4/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 11514.1846 - val_loss: 2879.8716\n",
      "Epoch 5/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 6557.7617 - val_loss: 1100.4091\n",
      "Epoch 6/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 5382.2637 - val_loss: 900.7040\n",
      "Epoch 7/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 4155.2280 - val_loss: 1472.6522\n",
      "Epoch 8/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 4155.8301 - val_loss: 1298.4431\n",
      "Epoch 9/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 4937.6333 - val_loss: 1080.9750\n",
      "Epoch 10/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 3424.6682 - val_loss: 1209.7076\n",
      "Epoch 11/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - loss: 2864.4719 - val_loss: 3355.1399\n",
      "Epoch 12/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 4493.6274 - val_loss: 2886.6406\n",
      "Epoch 13/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 2572.7551 - val_loss: 718.4894\n",
      "Epoch 14/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 2409.0344 - val_loss: 458.0336\n",
      "Epoch 15/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 2462.5151 - val_loss: 484.1688\n",
      "Epoch 16/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 2903.3030 - val_loss: 484.1261\n",
      "Epoch 17/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 2056.1101 - val_loss: 977.0033\n",
      "Epoch 18/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 2935.0586 - val_loss: 1214.5403\n",
      "Epoch 19/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - loss: 1651.7040 - val_loss: 874.0253\n",
      "Epoch 20/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 1725.6595 - val_loss: 463.4902\n",
      "Epoch 21/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 1711.0449 - val_loss: 757.8050\n",
      "Epoch 22/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 3058.3486 - val_loss: 454.0732\n",
      "Epoch 23/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - loss: 1779.3135 - val_loss: 1004.5631\n",
      "Epoch 24/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 1877.2810 - val_loss: 2680.4495\n",
      "Epoch 25/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - loss: 3088.3840 - val_loss: 1525.4293\n",
      "Epoch 26/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 1171.5703 - val_loss: 362.5483\n",
      "Epoch 27/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - loss: 1633.5634 - val_loss: 276.6345\n",
      "Epoch 28/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - loss: 1852.0280 - val_loss: 790.3408\n",
      "Epoch 29/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 979.3342 - val_loss: 820.3263\n",
      "Epoch 30/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - loss: 1690.7487 - val_loss: 2651.9067\n",
      "Epoch 31/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 1161.7230 - val_loss: 1025.1047\n",
      "Epoch 32/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 1242.2192 - val_loss: 307.4673\n",
      "Epoch 33/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 1292.0325 - val_loss: 276.1109\n",
      "Epoch 34/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 1516.4448 - val_loss: 385.3080\n",
      "Epoch 35/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 1165.1373 - val_loss: 327.7365\n",
      "Epoch 36/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 1269.7048 - val_loss: 392.0808\n",
      "Epoch 37/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 1130.0245 - val_loss: 1010.6744\n",
      "Epoch 38/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 1351.2717 - val_loss: 847.4997\n",
      "Epoch 39/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 1046.8344 - val_loss: 572.2789\n",
      "Epoch 40/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 887.8787 - val_loss: 897.9020\n",
      "Epoch 41/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 933.6884 - val_loss: 1024.8037\n",
      "Epoch 42/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 898.1900 - val_loss: 257.7449\n",
      "Epoch 43/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 1154.0844 - val_loss: 678.9757\n",
      "Epoch 44/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 1850.2939 - val_loss: 560.1223\n",
      "Epoch 45/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 956.3306 - val_loss: 1019.0946\n",
      "Epoch 46/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 1100.1156 - val_loss: 1139.1985\n",
      "Epoch 47/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1036.6329 - val_loss: 961.3912\n",
      "Epoch 48/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 1597.5609 - val_loss: 1068.6315\n",
      "Epoch 49/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 1060.6429 - val_loss: 1491.8130\n",
      "Epoch 50/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 840.6865 - val_loss: 2621.0244\n",
      "Epoch 51/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 1043.1646 - val_loss: 2455.6719\n",
      "Epoch 52/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 1107.4586 - val_loss: 1400.9358\n",
      "Fold 5 Test Metrics:\n",
      "R²: 0.9502 | RMSE: 16.0544 | MAE: 13.9315 | SMAPE: 8.1792%\n",
      "\n",
      "Starting Permutation Feature Importance Analysis...\n",
      "Baseline R²: 0.9448\n",
      "Permuting MLP features...\n",
      "Permuting GNN features...\n",
      "Permuting Raster features...\n",
      "\n",
      "================================================================================\n",
      "Final Cross-Validation Results (Averaged over 5 folds):\n",
      "================================================================================\n",
      "Average R²: 0.8976\n",
      "Average RMSE: 21.0107\n",
      "Average MAE: 16.4019\n",
      "Average SMAPE: 9.8253%\n",
      "\n",
      "--- Average Feature Importance (Permutation) ---\n",
      "Raster_SiltR.tif: 812.2796\n",
      "Raster_SandR.tif: 672.0498\n",
      "Raster_CdR.tif: 495.8186\n",
      "Raster_AsR.tif: 475.2093\n",
      "Raster_ClayR.tif: 453.6989\n",
      "Raster_CuR.tif: 426.5892\n",
      "Raster_NiR.tif: 403.4518\n",
      "Raster_CrR.tif: 320.2871\n",
      "Raster_Pb_R.tif: 290.3707\n",
      "MLP_ClayW: 0.0043\n",
      "MLP_PbW: 0.0040\n",
      "MLP_FeW: 0.0037\n",
      "MLP_NiW: 0.0036\n",
      "MLP_SiltW: 0.0023\n",
      "MLP_CdW: 0.0010\n",
      "MLP_MW: 0.0007\n",
      "MLP_CuW: 0.0004\n",
      "MLP_AsW: 0.0002\n",
      "MLP_hydro_dist_brick: 0.0001\n",
      "MLP_CrW: 0.0001\n",
      "MLP_hydro_dist_ind: 0.0001\n",
      "MLP_SandW: 0.0001\n",
      "Raster_bui.tif: 0.0000\n",
      "Raster_ndsi.tif: 0.0000\n",
      "Raster_savi.tif: 0.0000\n",
      "Raster_ndbsi.tif: 0.0000\n",
      "Raster_ui.tif: 0.0000\n",
      "Raster_ndwi.tif: 0.0000\n",
      "Raster_ndbi.tif: 0.0000\n",
      "Raster_awei.tif: 0.0000\n",
      "Raster_evi.tif: 0.0000\n",
      "Raster_mndwi.tif: 0.0000\n",
      "Raster_ndvi.tif: 0.0000\n",
      "Raster_LULC2020.tif: 0.0000\n",
      "Raster_LULC2021.tif: 0.0000\n",
      "Raster_LULC2022.tif: 0.0000\n",
      "Raster_LULC2019.tif: 0.0000\n",
      "Raster_LULC2018.tif: 0.0000\n",
      "Raster_LULC2017.tif: 0.0000\n",
      "GNN: -0.0000\n",
      "MLP_num_industry: -0.0008\n",
      "MLP_num_brick_field: -0.0013\n",
      "\n",
      "Analysis complete. Results are printed above.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout, Layer, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import sys\n",
    "import pickle # Import the pickle library for saving objects\n",
    "\n",
    "# Set a consistent seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "# NOTE: This script assumes the following file paths are correct.\n",
    "try:\n",
    "    orig = pd.read_csv(\"../../data/WinterSeason1.csv\")\n",
    "    river_100 = pd.read_csv(\"../data/Samples_100W.csv\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Required data file not found. Please check your file paths.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters and Metadata ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "# Get the pixel resolution from the first raster to set a uniform patch size\n",
    "try:\n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        pixel_size = src.transform.a\n",
    "except IndexError:\n",
    "    print(\"Error: No raster files found in the specified directories.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Create a dictionary to store raster metadata for fast access\n",
    "raster_metadata = {}\n",
    "for path in raster_paths:\n",
    "    with rasterio.open(path) as src:\n",
    "        raster_metadata[path] = {\n",
    "            'transform': src.transform,\n",
    "            'crs': src.crs,\n",
    "            'width': src.width,\n",
    "            'height': src.height\n",
    "        }\n",
    "\n",
    "# ==================== 3. Define a Custom Data Generator ==================== #\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Custom Keras Sequence for generating batches of data.\n",
    "    Handles three different input types: MLP features, GNN features,\n",
    "    and raster image patches, loading rasters on-the-fly to save memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, mlp_data, gnn_data, y, coords, raster_paths, buffer_radius_m, pixel_size, batch_size=4, shuffle=True):\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.coords = coords\n",
    "        self.raster_paths = raster_paths\n",
    "        # Calculate the uniform patch size in pixels based on the buffer radius and pixel size\n",
    "        # We need a square patch, so the size is 2 * radius / pixel_size\n",
    "        self.patch_size = int(round((2 * buffer_radius_m) / pixel_size))\n",
    "        # Ensure patch size is at least 1 and is an even number for easy centering\n",
    "        if self.patch_size % 2 != 0:\n",
    "            self.patch_size += 1\n",
    "        self.patch_size = max(self.patch_size, 2)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def get_raster_patches(self, coords_batch):\n",
    "        \"\"\"\n",
    "        Extracts a patch of raster data for each coordinate in the batch.\n",
    "        Loads rasters on-the-fly to save memory and robustly handles boundaries.\n",
    "        \"\"\"\n",
    "        patches_for_rasters = []\n",
    "        for path in self.raster_paths:\n",
    "            patches_for_this_raster = []\n",
    "            try:\n",
    "                with rasterio.open(path) as src:\n",
    "                    for lon, lat in coords_batch:\n",
    "                        # Get pixel coordinates\n",
    "                        row, col = src.index(lon, lat)\n",
    "                        \n",
    "                        # Define a window to read around the pixel, handling boundaries\n",
    "                        half_patch = self.patch_size // 2\n",
    "                        left = int(col - half_patch)\n",
    "                        top = int(row - half_patch)\n",
    "                        right = int(col + half_patch)\n",
    "                        bottom = int(row + half_patch)\n",
    "\n",
    "                        # Create a new, empty array for the final padded patch\n",
    "                        padded_patch = np.zeros((self.patch_size, self.patch_size), dtype='float32')\n",
    "\n",
    "                        # Calculate the window in the raster's coordinate space to read from\n",
    "                        # And the offset in the padded_patch to write to\n",
    "                        read_left = max(0, left)\n",
    "                        read_top = max(0, top)\n",
    "                        read_right = min(src.width, right)\n",
    "                        read_bottom = min(src.height, bottom)\n",
    "\n",
    "                        # Check if the calculated window has a valid size\n",
    "                        read_width = read_right - read_left\n",
    "                        read_height = read_bottom - read_top\n",
    "                        \n",
    "                        if read_width > 0 and read_height > 0:\n",
    "                            write_left = read_left - left\n",
    "                            write_top = read_top - top\n",
    "                            write_right = write_left + read_width\n",
    "                            write_bottom = write_top + read_height\n",
    "\n",
    "                            # Create the window object for rasterio to read from\n",
    "                            window = Window(read_left, read_top, read_width, read_height)\n",
    "\n",
    "                            # Read the data from the raster\n",
    "                            patch_data = src.read(1, window=window)\n",
    "                            # Place the read data into the padded patch\n",
    "                            padded_patch[write_top:write_bottom, write_left:write_right] = patch_data\n",
    "                        \n",
    "                        patches_for_this_raster.append(padded_patch)\n",
    "            \n",
    "                # Stack the patches for this raster\n",
    "                patches_for_rasters.append(np.stack(patches_for_this_raster, axis=0))\n",
    "            except Exception as e:\n",
    "                # This handles cases where a raster file might be missing or corrupted\n",
    "                patches_for_rasters.append(np.zeros((len(coords_batch), self.patch_size, self.patch_size), dtype='float32'))\n",
    "\n",
    "\n",
    "        # Stack all raster patches together\n",
    "        final_patches = np.stack(patches_for_rasters, axis=-1)\n",
    "        return final_patches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "        batch_y = self.y[batch_indices]\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        \n",
    "        # Get raster data for the current batch\n",
    "        batch_rasters = self.get_raster_patches(batch_coords)\n",
    "        \n",
    "        # Return a dictionary of inputs and the output\n",
    "        return {\"mlp_input\": batch_mlp, \"gnn_input\": batch_gnn, \"raster_input\": batch_rasters}, batch_y\n",
    "\n",
    "# ==================== 4. Define GNN-MLP-Raster Fusion Model ==================== #\n",
    "def build_fusion_model(mlp_dim, gnn_dim, raster_patch_size, num_rasters):\n",
    "    \"\"\"\n",
    "    Builds the multi-input Keras model with branches for MLP, GNN, and Rasters.\n",
    "    \"\"\"\n",
    "    # Inputs for all branches\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    raster_input = Input(shape=(raster_patch_size, raster_patch_size, num_rasters), name=\"raster_input\")\n",
    "\n",
    "    # --- MLP Branch ---\n",
    "    mlp_embedding = Dense(128, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(64, activation=\"relu\", name=\"mlp_embedding\")(mlp_embedding)\n",
    "\n",
    "    # --- GNN Branch ---\n",
    "    gnn_embedding = Dense(128, activation=\"relu\")(gnn_input)\n",
    "    gnn_embedding = Dense(64, activation=\"relu\", name=\"gnn_embedding\")(gnn_embedding)\n",
    "    \n",
    "    # --- Raster Branch (using a simple CNN) ---\n",
    "    raster_conv = Conv2D(32, (3, 3), activation=\"relu\")(raster_input)\n",
    "    raster_pool = MaxPooling2D((2, 2))(raster_conv)\n",
    "    raster_conv = Conv2D(64, (3, 3), activation=\"relu\")(raster_pool)\n",
    "    raster_pool = MaxPooling2D((2, 2))(raster_conv)\n",
    "    raster_flatten = Flatten()(raster_pool)\n",
    "    raster_embedding = Dense(64, activation=\"relu\", name=\"raster_embedding\")(raster_flatten)\n",
    "\n",
    "    # --- Concatenate Embeddings ---\n",
    "    combined = Concatenate()([mlp_embedding, gnn_embedding, raster_embedding])\n",
    "    \n",
    "    # Final dense layers for prediction\n",
    "    f = Dense(128, activation=\"relu\")(combined)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[mlp_input, gnn_input, raster_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# ==================== 5. Define Evaluation & Importance Functions ==================== #\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    \"\"\"Calculates Symmetric Mean Absolute Percentage Error (SMAPE).\"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    # Avoid division by zero\n",
    "    mask = denominator == 0\n",
    "    smape_val = np.where(mask, 0, numerator / denominator)\n",
    "    return 100 * np.mean(smape_val)\n",
    "\n",
    "def evaluate_model(model, data_inputs, y_test, return_preds=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on given data and returns R², RMSE, MAE, and SMAPE.\n",
    "    Handles both Keras Generators and direct numpy arrays.\n",
    "    \"\"\"\n",
    "    if isinstance(data_inputs, DataGenerator):\n",
    "        y_pred = model.predict(data_inputs, verbose=0).flatten()\n",
    "    else:\n",
    "        y_pred = model.predict(data_inputs, verbose=0).flatten()\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        # Align true labels with predictions if using a generator\n",
    "        y_true_aligned = y_test[:len(y_pred)]\n",
    "        r2 = r2_score(y_true_aligned, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_aligned, y_pred))\n",
    "        mae = mean_absolute_error(y_true_aligned, y_pred)\n",
    "        smape = calculate_smape(y_true_aligned, y_pred)\n",
    "        return r2, rmse, mae, smape\n",
    "\n",
    "def calculate_permutation_importance(model, mlp_data, gnn_data, raster_data, y_true, mlp_features, raster_features):\n",
    "    \"\"\"\n",
    "    Calculates permutation feature importance for all individual features.\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting Permutation Feature Importance Analysis...\")\n",
    "    \n",
    "    # Create the combined input for the model\n",
    "    initial_inputs = {\"mlp_input\": mlp_data, \"gnn_input\": gnn_data, \"raster_input\": raster_data}\n",
    "    \n",
    "    # Get baseline R² on the unshuffled data\n",
    "    baseline_r2, _, _, _ = evaluate_model(model, initial_inputs, y_true)\n",
    "    print(f\"Baseline R²: {baseline_r2:.4f}\")\n",
    "    \n",
    "    importance = {}\n",
    "    \n",
    "    # 1. Permute individual MLP features\n",
    "    print(\"Permuting MLP features...\")\n",
    "    for i, feature in enumerate(mlp_features):\n",
    "        shuffled_mlp_data = mlp_data.copy()\n",
    "        np.random.shuffle(shuffled_mlp_data[:, i])\n",
    "        shuffled_inputs = {\"mlp_input\": shuffled_mlp_data, \"gnn_input\": gnn_data, \"raster_input\": raster_data}\n",
    "        shuffled_r2, _, _, _ = evaluate_model(model, shuffled_inputs, y_true)\n",
    "        importance[f'MLP_{feature}'] = baseline_r2 - shuffled_r2\n",
    "    \n",
    "    # 2. Permute GNN input\n",
    "    print(\"Permuting GNN features...\")\n",
    "    shuffled_gnn_data = gnn_data.copy()\n",
    "    np.random.shuffle(shuffled_gnn_data)\n",
    "    shuffled_inputs = {\"mlp_input\": mlp_data, \"gnn_input\": shuffled_gnn_data, \"raster_input\": raster_data}\n",
    "    shuffled_r2, _, _, _ = evaluate_model(model, shuffled_inputs, y_true)\n",
    "    importance['GNN'] = baseline_r2 - shuffled_r2\n",
    "    \n",
    "    # 3. Permute Raster inputs\n",
    "    print(\"Permuting Raster features...\")\n",
    "    for i, feature in enumerate(raster_features):\n",
    "        shuffled_raster_data = raster_data.copy()\n",
    "        # Shuffle a single channel (raster band)\n",
    "        shuffled_raster_data[:, :, :, i] = np.random.permutation(shuffled_raster_data[:, :, :, i].flatten()).reshape(shuffled_raster_data.shape[0], shuffled_raster_data.shape[1], shuffled_raster_data.shape[2])\n",
    "        shuffled_inputs = {\"mlp_input\": mlp_data, \"gnn_input\": gnn_data, \"raster_input\": shuffled_raster_data}\n",
    "        shuffled_r2, _, _, _ = evaluate_model(model, shuffled_inputs, y_true)\n",
    "        importance[f'Raster_{os.path.basename(feature)}'] = baseline_r2 - shuffled_r2\n",
    "        \n",
    "    return importance\n",
    "\n",
    "# ==================== 6. Main Analysis with K-Fold CV ==================== #\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing GNN-MLP-Raster Fusion Model with 5-Fold Cross-Validation\")\n",
    "print(f\"Using a uniform patch size of {int(round((2 * 500) / pixel_size))} pixels for a 500m buffer.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine all data for K-Fold splitting\n",
    "full_data = pd.concat([orig, river_100], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "full_coords = full_data[['Long','Lat']].values\n",
    "full_y = full_data['RI'].values\n",
    "full_mlp_data = full_data[numeric_cols].values\n",
    "full_raster_data = full_coords # This will be processed by the generator\n",
    "\n",
    "# Pre-process MLP data with StandardScaler\n",
    "scaler = StandardScaler()\n",
    "full_mlp_data = scaler.fit_transform(full_mlp_data)\n",
    "\n",
    "# K-Fold setup\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "all_feature_importances = {}\n",
    "buffer_radius_m = 500\n",
    "raster_patch_size = int(round((2 * buffer_radius_m) / pixel_size))\n",
    "if raster_patch_size % 2 != 0:\n",
    "    raster_patch_size += 1\n",
    "raster_patch_size = max(raster_patch_size, 2)\n",
    "num_rasters = len(raster_paths)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(full_data)):\n",
    "    print(f\"\\n--- Starting Fold {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    # Get train and test data for this fold\n",
    "    train_mlp, test_mlp = full_mlp_data[train_index], full_mlp_data[test_index]\n",
    "    train_coords, test_coords = full_coords[train_index], full_coords[test_index]\n",
    "    y_train, y_test = full_y[train_index], full_y[test_index]\n",
    "    \n",
    "    # Prepare GNN input (adjacency matrix based on distances)\n",
    "    dist_mat_train = distance_matrix(train_coords, train_coords)\n",
    "    gnn_train = np.exp(-dist_mat_train / 10)\n",
    "    \n",
    "    dist_mat_test_train = distance_matrix(test_coords, train_coords)\n",
    "    gnn_test = np.exp(-dist_mat_test_train / 10)\n",
    "\n",
    "    # Clean up memory\n",
    "    del dist_mat_train, dist_mat_test_train\n",
    "    gc.collect()\n",
    "\n",
    "    # Re-build and compile the model for each fold\n",
    "    model = build_fusion_model(mlp_dim=train_mlp.shape[1], gnn_dim=gnn_train.shape[1], \n",
    "                               raster_patch_size=raster_patch_size, num_rasters=num_rasters)\n",
    "    \n",
    "    if fold == 0:\n",
    "        model.summary()\n",
    "    \n",
    "    # Create data generators\n",
    "    train_generator = DataGenerator(\n",
    "        mlp_data=train_mlp, gnn_data=gnn_train, y=y_train, coords=train_coords,\n",
    "        raster_paths=raster_paths, buffer_radius_m=buffer_radius_m, pixel_size=pixel_size, batch_size=4, shuffle=True\n",
    "    )\n",
    "\n",
    "    test_generator = DataGenerator(\n",
    "        mlp_data=test_mlp, gnn_data=gnn_test, y=y_test, coords=test_coords,\n",
    "        raster_paths=raster_paths, buffer_radius_m=buffer_radius_m, pixel_size=pixel_size, batch_size=4, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=100,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping],\n",
    "        validation_data=test_generator\n",
    "    )\n",
    "\n",
    "    # Evaluate on the test data\n",
    "    r2_test, rmse_test, mae_test, smape_test = evaluate_model(model, test_generator, y_test)\n",
    "    fold_results.append({'R2': r2_test, 'RMSE': rmse_test, 'MAE': mae_test, 'SMAPE': smape_test})\n",
    "    \n",
    "    print(f\"Fold {fold+1} Test Metrics:\")\n",
    "    print(f\"R²: {r2_test:.4f} | RMSE: {rmse_test:.4f} | MAE: {mae_test:.4f} | SMAPE: {smape_test:.4f}%\")\n",
    "\n",
    "    # Calculate and store feature importance for this fold\n",
    "    # Get all test data as numpy arrays for importance calculation\n",
    "    test_mlp_full = test_generator.mlp_data\n",
    "    test_gnn_full = test_generator.gnn_data\n",
    "    test_y_full = test_generator.y\n",
    "    test_coords_full = test_generator.coords\n",
    "    \n",
    "    # Create a single batch for raster data\n",
    "    test_rasters_full = test_generator.get_raster_patches(test_coords_full)\n",
    "    \n",
    "    importance = calculate_permutation_importance(model, test_mlp_full, test_gnn_full, test_rasters_full, test_y_full, numeric_cols, raster_paths)\n",
    "    for feature, score in importance.items():\n",
    "        if feature not in all_feature_importances:\n",
    "            all_feature_importances[feature] = []\n",
    "        all_feature_importances[feature].append(score)\n",
    "\n",
    "    del model, history, train_generator, test_generator\n",
    "    gc.collect()\n",
    "\n",
    "# Calculate and print final averages\n",
    "avg_results = pd.DataFrame(fold_results).mean()\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Final Cross-Validation Results (Averaged over {n_splits} folds):\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Average R²: {avg_results['R2']:.4f}\")\n",
    "print(f\"Average RMSE: {avg_results['RMSE']:.4f}\")\n",
    "print(f\"Average MAE: {avg_results['MAE']:.4f}\")\n",
    "print(f\"Average SMAPE: {avg_results['SMAPE']:.4f}%\")\n",
    "\n",
    "# Calculate and print average feature importance\n",
    "print(\"\\n--- Average Feature Importance (Permutation) ---\")\n",
    "avg_importance = {k: np.mean(v) for k, v in all_feature_importances.items()}\n",
    "sorted_importance = sorted(avg_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, score in sorted_importance:\n",
    "    print(f\"{feature}: {score:.4f}\")\n",
    "\n",
    "# ==================== 7. Save all info to a folder ==================== #\n",
    "# NOTE: Removed the file saving functionality as requested. The output is now\n",
    "# printed directly to the console.\n",
    "\n",
    "print(\"\\nAnalysis complete. Results are printed above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a98a2fd-2a6b-4b98-9e02-c62a60b9c79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Analyzing GNN-MLP-Raster Fusion Model (Single Run)\n",
      "Using a uniform patch size of 100 pixels for a 500m buffer.\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ raster_input        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,520</span> │ raster_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ mlp_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,032</span> │ gnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33856</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ raster_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,166,848</span> │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mlp_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ gnn_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ raster_embedding… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ raster_input        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m26\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m98\u001b[0m,    │      \u001b[38;5;34m7,520\u001b[0m │ raster_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m49\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m47\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_2[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ mlp_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m12,032\u001b[0m │ gnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33856\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_3[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ raster_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │  \u001b[38;5;34m2,166,848\u001b[0m │ flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ mlp_embedding[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ gnn_embedding[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ raster_embedding… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m24,704\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,256,481</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,256,481\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,256,481</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,256,481\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Training ---\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 175ms/step - loss: 222932.2969 - val_loss: 28009.3730\n",
      "Epoch 2/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 223441.4375 - val_loss: 4882.5195\n",
      "Epoch 3/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - loss: 29547.0410 - val_loss: 5285.2686\n",
      "Epoch 4/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - loss: 14201.7383 - val_loss: 4018.3577\n",
      "Epoch 5/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 12184.5234 - val_loss: 3284.9578\n",
      "Epoch 6/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - loss: 13595.1885 - val_loss: 2380.7761\n",
      "Epoch 7/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 3847.5535 - val_loss: 2292.3977\n",
      "Epoch 8/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - loss: 5849.3936 - val_loss: 1441.3102\n",
      "Epoch 9/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - loss: 6954.6582 - val_loss: 1148.8159\n",
      "Epoch 10/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - loss: 4760.7393 - val_loss: 1268.5128\n",
      "Epoch 11/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 4002.7290 - val_loss: 1126.5087\n",
      "Epoch 12/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - loss: 2758.3884 - val_loss: 1094.7211\n",
      "Epoch 13/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 2477.7280 - val_loss: 913.7645\n",
      "Epoch 14/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - loss: 2559.0115 - val_loss: 1018.9255\n",
      "Epoch 15/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - loss: 2351.1746 - val_loss: 1079.8004\n",
      "Epoch 16/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - loss: 3355.9143 - val_loss: 2066.3201\n",
      "Epoch 17/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - loss: 2387.0151 - val_loss: 990.8476\n",
      "Epoch 18/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - loss: 3484.7944 - val_loss: 688.9066\n",
      "Epoch 19/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - loss: 2355.4463 - val_loss: 730.1280\n",
      "Epoch 20/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - loss: 1669.0131 - val_loss: 825.2803\n",
      "Epoch 21/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - loss: 2058.0752 - val_loss: 405.7509\n",
      "Epoch 22/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - loss: 2495.9094 - val_loss: 533.9661\n",
      "Epoch 23/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - loss: 3009.4810 - val_loss: 691.6329\n",
      "Epoch 24/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - loss: 1637.3568 - val_loss: 701.1221\n",
      "Epoch 25/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - loss: 2487.7827 - val_loss: 537.4644\n",
      "Epoch 26/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 1449.5581 - val_loss: 710.6362\n",
      "Epoch 27/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - loss: 1692.3960 - val_loss: 605.4865\n",
      "Epoch 28/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 173ms/step - loss: 2826.1545 - val_loss: 496.8951\n",
      "Epoch 29/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 2397.9683 - val_loss: 345.8473\n",
      "Epoch 30/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - loss: 2288.0237 - val_loss: 516.2490\n",
      "Epoch 31/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - loss: 2019.0973 - val_loss: 787.5751\n",
      "Epoch 32/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - loss: 1429.5742 - val_loss: 476.8661\n",
      "Epoch 33/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - loss: 1592.6064 - val_loss: 488.6971\n",
      "Epoch 34/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - loss: 1755.5295 - val_loss: 762.3242\n",
      "Epoch 35/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - loss: 1449.3320 - val_loss: 426.8015\n",
      "Epoch 36/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - loss: 1784.3434 - val_loss: 544.1052\n",
      "Epoch 37/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - loss: 1267.4116 - val_loss: 243.5002\n",
      "Epoch 38/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - loss: 1151.6016 - val_loss: 190.7231\n",
      "Epoch 39/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - loss: 1444.2208 - val_loss: 254.8560\n",
      "Epoch 40/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - loss: 844.1675 - val_loss: 699.7357\n",
      "Epoch 41/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - loss: 1099.2527 - val_loss: 528.7281\n",
      "Epoch 42/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 184ms/step - loss: 886.4319 - val_loss: 407.9691\n",
      "Epoch 43/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 760.7263 - val_loss: 566.8835\n",
      "Epoch 44/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - loss: 1310.4619 - val_loss: 366.0342\n",
      "Epoch 45/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 754.9462 - val_loss: 882.7805\n",
      "Epoch 46/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - loss: 1107.2527 - val_loss: 275.0060\n",
      "Epoch 47/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - loss: 1098.9370 - val_loss: 412.4683\n",
      "Epoch 48/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - loss: 845.0574 - val_loss: 244.6524\n",
      "Epoch 49/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - loss: 1099.6740 - val_loss: 760.9738\n",
      "Epoch 50/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - loss: 908.8618 - val_loss: 523.9377\n",
      "Epoch 51/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - loss: 686.6624 - val_loss: 190.4391\n",
      "Epoch 52/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - loss: 1009.6221 - val_loss: 124.4839\n",
      "Epoch 53/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - loss: 721.4048 - val_loss: 218.0287\n",
      "Epoch 54/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - loss: 1100.3599 - val_loss: 1004.7676\n",
      "Epoch 55/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - loss: 788.2568 - val_loss: 463.1086\n",
      "Epoch 56/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - loss: 1126.1886 - val_loss: 539.5051\n",
      "Epoch 57/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - loss: 908.5811 - val_loss: 409.9253\n",
      "Epoch 58/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 739.4405 - val_loss: 645.4571\n",
      "Epoch 59/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - loss: 533.2357 - val_loss: 684.6094\n",
      "Epoch 60/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - loss: 1112.5920 - val_loss: 807.6190\n",
      "Epoch 61/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - loss: 741.8108 - val_loss: 217.6508\n",
      "Epoch 62/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - loss: 695.6207 - val_loss: 445.9567\n",
      "Epoch 63/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - loss: 1291.0742 - val_loss: 416.4712\n",
      "Epoch 64/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - loss: 814.2670 - val_loss: 761.4118\n",
      "Epoch 65/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - loss: 792.1563 - val_loss: 658.0029\n",
      "Epoch 66/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - loss: 673.2186 - val_loss: 612.9316\n",
      "Epoch 67/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 897.7460 - val_loss: 566.4679\n",
      "Epoch 68/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - loss: 560.2123 - val_loss: 944.1376\n",
      "Epoch 69/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 170ms/step - loss: 721.4183 - val_loss: 737.0779\n",
      "Epoch 70/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - loss: 927.8600 - val_loss: 419.0314\n",
      "Epoch 71/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 671.9572 - val_loss: 178.8280\n",
      "Epoch 72/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - loss: 825.9568 - val_loss: 343.8261\n",
      "Epoch 73/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - loss: 518.3447 - val_loss: 685.1282\n",
      "Epoch 74/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - loss: 682.5502 - val_loss: 1536.4218\n",
      "Epoch 75/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - loss: 857.2125 - val_loss: 585.0803\n",
      "Epoch 76/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - loss: 849.7620 - val_loss: 779.8146\n",
      "Epoch 77/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - loss: 655.0020 - val_loss: 1114.4136\n",
      "\n",
      "================================================================================\n",
      "Final Model Performance on Test Set\n",
      "================================================================================\n",
      "R²: 0.9758\n",
      "RMSE: 11.1572\n",
      "MAE: 8.5361\n",
      "SMAPE: 5.4587%\n",
      "\n",
      "--- Starting Permutation Feature Importance Analysis ---\n",
      "Baseline R²: 0.9758\n",
      "Permuting MLP features...\n",
      "Permuting GNN features...\n",
      "Permuting Raster features...\n",
      "\n",
      "--- Summary of Permutation Importance ---\n",
      "Raster_PbW.tif: 0.9324\n",
      "Raster_CuW.tif: 0.1184\n",
      "Raster_NiW.tif: 0.1130\n",
      "Raster_CrW.tif: 0.0810\n",
      "Raster_SiltW.tif: 0.0384\n",
      "Raster_SandW.tif: 0.0227\n",
      "Raster_AsW.tif: 0.0101\n",
      "MLP_CrW: 0.0044\n",
      "MLP_SiltW: 0.0044\n",
      "MLP_CuW: 0.0042\n",
      "MLP_NiW: 0.0042\n",
      "MLP_FeW: 0.0040\n",
      "MLP_PbW: 0.0039\n",
      "MLP_CdW: 0.0016\n",
      "MLP_hydro_dist_brick: 0.0008\n",
      "MLP_ClayW: 0.0005\n",
      "MLP_AsW: 0.0005\n",
      "MLP_MW: 0.0004\n",
      "MLP_num_brick_field: 0.0003\n",
      "GNN: 0.0001\n",
      "MLP_SandW: 0.0000\n",
      "Raster_bui.tif: 0.0000\n",
      "Raster_ndsi.tif: 0.0000\n",
      "Raster_savi.tif: 0.0000\n",
      "Raster_ndbsi.tif: 0.0000\n",
      "Raster_ui.tif: 0.0000\n",
      "Raster_ndwi.tif: 0.0000\n",
      "Raster_ndbi.tif: 0.0000\n",
      "Raster_awei.tif: 0.0000\n",
      "Raster_evi.tif: 0.0000\n",
      "Raster_mndwi.tif: 0.0000\n",
      "Raster_ndvi.tif: 0.0000\n",
      "Raster_LULC2020.tif: 0.0000\n",
      "Raster_LULC2021.tif: 0.0000\n",
      "Raster_LULC2022.tif: 0.0000\n",
      "Raster_LULC2019.tif: 0.0000\n",
      "Raster_LULC2018.tif: 0.0000\n",
      "Raster_LULC2017.tif: 0.0000\n",
      "Raster_ClayW.tif: -0.0001\n",
      "MLP_num_industry: -0.0001\n",
      "MLP_hydro_dist_ind: -0.0003\n",
      "Raster_CdW.tif: -0.0005\n"
     ]
    }
   ],
   "source": [
    "# ==================== 0. Necessary Imports and Setup ==================== #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import sys\n",
    "import pickle # Import the pickle library for saving objects\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "\n",
    "# Set a consistent seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Enable NumPy-like behavior in TensorFlow\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "# NOTE: This script assumes the following file paths are correct.\n",
    "try:\n",
    "    orig = pd.read_csv(\"../../data/WinterSeason1.csv\")\n",
    "    river_100 = pd.read_csv(\"../data/Samples_100W.csv\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Required data file not found. Please check your file paths.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters and Metadata ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDWW/*.tif\")\n",
    "\n",
    "# Get the pixel resolution from the first raster to set a uniform patch size\n",
    "try:\n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        pixel_size = src.transform.a\n",
    "except IndexError:\n",
    "    print(\"Error: No raster files found in the specified directories.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Create a dictionary to store raster metadata for fast access\n",
    "raster_metadata = {}\n",
    "for path in raster_paths:\n",
    "    with rasterio.open(path) as src:\n",
    "        raster_metadata[path] = {\n",
    "            'transform': src.transform,\n",
    "            'crs': src.crs,\n",
    "            'width': src.width,\n",
    "            'height': src.height\n",
    "        }\n",
    "\n",
    "# ==================== 3. Define a Custom Data Generator ==================== #\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Custom Keras Sequence for generating batches of data.\n",
    "    Handles three different input types: MLP features, GNN features,\n",
    "    and raster image patches, loading rasters on-the-fly to save memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, mlp_data, gnn_data, y, coords, raster_paths, buffer_radius_m, pixel_size, batch_size=4, shuffle=True):\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.coords = coords\n",
    "        self.raster_paths = raster_paths\n",
    "        # Calculate the uniform patch size in pixels based on the buffer radius and pixel size\n",
    "        # We need a square patch, so the size is 2 * radius / pixel_size\n",
    "        self.patch_size = int(round((2 * buffer_radius_m) / pixel_size))\n",
    "        # Ensure patch size is at least 1 and is an even number for easy centering\n",
    "        if self.patch_size % 2 != 0:\n",
    "            self.patch_size += 1\n",
    "        self.patch_size = max(self.patch_size, 2)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def get_raster_patches(self, coords_batch):\n",
    "        \"\"\"\n",
    "        Extracts a patch of raster data for each coordinate in the batch.\n",
    "        Loads rasters on-the-fly to save memory and robustly handles boundaries.\n",
    "        \"\"\"\n",
    "        patches_for_rasters = []\n",
    "        for path in self.raster_paths:\n",
    "            patches_for_this_raster = []\n",
    "            try:\n",
    "                with rasterio.open(path) as src:\n",
    "                    for lon, lat in coords_batch:\n",
    "                        # Get pixel coordinates\n",
    "                        row, col = src.index(lon, lat)\n",
    "                    \n",
    "                        # Define a window to read around the pixel, handling boundaries\n",
    "                        half_patch = self.patch_size // 2\n",
    "                        left = int(col - half_patch)\n",
    "                        top = int(row - half_patch)\n",
    "                        right = int(col + half_patch)\n",
    "                        bottom = int(row + half_patch)\n",
    "\n",
    "                        # Create a new, empty array for the final padded patch\n",
    "                        padded_patch = np.zeros((self.patch_size, self.patch_size), dtype='float32')\n",
    "\n",
    "                        # Calculate the window in the raster's coordinate space to read from\n",
    "                        # And the offset in the padded_patch to write to\n",
    "                        read_left = max(0, left)\n",
    "                        read_top = max(0, top)\n",
    "                        read_right = min(src.width, right)\n",
    "                        read_bottom = min(src.height, bottom)\n",
    "\n",
    "                        # Check if the calculated window has a valid size\n",
    "                        read_width = read_right - read_left\n",
    "                        read_height = read_bottom - read_top\n",
    "                    \n",
    "                        if read_width > 0 and read_height > 0:\n",
    "                            write_left = read_left - left\n",
    "                            write_top = read_top - top\n",
    "                            write_right = write_left + read_width\n",
    "                            write_bottom = write_top + read_height\n",
    "\n",
    "                            # Create the window object for rasterio to read from\n",
    "                            window = Window(read_left, read_top, read_width, read_height)\n",
    "\n",
    "                            # Read the data from the raster\n",
    "                            patch_data = src.read(1, window=window)\n",
    "                            # Place the read data into the padded patch\n",
    "                            padded_patch[write_top:write_bottom, write_left:write_right] = patch_data\n",
    "                    \n",
    "                        patches_for_this_raster.append(padded_patch)\n",
    "            \n",
    "                # Stack the patches for this raster\n",
    "                patches_for_rasters.append(np.stack(patches_for_this_raster, axis=0))\n",
    "            except Exception as e:\n",
    "                # This handles cases where a raster file might be missing or corrupted\n",
    "                patches_for_rasters.append(np.zeros((len(coords_batch), self.patch_size, self.patch_size), dtype='float32'))\n",
    "\n",
    "\n",
    "        # Stack all raster patches together\n",
    "        final_patches = np.stack(patches_for_rasters, axis=-1)\n",
    "        return final_patches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "        batch_y = self.y[batch_indices]\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        \n",
    "        # Get raster data for the current batch\n",
    "        batch_rasters = self.get_raster_patches(batch_coords)\n",
    "        \n",
    "        # Return a dictionary of inputs and the output\n",
    "        return {\"mlp_input\": batch_mlp, \"gnn_input\": batch_gnn, \"raster_input\": batch_rasters}, batch_y\n",
    "\n",
    "# ==================== 4. Define GNN-MLP-Raster Fusion Model ==================== #\n",
    "def build_fusion_model(mlp_dim, gnn_dim, raster_patch_size, num_rasters):\n",
    "    \"\"\"\n",
    "    Builds the multi-input Keras model with branches for MLP, GNN, and Rasters.\n",
    "    \"\"\"\n",
    "    # Inputs for all branches\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    raster_input = Input(shape=(raster_patch_size, raster_patch_size, num_rasters), name=\"raster_input\")\n",
    "\n",
    "    # --- MLP Branch ---\n",
    "    mlp_embedding = Dense(128, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(64, activation=\"relu\", name=\"mlp_embedding\")(mlp_embedding)\n",
    "\n",
    "    # --- GNN Branch ---\n",
    "    gnn_embedding = Dense(128, activation=\"relu\")(gnn_input)\n",
    "    gnn_embedding = Dense(64, activation=\"relu\", name=\"gnn_embedding\")(gnn_embedding)\n",
    "    \n",
    "    # --- Raster Branch (using a simple CNN) ---\n",
    "    raster_conv = Conv2D(32, (3, 3), activation=\"relu\")(raster_input)\n",
    "    raster_pool = MaxPooling2D((2, 2))(raster_conv)\n",
    "    raster_conv = Conv2D(64, (3, 3), activation=\"relu\")(raster_pool)\n",
    "    raster_pool = MaxPooling2D((2, 2))(raster_conv)\n",
    "    raster_flatten = Flatten()(raster_pool)\n",
    "    raster_embedding = Dense(64, activation=\"relu\", name=\"raster_embedding\")(raster_flatten)\n",
    "\n",
    "    # --- Concatenate Embeddings ---\n",
    "    combined = Concatenate()([mlp_embedding, gnn_embedding, raster_embedding])\n",
    "    \n",
    "    # Final dense layers for prediction\n",
    "    f = Dense(128, activation=\"relu\")(combined)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[mlp_input, gnn_input, raster_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# ==================== 5. Define Evaluation & Importance Functions ==================== #\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    \"\"\"Calculates Symmetric Mean Absolute Percentage Error (SMAPE).\"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    # Avoid division by zero\n",
    "    mask = denominator == 0\n",
    "    smape_val = np.where(mask, 0, numerator / denominator)\n",
    "    return 100 * np.mean(smape_val)\n",
    "\n",
    "def evaluate_model(model, data_inputs, y_test, return_preds=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on given data and returns R², RMSE, MAE, and SMAPE.\n",
    "    Handles both Keras Generators and direct numpy arrays.\n",
    "    \"\"\"\n",
    "    if isinstance(data_inputs, DataGenerator):\n",
    "        y_pred = model.predict(data_inputs, verbose=0).flatten()\n",
    "    else:\n",
    "        y_pred = model.predict(data_inputs, verbose=0).flatten()\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        # Align true labels with predictions if using a generator\n",
    "        y_true_aligned = y_test[:len(y_pred)]\n",
    "        r2 = r2_score(y_true_aligned, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_aligned, y_pred))\n",
    "        mae = mean_absolute_error(y_true_aligned, y_pred)\n",
    "        smape = calculate_smape(y_true_aligned, y_pred)\n",
    "        return r2, rmse, mae, smape\n",
    "\n",
    "def calculate_permutation_importance(model, mlp_data, gnn_data, raster_data, y_true, mlp_features, raster_features):\n",
    "    \"\"\"\n",
    "    Calculates permutation feature importance for all individual features.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Permutation Feature Importance Analysis ---\")\n",
    "    \n",
    "    # Create the combined input for the model\n",
    "    initial_inputs = {\"mlp_input\": mlp_data, \"gnn_input\": gnn_data, \"raster_input\": raster_data}\n",
    "    \n",
    "    # Get baseline R² on the unshuffled data\n",
    "    baseline_r2, _, _, _ = evaluate_model(model, initial_inputs, y_true)\n",
    "    print(f\"Baseline R²: {baseline_r2:.4f}\")\n",
    "    \n",
    "    importance = {}\n",
    "    \n",
    "    # 1. Permute individual MLP features\n",
    "    print(\"Permuting MLP features...\")\n",
    "    for i, feature in enumerate(mlp_features):\n",
    "        shuffled_mlp_data = mlp_data.copy()\n",
    "        np.random.shuffle(shuffled_mlp_data[:, i])\n",
    "        shuffled_inputs = {\"mlp_input\": shuffled_mlp_data, \"gnn_input\": gnn_data, \"raster_input\": raster_data}\n",
    "        shuffled_r2, _, _, _ = evaluate_model(model, shuffled_inputs, y_true)\n",
    "        importance[f'MLP_{feature}'] = baseline_r2 - shuffled_r2\n",
    "    \n",
    "    # 2. Permute GNN input (as a single block)\n",
    "    print(\"Permuting GNN features...\")\n",
    "    shuffled_gnn_data = gnn_data.copy()\n",
    "    np.random.shuffle(shuffled_gnn_data)\n",
    "    shuffled_inputs = {\"mlp_input\": mlp_data, \"gnn_input\": shuffled_gnn_data, \"raster_input\": raster_data}\n",
    "    shuffled_r2, _, _, _ = evaluate_model(model, shuffled_inputs, y_true)\n",
    "    importance['GNN'] = baseline_r2 - shuffled_r2\n",
    "    \n",
    "    # 3. Permute Raster inputs (each raster band as a feature)\n",
    "    print(\"Permuting Raster features...\")\n",
    "    for i, feature in enumerate(raster_features):\n",
    "        shuffled_raster_data = raster_data.copy()\n",
    "        # Reshape the channel to a 2D array (samples, pixels) for easy shuffling\n",
    "        reshaped_channel = shuffled_raster_data[:, :, :, i].reshape(shuffled_raster_data.shape[0], -1)\n",
    "        # Shuffle each row independently to keep per-sample values\n",
    "        np.random.shuffle(reshaped_channel)\n",
    "        # Reshape back to the original shape\n",
    "        shuffled_raster_data[:, :, :, i] = reshaped_channel.reshape(shuffled_raster_data.shape[0], shuffled_raster_data.shape[1], shuffled_raster_data.shape[2])\n",
    "        shuffled_inputs = {\"mlp_input\": mlp_data, \"gnn_input\": gnn_data, \"raster_input\": shuffled_raster_data}\n",
    "        shuffled_r2, _, _, _ = evaluate_model(model, shuffled_inputs, y_true)\n",
    "        importance[f'Raster_{os.path.basename(feature)}'] = baseline_r2 - shuffled_r2\n",
    "        \n",
    "    return importance\n",
    "\n",
    "def calculate_intrinsic_importance(model, mlp_features, raster_features):\n",
    "    \"\"\"\n",
    "    Calculates intrinsic feature importance based on the L2 norm of the weights\n",
    "    of the connections from each branch's embedding layer to the first\n",
    "    combined dense layer.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Intrinsic Feature Importance Analysis ---\")\n",
    "    \n",
    "    # Get the embedding layers\n",
    "    mlp_embedding_layer = model.get_layer(\"mlp_embedding\")\n",
    "    gnn_embedding_layer = model.get_layer(\"gnn_embedding\")\n",
    "    raster_embedding_layer = model.get_layer(\"raster_embedding\")\n",
    "\n",
    "    # Get the weights connecting each branch's embedding layer to the output\n",
    "    # For a simple feedforward network, we can look at the weights to the next layer\n",
    "    \n",
    "    # This assumes the first Dense layer after concatenation is the target\n",
    "    # For a more rigorous approach, one would use integrated gradients or similar methods.\n",
    "    \n",
    "    # --- MLP Feature Importance ---\n",
    "    # The weights from the MLP input to the first dense layer\n",
    "    mlp_weights = model.get_layer(index=1).get_weights()[0]\n",
    "    mlp_feature_importance = np.linalg.norm(mlp_weights, axis=1)\n",
    "    \n",
    "    print(\"\\nIntrinsic Importance (MLP Features):\")\n",
    "    for feature, score in zip(mlp_features, mlp_feature_importance):\n",
    "        # Use .item() to extract the scalar before formatting, regardless of type\n",
    "        print(f\"MLP_{feature}: {score.item():.4f}\")\n",
    "        \n",
    "    # --- GNN Branch Importance (as a single unit) ---\n",
    "    # The GNN input is the adjacency matrix, so we treat it as a single block\n",
    "    gnn_weights = model.get_layer(\"gnn_embedding\").get_weights()[0]\n",
    "    gnn_branch_importance = np.linalg.norm(gnn_weights)\n",
    "    print(f\"\\nIntrinsic Importance (GNN Branch): {gnn_branch_importance.item():.4f}\")\n",
    "\n",
    "    # --- Raster Channel Importance ---\n",
    "    # The weights from the last CNN layer to the first dense layer in the raster branch\n",
    "    # The weights are in the shape (pixels_flattened, embedding_dim)\n",
    "    raster_embedding_weights = model.get_layer(\"raster_embedding\").get_weights()[0]\n",
    "    \n",
    "    # We can get a rough per-channel importance by summing the absolute weights for each channel\n",
    "    # This is an approximation as the CNN learns complex spatial features\n",
    "    # A more precise method would be to analyze the filters, but this is a good proxy.\n",
    "    raster_input_shape = model.get_layer(\"raster_input\").input_shape[1:]\n",
    "    num_rasters = raster_input_shape[-1]\n",
    "    \n",
    "    # Get the weights of the first Conv2D layer\n",
    "    first_conv_weights = model.get_layer(index=3).get_weights()[0]\n",
    "    \n",
    "    print(\"\\nIntrinsic Importance (Raster Channels - based on first layer filters):\")\n",
    "    for i in range(num_rasters):\n",
    "        channel_weights = first_conv_weights[:, :, i, :]\n",
    "        importance_score = np.linalg.norm(channel_weights)\n",
    "        print(f\"Raster_{os.path.basename(raster_features[i])}: {importance_score.item():.4f}\")\n",
    "    \n",
    "def calculate_lime_importance(model, test_mlp_data, test_gnn_data, test_raster_data, mlp_features, raster_features):\n",
    "    \"\"\"\n",
    "    Calculates LIME (Local Interpretable Model-agnostic Explanations) importance.\n",
    "    LIME is applied to a combined set of MLP and flattened raster features,\n",
    "    as GNN input is context-dependent and not suitable for LIME.\n",
    "    Note: LIME can be memory intensive, so we use a small number of samples.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting LIME Feature Importance Analysis ---\")\n",
    "    \n",
    "    # Flatten the raster data to a 2D array\n",
    "    flat_raster_data = test_raster_data.reshape(test_raster_data.shape[0], -1)\n",
    "    \n",
    "    # Combine MLP and flattened raster data for LIME\n",
    "    combined_data = np.hstack([test_mlp_data, flat_raster_data])\n",
    "    \n",
    "    # Create the full list of feature names for the combined data\n",
    "    raster_feature_names = [f\"Raster_{os.path.basename(path)}_{i}\" for path in raster_features for i in range(test_raster_data.shape[1] * test_raster_data.shape[2])]\n",
    "    feature_names = list(mlp_features) + raster_feature_names\n",
    "    \n",
    "    # Define a prediction function that LIME can use\n",
    "    def predict_fn(x):\n",
    "        # Unpack the combined features back to their original shapes\n",
    "        mlp_slice = x[:, :len(mlp_features)]\n",
    "        raster_slice = x[:, len(mlp_features):].reshape(x.shape[0], test_raster_data.shape[1], test_raster_data.shape[2], len(raster_features))\n",
    "        \n",
    "        # We need a dummy GNN input for the model prediction\n",
    "        dummy_gnn = np.zeros((x.shape[0], test_gnn_data.shape[1]))\n",
    "        \n",
    "        # Return the model's predictions (LIME expects a single value per sample)\n",
    "        return model.predict({\"mlp_input\": mlp_slice, \"gnn_input\": dummy_gnn, \"raster_input\": raster_slice}, verbose=0)\n",
    "    \n",
    "    # Initialize the LIME explainer\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=combined_data, \n",
    "        feature_names=feature_names, \n",
    "        class_names=[\"RI Prediction\"], \n",
    "        mode='regression'\n",
    "    )\n",
    "    \n",
    "    # Choose a few samples to explain\n",
    "    num_samples = 3 # Reduced to 3 to avoid memory issues\n",
    "    sample_indices = np.random.choice(range(len(test_mlp_data)), num_samples, replace=False)\n",
    "    \n",
    "    lime_importance_scores = {}\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        print(f\"Generating LIME explanation for sample {idx}...\")\n",
    "        explanation = explainer.explain_instance(\n",
    "            data_row=combined_data[idx], \n",
    "            predict_fn=predict_fn, \n",
    "            num_features=10 # Explain the top 10 most important features, as requested\n",
    "        )\n",
    "        for feature, weight in explanation.as_list():\n",
    "            if feature not in lime_importance_scores:\n",
    "                lime_importance_scores[feature] = []\n",
    "            lime_importance_scores[feature].append(abs(weight))\n",
    "            \n",
    "    # Aggregate and average the importance scores\n",
    "    avg_lime_importance = {\n",
    "        feature: np.mean(scores) for feature, scores in lime_importance_scores.items()\n",
    "    }\n",
    "    \n",
    "    # Sort and print the top 10 features\n",
    "    print(\"\\nTop 10 LIME Features (Average Absolute Weight):\")\n",
    "    sorted_lime = sorted(avg_lime_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "    for feature, score in sorted_lime[:10]:\n",
    "        print(f\"{feature}: {score:.4f}\")\n",
    "    \n",
    "    return avg_lime_importance\n",
    "\n",
    "# ==================== 6. Main Analysis without K-Fold CV ==================== #\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analyzing GNN-MLP-Raster Fusion Model (Single Run)\")\n",
    "print(f\"Using a uniform patch size of {int(round((2 * 500) / pixel_size))} pixels for a 500m buffer.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine all data\n",
    "full_data = pd.concat([orig, river_100], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "full_coords = full_data[['Long','Lat']].values\n",
    "full_y = full_data['RI'].values\n",
    "full_mlp_data = full_data[numeric_cols].values\n",
    "\n",
    "# Pre-process MLP data with StandardScaler\n",
    "scaler = StandardScaler()\n",
    "full_mlp_data = scaler.fit_transform(full_mlp_data)\n",
    "\n",
    "# Split data into training and testing sets (e.g., 80% train, 20% test)\n",
    "train_mlp, test_mlp, train_coords, test_coords, y_train, y_test = train_test_split(\n",
    "    full_mlp_data, full_coords, full_y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Prepare GNN input (adjacency matrix based on distances)\n",
    "dist_mat_train = distance_matrix(train_coords, train_coords)\n",
    "gnn_train = np.exp(-dist_mat_train / 10)\n",
    "    \n",
    "dist_mat_test_train = distance_matrix(test_coords, train_coords)\n",
    "gnn_test = np.exp(-dist_mat_test_train / 10)\n",
    "\n",
    "# Clean up memory\n",
    "del dist_mat_train, dist_mat_test_train\n",
    "gc.collect()\n",
    "\n",
    "# Define patch size and number of rasters\n",
    "buffer_radius_m = 500\n",
    "raster_patch_size = int(round((2 * buffer_radius_m) / pixel_size))\n",
    "if raster_patch_size % 2 != 0:\n",
    "    raster_patch_size += 1\n",
    "raster_patch_size = max(raster_patch_size, 2)\n",
    "num_rasters = len(raster_paths)\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_fusion_model(mlp_dim=train_mlp.shape[1], gnn_dim=gnn_train.shape[1], \n",
    "                             raster_patch_size=raster_patch_size, num_rasters=num_rasters)\n",
    "\n",
    "# Print model summary for inspection\n",
    "model.summary()\n",
    "    \n",
    "# Create data generators for training and testing\n",
    "train_generator = DataGenerator(\n",
    "    mlp_data=train_mlp, gnn_data=gnn_train, y=y_train, coords=train_coords,\n",
    "    raster_paths=raster_paths, buffer_radius_m=buffer_radius_m, pixel_size=pixel_size, batch_size=4, shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = DataGenerator(\n",
    "    mlp_data=test_mlp, gnn_data=gnn_test, y=y_test, coords=test_coords,\n",
    "    raster_paths=raster_paths, buffer_radius_m=buffer_radius_m, pixel_size=pixel_size, batch_size=4, shuffle=False\n",
    ")\n",
    "    \n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- Model Training ---\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=test_generator\n",
    ")\n",
    "\n",
    "# Evaluate on the test data\n",
    "r2_test, rmse_test, mae_test, smape_test = evaluate_model(model, test_generator, y_test)\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Final Model Performance on Test Set\")\n",
    "print(\"=\"*80)\n",
    "print(f\"R²: {r2_test:.4f}\")\n",
    "print(f\"RMSE: {rmse_test:.4f}\")\n",
    "print(f\"MAE: {mae_test:.4f}\")\n",
    "print(f\"SMAPE: {smape_test:.4f}%\")\n",
    "\n",
    "# ==================== 7. Feature Importance Analysis ==================== #\n",
    "\n",
    "# --- Prepare data for importance functions (needs to be full numpy arrays) ---\n",
    "# Get all test data from the generator\n",
    "test_mlp_full = test_generator.mlp_data\n",
    "test_gnn_full = test_generator.gnn_data\n",
    "test_y_full = test_generator.y\n",
    "test_coords_full = test_generator.coords\n",
    "test_rasters_full = test_generator.get_raster_patches(test_coords_full)\n",
    "\n",
    "# --- Permutation Importance ---\n",
    "permutation_importance_scores = calculate_permutation_importance(\n",
    "    model, \n",
    "    test_mlp_full, \n",
    "    test_gnn_full, \n",
    "    test_rasters_full, \n",
    "    test_y_full, \n",
    "    numeric_cols, \n",
    "    raster_paths\n",
    ")\n",
    "print(\"\\n--- Summary of Permutation Importance ---\")\n",
    "sorted_perm_importance = sorted(permutation_importance_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, score in sorted_perm_importance:\n",
    "    print(f\"{feature}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce320f9-007d-40f0-81d6-5d93af48995a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Analyzing GNN-MLP-Raster Fusion Model (Single Run)\n",
      "Using a uniform patch size of 100 pixels for a 500m buffer.\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ raster_input        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,520</span> │ raster_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ mlp_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,032</span> │ gnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33856</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ raster_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,166,848</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mlp_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ gnn_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ raster_embedding… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ raster_input        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m26\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m98\u001b[0m,    │      \u001b[38;5;34m7,520\u001b[0m │ raster_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m49\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m47\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ mlp_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m12,032\u001b[0m │ gnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33856\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ raster_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │  \u001b[38;5;34m2,166,848\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ mlp_embedding[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ gnn_embedding[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ raster_embedding… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m24,704\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,256,481</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,256,481\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,256,481</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,256,481\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Permutation Feature Importance Analysis ---\n",
      "Baseline R²: 0.9537\n",
      "Permuting MLP features...\n",
      "Permuting GNN features...\n",
      "Permuting Raster features...\n",
      "\n",
      "--- Summary of Permutation Importance ---\n",
      "Raster_PbW.tif: 1.8036\n",
      "Raster_NiW.tif: 0.1315\n",
      "Raster_SiltW.tif: 0.0815\n",
      "Raster_CrW.tif: 0.0520\n",
      "Raster_AsW.tif: 0.0498\n",
      "Raster_CuW.tif: 0.0405\n",
      "Raster_SandW.tif: 0.0144\n",
      "MLP_CuW: 0.0039\n",
      "MLP_FeW: 0.0026\n",
      "MLP_SiltW: 0.0023\n",
      "MLP_MW: 0.0017\n",
      "MLP_CrW: 0.0014\n",
      "Raster_ClayW.tif: 0.0009\n",
      "MLP_SandW: 0.0007\n",
      "MLP_hydro_dist_ind: 0.0006\n",
      "MLP_NiW: 0.0004\n",
      "MLP_CdW: 0.0002\n",
      "GNN: 0.0001\n",
      "MLP_num_brick_field: 0.0001\n",
      "MLP_num_industry: 0.0000\n",
      "Raster_bui.tif: 0.0000\n",
      "Raster_ndsi.tif: 0.0000\n",
      "Raster_savi.tif: 0.0000\n",
      "Raster_ndbsi.tif: 0.0000\n",
      "Raster_ui.tif: 0.0000\n",
      "Raster_ndwi.tif: 0.0000\n",
      "Raster_ndbi.tif: 0.0000\n",
      "Raster_awei.tif: 0.0000\n",
      "Raster_evi.tif: 0.0000\n",
      "Raster_mndwi.tif: 0.0000\n",
      "Raster_ndvi.tif: 0.0000\n",
      "Raster_LULC2020.tif: 0.0000\n",
      "Raster_LULC2021.tif: 0.0000\n",
      "Raster_LULC2022.tif: 0.0000\n",
      "Raster_LULC2019.tif: 0.0000\n",
      "Raster_LULC2018.tif: 0.0000\n",
      "Raster_LULC2017.tif: 0.0000\n",
      "Raster_CdW.tif: -0.0000\n",
      "MLP_ClayW: -0.0002\n",
      "MLP_hydro_dist_brick: -0.0002\n",
      "MLP_AsW: -0.0004\n",
      "MLP_PbW: -0.0010\n"
     ]
    }
   ],
   "source": [
    "# ==================== 0. Necessary Imports and Setup ==================== #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import sys\n",
    "import pickle # Import the pickle library for saving objects\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "\n",
    "# Set a consistent seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Enable NumPy-like behavior in TensorFlow\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "# NOTE: This script assumes the following file paths are correct.\n",
    "try:\n",
    "    orig = pd.read_csv(\"../../data/WinterSeason1.csv\")\n",
    "    river_100 = pd.read_csv(\"../data/Samples_100W.csv\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Required data file not found. Please check your file paths.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters and Metadata ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDWW/*.tif\")\n",
    "\n",
    "# Get the pixel resolution from the first raster to set a uniform patch size\n",
    "try:\n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        pixel_size = src.transform.a\n",
    "except IndexError:\n",
    "    print(\"Error: No raster files found in the specified directories.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Create a dictionary to store raster metadata for fast access\n",
    "raster_metadata = {}\n",
    "for path in raster_paths:\n",
    "    with rasterio.open(path) as src:\n",
    "        raster_metadata[path] = {\n",
    "            'transform': src.transform,\n",
    "            'crs': src.crs,\n",
    "            'width': src.width,\n",
    "            'height': src.height\n",
    "        }\n",
    "\n",
    "# ==================== 3. Define a Custom Data Generator ==================== #\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Custom Keras Sequence for generating batches of data.\n",
    "    Handles three different input types: MLP features, GNN features,\n",
    "    and raster image patches, loading rasters on-the-fly to save memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, mlp_data, gnn_data, y, coords, raster_paths, buffer_radius_m, pixel_size, batch_size=4, shuffle=True):\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.coords = coords\n",
    "        self.raster_paths = raster_paths\n",
    "        # Calculate the uniform patch size in pixels based on the buffer radius and pixel size\n",
    "        # We need a square patch, so the size is 2 * radius / pixel_size\n",
    "        self.patch_size = int(round((2 * buffer_radius_m) / pixel_size))\n",
    "        # Ensure patch size is at least 1 and is an even number for easy centering\n",
    "        if self.patch_size % 2 != 0:\n",
    "            self.patch_size += 1\n",
    "        self.patch_size = max(self.patch_size, 2)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def get_raster_patches(self, coords_batch):\n",
    "        \"\"\"\n",
    "        Extracts a patch of raster data for each coordinate in the batch.\n",
    "        Loads rasters on-the-fly to save memory and robustly handles boundaries.\n",
    "        \"\"\"\n",
    "        patches_for_rasters = []\n",
    "        for path in self.raster_paths:\n",
    "            patches_for_this_raster = []\n",
    "            try:\n",
    "                with rasterio.open(path) as src:\n",
    "                    for lon, lat in coords_batch:\n",
    "                        # Get pixel coordinates\n",
    "                        row, col = src.index(lon, lat)\n",
    "                    \n",
    "                        # Define a window to read around the pixel, handling boundaries\n",
    "                        half_patch = self.patch_size // 2\n",
    "                        left = int(col - half_patch)\n",
    "                        top = int(row - half_patch)\n",
    "                        right = int(col + half_patch)\n",
    "                        bottom = int(row + half_patch)\n",
    "\n",
    "                        # Create a new, empty array for the final padded patch\n",
    "                        padded_patch = np.zeros((self.patch_size, self.patch_size), dtype='float32')\n",
    "\n",
    "                        # Calculate the window in the raster's coordinate space to read from\n",
    "                        # And the offset in the padded_patch to write to\n",
    "                        read_left = max(0, left)\n",
    "                        read_top = max(0, top)\n",
    "                        read_right = min(src.width, right)\n",
    "                        read_bottom = min(src.height, bottom)\n",
    "\n",
    "                        # Check if the calculated window has a valid size\n",
    "                        read_width = read_right - read_left\n",
    "                        read_height = read_bottom - read_top\n",
    "                    \n",
    "                        if read_width > 0 and read_height > 0:\n",
    "                            write_left = read_left - left\n",
    "                            write_top = read_top - top\n",
    "                            write_right = write_left + read_width\n",
    "                            write_bottom = write_top + read_height\n",
    "\n",
    "                            # Create the window object for rasterio to read from\n",
    "                            window = Window(read_left, read_top, read_width, read_height)\n",
    "\n",
    "                            # Read the data from the raster\n",
    "                            patch_data = src.read(1, window=window)\n",
    "                            # Place the read data into the padded patch\n",
    "                            padded_patch[write_top:write_bottom, write_left:write_right] = patch_data\n",
    "                    \n",
    "                        patches_for_this_raster.append(padded_patch)\n",
    "            \n",
    "                # Stack the patches for this raster\n",
    "                patches_for_rasters.append(np.stack(patches_for_this_raster, axis=0))\n",
    "            except Exception as e:\n",
    "                # This handles cases where a raster file might be missing or corrupted\n",
    "                patches_for_rasters.append(np.zeros((len(coords_batch), self.patch_size, self.patch_size), dtype='float32'))\n",
    "\n",
    "\n",
    "        # Stack all raster patches together\n",
    "        final_patches = np.stack(patches_for_rasters, axis=-1)\n",
    "        return final_patches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "        batch_y = self.y[batch_indices]\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        \n",
    "        # Get raster data for the current batch\n",
    "        batch_rasters = self.get_raster_patches(batch_coords)\n",
    "        \n",
    "        # Return a dictionary of inputs and the output\n",
    "        return {\"mlp_input\": batch_mlp, \"gnn_input\": batch_gnn, \"raster_input\": batch_rasters}, batch_y\n",
    "\n",
    "# ==================== 4. Define GNN-MLP-Raster Fusion Model ==================== #\n",
    "def build_fusion_model(mlp_dim, gnn_dim, raster_patch_size, num_rasters):\n",
    "    \"\"\"\n",
    "    Builds the multi-input Keras model with branches for MLP, GNN, and Rasters.\n",
    "    \"\"\"\n",
    "    # Inputs for all branches\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    raster_input = Input(shape=(raster_patch_size, raster_patch_size, num_rasters), name=\"raster_input\")\n",
    "\n",
    "    # --- MLP Branch ---\n",
    "    mlp_embedding = Dense(128, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(64, activation=\"relu\", name=\"mlp_embedding\")(mlp_embedding)\n",
    "\n",
    "    # --- GNN Branch ---\n",
    "    gnn_embedding = Dense(128, activation=\"relu\")(gnn_input)\n",
    "    gnn_embedding = Dense(64, activation=\"relu\", name=\"gnn_embedding\")(gnn_embedding)\n",
    "    \n",
    "    # --- Raster Branch (using a simple CNN) ---\n",
    "    raster_conv = Conv2D(32, (3, 3), activation=\"relu\")(raster_input)\n",
    "    raster_pool = MaxPooling2D((2, 2))(raster_conv)\n",
    "    raster_conv = Conv2D(64, (3, 3), activation=\"relu\")(raster_pool)\n",
    "    raster_pool = MaxPooling2D((2, 2))(raster_conv)\n",
    "    raster_flatten = Flatten()(raster_pool)\n",
    "    raster_embedding = Dense(64, activation=\"relu\", name=\"raster_embedding\")(raster_flatten)\n",
    "\n",
    "    # --- Concatenate Embeddings ---\n",
    "    combined = Concatenate()([mlp_embedding, gnn_embedding, raster_embedding])\n",
    "    \n",
    "    # Final dense layers for prediction\n",
    "    f = Dense(128, activation=\"relu\")(combined)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[mlp_input, gnn_input, raster_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# ==================== 5. Define Evaluation & Importance Functions ==================== #\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    \"\"\"Calculates Symmetric Mean Absolute Percentage Error (SMAPE).\"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    # Avoid division by zero\n",
    "    mask = denominator == 0\n",
    "    smape_val = np.where(mask, 0, numerator / denominator)\n",
    "    return 100 * np.mean(smape_val)\n",
    "\n",
    "def evaluate_model(model, data_inputs, y_test, return_preds=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on given data and returns R², RMSE, MAE, and SMAPE.\n",
    "    Handles both Keras Generators and direct numpy arrays.\n",
    "    \"\"\"\n",
    "    if isinstance(data_inputs, DataGenerator):\n",
    "        y_pred = model.predict(data_inputs, verbose=0).flatten()\n",
    "    else:\n",
    "        y_pred = model.predict(data_inputs, verbose=0).flatten()\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        # Align true labels with predictions if using a generator\n",
    "        y_true_aligned = y_test[:len(y_pred)]\n",
    "        r2 = r2_score(y_true_aligned, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_aligned, y_pred))\n",
    "        mae = mean_absolute_error(y_true_aligned, y_pred)\n",
    "        smape = calculate_smape(y_true_aligned, y_pred)\n",
    "        return r2, rmse, mae, smape\n",
    "\n",
    "def calculate_permutation_importance(model, mlp_data, gnn_data, raster_data, y_true, mlp_features, raster_features):\n",
    "    \"\"\"\n",
    "    Calculates permutation feature importance for all individual features.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Permutation Feature Importance Analysis ---\")\n",
    "    \n",
    "    # Create the combined input for the model\n",
    "    initial_inputs = {\"mlp_input\": mlp_data, \"gnn_input\": gnn_data, \"raster_input\": raster_data}\n",
    "    \n",
    "    # Get baseline R² on the unshuffled data\n",
    "    baseline_r2, _, _, _ = evaluate_model(model, initial_inputs, y_true)\n",
    "    print(f\"Baseline R²: {baseline_r2:.4f}\")\n",
    "    \n",
    "    importance = {}\n",
    "    \n",
    "    # 1. Permute individual MLP features\n",
    "    print(\"Permuting MLP features...\")\n",
    "    for i, feature in enumerate(mlp_features):\n",
    "        shuffled_mlp_data = mlp_data.copy()\n",
    "        np.random.shuffle(shuffled_mlp_data[:, i])\n",
    "        shuffled_inputs = {\"mlp_input\": shuffled_mlp_data, \"gnn_input\": gnn_data, \"raster_input\": raster_data}\n",
    "        shuffled_r2, _, _, _ = evaluate_model(model, shuffled_inputs, y_true)\n",
    "        importance[f'MLP_{feature}'] = baseline_r2 - shuffled_r2\n",
    "    \n",
    "    # 2. Permute GNN input (as a single block)\n",
    "    print(\"Permuting GNN features...\")\n",
    "    shuffled_gnn_data = gnn_data.copy()\n",
    "    np.random.shuffle(shuffled_gnn_data)\n",
    "    shuffled_inputs = {\"mlp_input\": mlp_data, \"gnn_input\": shuffled_gnn_data, \"raster_input\": raster_data}\n",
    "    shuffled_r2, _, _, _ = evaluate_model(model, shuffled_inputs, y_true)\n",
    "    importance['GNN'] = baseline_r2 - shuffled_r2\n",
    "    \n",
    "    # 3. Permute Raster inputs (each raster band as a feature)\n",
    "    print(\"Permuting Raster features...\")\n",
    "    for i, feature in enumerate(raster_features):\n",
    "        shuffled_raster_data = raster_data.copy()\n",
    "        # Reshape the channel to a 2D array (samples, pixels) for easy shuffling\n",
    "        reshaped_channel = shuffled_raster_data[:, :, :, i].reshape(shuffled_raster_data.shape[0], -1)\n",
    "        # Shuffle each row independently to keep per-sample values\n",
    "        np.random.shuffle(reshaped_channel)\n",
    "        # Reshape back to the original shape\n",
    "        shuffled_raster_data[:, :, :, i] = reshaped_channel.reshape(shuffled_raster_data.shape[0], shuffled_raster_data.shape[1], shuffled_raster_data.shape[2])\n",
    "        shuffled_inputs = {\"mlp_input\": mlp_data, \"gnn_input\": gnn_data, \"raster_input\": shuffled_raster_data}\n",
    "        shuffled_r2, _, _, _ = evaluate_model(model, shuffled_inputs, y_true)\n",
    "        importance[f'Raster_{os.path.basename(feature)}'] = baseline_r2 - shuffled_r2\n",
    "        \n",
    "    return importance\n",
    "\n",
    "def calculate_intrinsic_importance(model, mlp_features, raster_features):\n",
    "    \"\"\"\n",
    "    Calculates intrinsic feature importance based on the L2 norm of the weights\n",
    "    of the connections from each branch's embedding layer to the first\n",
    "    combined dense layer.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Intrinsic Feature Importance Analysis ---\")\n",
    "    \n",
    "    # Get the embedding layers\n",
    "    mlp_embedding_layer = model.get_layer(\"mlp_embedding\")\n",
    "    gnn_embedding_layer = model.get_layer(\"gnn_embedding\")\n",
    "    raster_embedding_layer = model.get_layer(\"raster_embedding\")\n",
    "\n",
    "    # Get the weights connecting each branch's embedding layer to the output\n",
    "    # For a simple feedforward network, we can look at the weights to the next layer\n",
    "    \n",
    "    # This assumes the first Dense layer after concatenation is the target\n",
    "    # For a more rigorous approach, one would use integrated gradients or similar methods.\n",
    "    \n",
    "    # --- MLP Feature Importance ---\n",
    "    # The weights from the MLP input to the first dense layer\n",
    "    mlp_weights = model.get_layer(index=1).get_weights()[0]\n",
    "    mlp_feature_importance = np.linalg.norm(mlp_weights, axis=1)\n",
    "    \n",
    "    print(\"\\nIntrinsic Importance (MLP Features):\")\n",
    "    for feature, score in zip(mlp_features, mlp_feature_importance):\n",
    "        # Use .item() to extract the scalar before formatting, regardless of type\n",
    "        print(f\"MLP_{feature}: {score.item():.4f}\")\n",
    "        \n",
    "    # --- GNN Branch Importance (as a single unit) ---\n",
    "    # The GNN input is the adjacency matrix, so we treat it as a single block\n",
    "    gnn_weights = model.get_layer(\"gnn_embedding\").get_weights()[0]\n",
    "    gnn_branch_importance = np.linalg.norm(gnn_weights)\n",
    "    print(f\"\\nIntrinsic Importance (GNN Branch): {gnn_branch_importance.item():.4f}\")\n",
    "\n",
    "    # --- Raster Channel Importance ---\n",
    "    # The weights from the last CNN layer to the first dense layer in the raster branch\n",
    "    # The weights are in the shape (pixels_flattened, embedding_dim)\n",
    "    raster_embedding_weights = model.get_layer(\"raster_embedding\").get_weights()[0]\n",
    "    \n",
    "    # We can get a rough per-channel importance by summing the absolute weights for each channel\n",
    "    # This is an approximation as the CNN learns complex spatial features\n",
    "    # A more precise method would be to analyze the filters, but this is a good proxy.\n",
    "    raster_input_shape = model.get_layer(\"raster_input\").input_shape[1:]\n",
    "    num_rasters = raster_input_shape[-1]\n",
    "    \n",
    "    # Get the weights of the first Conv2D layer\n",
    "    first_conv_weights = model.get_layer(index=3).get_weights()[0]\n",
    "    \n",
    "    print(\"\\nIntrinsic Importance (Raster Channels - based on first layer filters):\")\n",
    "    for i in range(num_rasters):\n",
    "        channel_weights = first_conv_weights[:, :, i, :]\n",
    "        importance_score = np.linalg.norm(channel_weights)\n",
    "        print(f\"Raster_{os.path.basename(raster_features[i])}: {importance_score.item():.4f}\")\n",
    "    \n",
    "def calculate_lime_importance(model, test_mlp_data, test_gnn_data, test_raster_data, mlp_features, raster_features):\n",
    "    \"\"\"\n",
    "    Calculates LIME (Local Interpretable Model-agnostic Explanations) importance.\n",
    "    LIME is applied to a combined set of MLP and flattened raster features,\n",
    "    as GNN input is context-dependent and not suitable for LIME.\n",
    "    Note: LIME can be memory intensive, so we use a small number of samples.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting LIME Feature Importance Analysis ---\")\n",
    "    \n",
    "    # Flatten the raster data to a 2D array\n",
    "    flat_raster_data = test_raster_data.reshape(test_raster_data.shape[0], -1)\n",
    "    \n",
    "    # Combine MLP and flattened raster data for LIME\n",
    "    combined_data = np.hstack([test_mlp_data, flat_raster_data])\n",
    "    \n",
    "    # Create the full list of feature names for the combined data\n",
    "    raster_feature_names = [f\"Raster_{os.path.basename(path)}_{i}\" for path in raster_features for i in range(test_raster_data.shape[1] * test_raster_data.shape[2])]\n",
    "    feature_names = list(mlp_features) + raster_feature_names\n",
    "    \n",
    "    # Define a prediction function that LIME can use\n",
    "    def predict_fn(x):\n",
    "        # Unpack the combined features back to their original shapes\n",
    "        mlp_slice = x[:, :len(mlp_features)]\n",
    "        raster_slice = x[:, len(mlp_features):].reshape(x.shape[0], test_raster_data.shape[1], test_raster_data.shape[2], len(raster_features))\n",
    "        \n",
    "        # We need a dummy GNN input for the model prediction\n",
    "        dummy_gnn = np.zeros((x.shape[0], test_gnn_data.shape[1]))\n",
    "        \n",
    "        # Return the model's predictions (LIME expects a single value per sample)\n",
    "        return model.predict({\"mlp_input\": mlp_slice, \"gnn_input\": dummy_gnn, \"raster_input\": raster_slice}, verbose=0)\n",
    "    \n",
    "    # Initialize the LIME explainer\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=combined_data, \n",
    "        feature_names=feature_names, \n",
    "        class_names=[\"RI Prediction\"], \n",
    "        mode='regression'\n",
    "    )\n",
    "    \n",
    "    # Choose a few samples to explain\n",
    "    num_samples = 3 # Reduced to 3 to avoid memory issues\n",
    "    sample_indices = np.random.choice(range(len(test_mlp_data)), num_samples, replace=False)\n",
    "    \n",
    "    lime_importance_scores = {}\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        print(f\"Generating LIME explanation for sample {idx}...\")\n",
    "        explanation = explainer.explain_instance(\n",
    "            data_row=combined_data[idx], \n",
    "            predict_fn=predict_fn, \n",
    "            num_features=10 # Explain the top 10 most important features, as requested\n",
    "        )\n",
    "        for feature, weight in explanation.as_list():\n",
    "            if feature not in lime_importance_scores:\n",
    "                lime_importance_scores[feature] = []\n",
    "            lime_importance_scores[feature].append(abs(weight))\n",
    "            \n",
    "    # Aggregate and average the importance scores\n",
    "    avg_lime_importance = {\n",
    "        feature: np.mean(scores) for feature, scores in lime_importance_scores.items()\n",
    "    }\n",
    "    \n",
    "    # Sort and print the top 10 features\n",
    "    print(\"\\nTop 10 LIME Features (Average Absolute Weight):\")\n",
    "    sorted_lime = sorted(avg_lime_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "    for feature, score in sorted_lime[:10]:\n",
    "        print(f\"{feature}: {score:.4f}\")\n",
    "    \n",
    "    return avg_lime_importance\n",
    "\n",
    "# ==================== 6. Main Analysis without K-Fold CV ==================== #\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analyzing GNN-MLP-Raster Fusion Model (Single Run)\")\n",
    "print(f\"Using a uniform patch size of {int(round((2 * 500) / pixel_size))} pixels for a 500m buffer.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine all data\n",
    "full_data = pd.concat([orig, river_100], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "full_coords = full_data[['Long','Lat']].values\n",
    "full_y = full_data['RI'].values\n",
    "full_mlp_data = full_data[numeric_cols].values\n",
    "\n",
    "# Pre-process MLP data with StandardScaler\n",
    "scaler = StandardScaler()\n",
    "full_mlp_data = scaler.fit_transform(full_mlp_data)\n",
    "\n",
    "# Split data into training and testing sets (e.g., 80% train, 20% test)\n",
    "train_mlp, test_mlp, train_coords, test_coords, y_train, y_test = train_test_split(\n",
    "    full_mlp_data, full_coords, full_y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Prepare GNN input (adjacency matrix based on distances)\n",
    "dist_mat_train = distance_matrix(train_coords, train_coords)\n",
    "gnn_train = np.exp(-dist_mat_train / 10)\n",
    "    \n",
    "dist_mat_test_train = distance_matrix(test_coords, train_coords)\n",
    "gnn_test = np.exp(-dist_mat_test_train / 10)\n",
    "\n",
    "# Clean up memory\n",
    "del dist_mat_train, dist_mat_test_train\n",
    "gc.collect()\n",
    "\n",
    "# Define patch size and number of rasters\n",
    "buffer_radius_m = 500\n",
    "raster_patch_size = int(round((2 * buffer_radius_m) / pixel_size))\n",
    "if raster_patch_size % 2 != 0:\n",
    "    raster_patch_size += 1\n",
    "raster_patch_size = max(raster_patch_size, 2)\n",
    "num_rasters = len(raster_paths)\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_fusion_model(mlp_dim=train_mlp.shape[1], gnn_dim=gnn_train.shape[1], \n",
    "                             raster_patch_size=raster_patch_size, num_rasters=num_rasters)\n",
    "\n",
    "# Print model summary for inspection\n",
    "model.summary()\n",
    "    \n",
    "# Create data generators for training and testing\n",
    "train_generator = DataGenerator(\n",
    "    mlp_data=train_mlp, gnn_data=gnn_train, y=y_train, coords=train_coords,\n",
    "    raster_paths=raster_paths, buffer_radius_m=buffer_radius_m, pixel_size=pixel_size, batch_size=4, shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = DataGenerator(\n",
    "    mlp_data=test_mlp, gnn_data=gnn_test, y=y_test, coords=test_coords,\n",
    "    raster_paths=raster_paths, buffer_radius_m=buffer_radius_m, pixel_size=pixel_size, batch_size=4, shuffle=False\n",
    ")\n",
    "    \n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- Model Training ---\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=0,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=test_generator\n",
    ")\n",
    "\n",
    "# ==================== 7. Feature Importance Analysis ==================== #\n",
    "\n",
    "# --- Prepare data for importance functions (needs to be full numpy arrays) ---\n",
    "# Get all test data from the generator\n",
    "test_mlp_full = test_generator.mlp_data\n",
    "test_gnn_full = test_generator.gnn_data\n",
    "test_y_full = test_generator.y\n",
    "test_coords_full = test_generator.coords\n",
    "test_rasters_full = test_generator.get_raster_patches(test_coords_full)\n",
    "\n",
    "# --- Permutation Importance ---\n",
    "permutation_importance_scores = calculate_permutation_importance(\n",
    "    model, \n",
    "    test_mlp_full, \n",
    "    test_gnn_full, \n",
    "    test_rasters_full, \n",
    "    test_y_full, \n",
    "    numeric_cols, \n",
    "    raster_paths\n",
    ")\n",
    "print(\"\\n--- Summary of Permutation Importance ---\")\n",
    "sorted_perm_importance = sorted(permutation_importance_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, score in sorted_perm_importance:\n",
    "    print(f\"{feature}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb2dd82c-ee1c-480d-a7da-7e05217db7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Analyzing GNN-MLP-Raster Fusion Model (Single Run)\n",
      "Using a uniform patch size of 100 pixels for a 500m buffer.\n",
      "================================================================================\n",
      "\n",
      "--- Model Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakibhhridoy/anaconda3/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting LIME analysis for a SINGLE random sample ---\n",
      "\n",
      "Explaining MLP features...\n",
      "LIME Explanation for MLP features:\n",
      "- FeW > 0.76: 2.6344\n",
      "- PbW > -0.03: 1.9294\n",
      "- CuW > -0.13: 1.7351\n",
      "- num_brick_field <= -0.29: 0.9112\n",
      "- NiW > 0.44: 0.8733\n",
      "- -0.30 < SiltW <= 0.88: -0.6198\n",
      "- -1.03 < CrW <= 0.64: -0.2316\n",
      "- -0.36 < MW <= -0.10: -0.1238\n",
      "- -0.07 < AsW <= 1.41: -0.1234\n",
      "- -0.02 < SandW <= 1.36: -0.1224\n",
      "- -0.59 < hydro_dist_ind <= -0.03: -0.1178\n",
      "- -0.32 < ClayW <= -0.11: 0.0718\n",
      "- num_industry <= -0.26: 0.0669\n",
      "- CdW > -0.28: 0.0396\n",
      "- hydro_dist_brick <= -0.76: -0.0193\n"
     ]
    }
   ],
   "source": [
    "# ==================== 0. Necessary Imports and Setup ==================== #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import sys\n",
    "import pickle # Import the pickle library for saving objects\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "\n",
    "# Import LIME components for explanation\n",
    "try:\n",
    "    from lime.lime_tabular import LimeTabularExplainer\n",
    "    from lime.lime_image import LimeImageExplainer\n",
    "except ImportError:\n",
    "    print(\"LIME is not installed. Please install it using: pip install lime\")\n",
    "    sys.exit()\n",
    "\n",
    "# Set a consistent seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Enable NumPy-like behavior in TensorFlow\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "# NOTE: This script assumes the following file paths are correct.\n",
    "try:\n",
    "    orig = pd.read_csv(\"../../data/WinterSeason1.csv\")\n",
    "    river_100 = pd.read_csv(\"../data/Samples_100W.csv\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Required data file not found. Please check your file paths.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters and Metadata ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDWW/*.tif\")\n",
    "\n",
    "# Get the pixel resolution from the first raster to set a uniform patch size\n",
    "try:\n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        pixel_size = src.transform.a\n",
    "except IndexError:\n",
    "    print(\"Error: No raster files found in the specified directories.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Create a dictionary to store raster metadata for fast access\n",
    "raster_metadata = {}\n",
    "for path in raster_paths:\n",
    "    with rasterio.open(path) as src:\n",
    "        raster_metadata[path] = {\n",
    "            'transform': src.transform,\n",
    "            'crs': src.crs,\n",
    "            'width': src.width,\n",
    "            'height': src.height\n",
    "        }\n",
    "\n",
    "# ==================== 3. Define a Custom Data Generator ==================== #\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Custom Keras Sequence for generating batches of data.\n",
    "    Handles three different input types: MLP features, GNN features,\n",
    "    and raster image patches, loading rasters on-the-fly to save memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, mlp_data, gnn_data, y, coords, raster_paths, buffer_radius_m, pixel_size, batch_size=4, shuffle=True):\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.coords = coords\n",
    "        self.raster_paths = raster_paths\n",
    "        # Calculate the uniform patch size in pixels based on the buffer radius and pixel size\n",
    "        self.patch_size = int(round((2 * buffer_radius_m) / pixel_size))\n",
    "        if self.patch_size % 2 != 0:\n",
    "            self.patch_size += 1\n",
    "        self.patch_size = max(self.patch_size, 2)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def get_raster_patches(self, coords_batch):\n",
    "        patches_for_rasters = []\n",
    "        for path in self.raster_paths:\n",
    "            patches_for_this_raster = []\n",
    "            try:\n",
    "                with rasterio.open(path) as src:\n",
    "                    for lon, lat in coords_batch:\n",
    "                        row, col = src.index(lon, lat)\n",
    "                        half_patch = self.patch_size // 2\n",
    "                        left = int(col - half_patch)\n",
    "                        top = int(row - half_patch)\n",
    "                        right = int(col + half_patch)\n",
    "                        bottom = int(row + half_patch)\n",
    "                        padded_patch = np.zeros((self.patch_size, self.patch_size), dtype='float32')\n",
    "                        read_left = max(0, left)\n",
    "                        read_top = max(0, top)\n",
    "                        read_right = min(src.width, right)\n",
    "                        read_bottom = min(src.height, bottom)\n",
    "                        read_width = read_right - read_left\n",
    "                        read_height = read_bottom - read_top\n",
    "                    \n",
    "                        if read_width > 0 and read_height > 0:\n",
    "                            write_left = read_left - left\n",
    "                            write_top = read_top - top\n",
    "                            write_right = write_left + read_width\n",
    "                            write_bottom = write_top + read_height\n",
    "                            window = Window(read_left, read_top, read_width, read_height)\n",
    "                            patch_data = src.read(1, window=window)\n",
    "                            padded_patch[write_top:write_bottom, write_left:write_right] = patch_data\n",
    "                        patches_for_this_raster.append(padded_patch)\n",
    "                patches_for_rasters.append(np.stack(patches_for_this_raster, axis=0))\n",
    "            except Exception as e:\n",
    "                patches_for_rasters.append(np.zeros((len(coords_batch), self.patch_size, self.patch_size), dtype='float32'))\n",
    "        final_patches = np.stack(patches_for_rasters, axis=-1)\n",
    "        return final_patches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "        batch_y = self.y[batch_indices]\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        batch_rasters = self.get_raster_patches(batch_coords)\n",
    "        return {\"mlp_input\": batch_mlp, \"gnn_input\": batch_gnn, \"raster_input\": batch_rasters}, batch_y\n",
    "\n",
    "# ==================== 4. Define GNN-MLP-Raster Fusion Model ==================== #\n",
    "def build_fusion_model(mlp_dim, gnn_dim, raster_patch_size, num_rasters):\n",
    "    \"\"\"\n",
    "    Builds the multi-input Keras model with branches for MLP, GNN, and Rasters.\n",
    "    \"\"\"\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    raster_input = Input(shape=(raster_patch_size, raster_patch_size, num_rasters), name=\"raster_input\")\n",
    "\n",
    "    mlp_embedding = Dense(128, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(64, activation=\"relu\", name=\"mlp_embedding\")(mlp_embedding)\n",
    "\n",
    "    gnn_embedding = Dense(128, activation=\"relu\")(gnn_input)\n",
    "    gnn_embedding = Dense(64, activation=\"relu\", name=\"gnn_embedding\")(gnn_embedding)\n",
    "    \n",
    "    raster_conv = Conv2D(32, (3, 3), activation=\"relu\")(raster_input)\n",
    "    raster_pool = MaxPooling2D((2, 2))(raster_conv)\n",
    "    raster_conv = Conv2D(64, (3, 3), activation=\"relu\")(raster_pool)\n",
    "    raster_pool = MaxPooling2D((2, 2))(raster_conv)\n",
    "    raster_flatten = Flatten()(raster_pool)\n",
    "    raster_embedding = Dense(64, activation=\"relu\", name=\"raster_embedding\")(raster_flatten)\n",
    "\n",
    "    combined = Concatenate()([mlp_embedding, gnn_embedding, raster_embedding])\n",
    "    \n",
    "    f = Dense(128, activation=\"relu\")(combined)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    model = Model(inputs=[mlp_input, gnn_input, raster_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# ==================== 5. Define Evaluation & Importance Functions ==================== #\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    \"\"\"Calculates Symmetric Mean Absolute Percentage Error (SMAPE).\"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    mask = denominator == 0\n",
    "    smape_val = np.where(mask, 0, numerator / denominator)\n",
    "    return 100 * np.mean(smape_val)\n",
    "\n",
    "def evaluate_model(model, data_inputs, y_test, return_preds=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on given data and returns R², RMSE, MAE, and SMAPE.\n",
    "    \"\"\"\n",
    "    if isinstance(data_inputs, DataGenerator):\n",
    "        y_pred = model.predict(data_inputs, verbose=0).flatten()\n",
    "    else:\n",
    "        y_pred = model.predict(data_inputs, verbose=0).flatten()\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        y_true_aligned = y_test[:len(y_pred)]\n",
    "        r2 = r2_score(y_true_aligned, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_aligned, y_pred))\n",
    "        mae = mean_absolute_error(y_true_aligned, y_pred)\n",
    "        smape = calculate_smape(y_true_aligned, y_pred)\n",
    "        return r2, rmse, mae, smape\n",
    "\n",
    "# ==================== 6. Main Analysis without K-Fold CV ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analyzing GNN-MLP-Raster Fusion Model (Single Run)\")\n",
    "print(f\"Using a uniform patch size of {int(round((2 * 500) / pixel_size))} pixels for a 500m buffer.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "full_data = pd.concat([orig, river_100], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "full_coords = full_data[['Long','Lat']].values\n",
    "full_y = full_data['RI'].values\n",
    "full_mlp_data = full_data[numeric_cols].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "full_mlp_data = scaler.fit_transform(full_mlp_data)\n",
    "\n",
    "train_mlp, test_mlp, train_coords, test_coords, y_train, y_test = train_test_split(\n",
    "    full_mlp_data, full_coords, full_y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "dist_mat_train = distance_matrix(train_coords, train_coords)\n",
    "gnn_train = np.exp(-dist_mat_train / 10)\n",
    "    \n",
    "dist_mat_test_train = distance_matrix(test_coords, train_coords)\n",
    "gnn_test = np.exp(-dist_mat_test_train / 10)\n",
    "\n",
    "del dist_mat_train, dist_mat_test_train\n",
    "gc.collect()\n",
    "\n",
    "buffer_radius_m = 500\n",
    "raster_patch_size = int(round((2 * buffer_radius_m) / pixel_size))\n",
    "if raster_patch_size % 2 != 0:\n",
    "    raster_patch_size += 1\n",
    "raster_patch_size = max(raster_patch_size, 2)\n",
    "num_rasters = len(raster_paths)\n",
    "\n",
    "model = build_fusion_model(mlp_dim=train_mlp.shape[1], gnn_dim=gnn_train.shape[1], \n",
    "                             raster_patch_size=raster_patch_size, num_rasters=num_rasters)\n",
    "    \n",
    "train_generator = DataGenerator(\n",
    "    mlp_data=train_mlp, gnn_data=gnn_train, y=y_train, coords=train_coords,\n",
    "    raster_paths=raster_paths, buffer_radius_m=buffer_radius_m, pixel_size=pixel_size, batch_size=4, shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = DataGenerator(\n",
    "    mlp_data=test_mlp, gnn_data=gnn_test, y=y_test, coords=test_coords,\n",
    "    raster_paths=raster_paths, buffer_radius_m=buffer_radius_m, pixel_size=pixel_size, batch_size=4, shuffle=False\n",
    ")\n",
    "    \n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- Model Training ---\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=0,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=test_generator\n",
    ")\n",
    "\n",
    "# ==================== 7. LIME Feature Importance Analysis ==================== #\n",
    "\n",
    "def predict_fn_for_lime(tabular_data, image_data, gnn_data_single_sample):\n",
    "    \"\"\"\n",
    "    A wrapper prediction function for LIME.\n",
    "    It combines the perturbed tabular and image data with the fixed GNN data\n",
    "    to make a prediction.\n",
    "    \"\"\"\n",
    "    # GNN data is fixed for LIME analysis as it's not a feature of a single sample\n",
    "    gnn_batch = np.tile(gnn_data_single_sample, (tabular_data.shape[0], 1))\n",
    "    \n",
    "    # Run prediction on the combined data\n",
    "    predictions = model.predict({\n",
    "        \"mlp_input\": tabular_data,\n",
    "        \"gnn_input\": gnn_batch,\n",
    "        \"raster_input\": image_data\n",
    "    }, verbose=0)\n",
    "    \n",
    "    # Return predictions in the format LIME expects for regression\n",
    "    return predictions\n",
    "\n",
    "# --- Prepare data for LIME analysis (needs to be full numpy arrays) ---\n",
    "# Get all test data from the generator\n",
    "test_mlp_full = test_generator.mlp_data\n",
    "test_gnn_full = test_generator.gnn_data\n",
    "test_y_full = test_generator.y\n",
    "test_coords_full = test_generator.coords\n",
    "test_rasters_full = test_generator.get_raster_patches(test_coords_full)\n",
    "\n",
    "# --- Set up explainers for each input type ---\n",
    "# LIME for MLP (tabular) data\n",
    "explainer_mlp = LimeTabularExplainer(\n",
    "    training_data=test_mlp_full,\n",
    "    feature_names=numeric_cols,\n",
    "    mode='regression'\n",
    ")\n",
    "\n",
    "# LIME for Raster (image) data\n",
    "explainer_raster = LimeImageExplainer()\n",
    "\n",
    "# --- Explain a single random sample to avoid memory crash ---\n",
    "print(\"\\n--- Starting LIME analysis for a SINGLE random sample ---\")\n",
    "random_index = np.random.randint(0, len(test_mlp_full))\n",
    "\n",
    "# Select the single sample data\n",
    "sample_mlp = test_mlp_full[random_index].reshape(1, -1)\n",
    "sample_gnn = test_gnn_full[random_index].reshape(1, -1)\n",
    "sample_raster = test_rasters_full[random_index]\n",
    "sample_y = test_y_full[random_index]\n",
    "\n",
    "# --- Get LIME explanation for MLP features ---\n",
    "print(\"\\nExplaining MLP features...\")\n",
    "# LIME's explain_instance expects a 1D array for tabular data\n",
    "lime_exp_mlp = explainer_mlp.explain_instance(\n",
    "    data_row=test_mlp_full[random_index], \n",
    "    predict_fn=lambda x: predict_fn_for_lime(x, np.tile(sample_raster, (x.shape[0], 1, 1, 1)), sample_gnn),\n",
    "    num_features=len(numeric_cols)\n",
    ")\n",
    "print(\"LIME Explanation for MLP features:\")\n",
    "for feature, weight in lime_exp_mlp.as_list():\n",
    "    print(f\"- {feature}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c15ba089-2bfc-4acc-a5b7-6c5f45cbb6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"GNN_MLP.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21431036-1b3a-41de-a112-f7ef433c10a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting LIME Feature Importance Analysis ---\n",
      "Generating LIME explanation for sample 12...\n"
     ]
    }
   ],
   "source": [
    "# --- LIME Importance ---\n",
    "# This can be computationally intensive, so it's run on a sample\n",
    "lime_importance_scores = calculate_lime_importance(\n",
    "    model, \n",
    "    test_mlp_full, \n",
    "    test_gnn_full, \n",
    "    test_rasters_full, \n",
    "    numeric_cols, \n",
    "    raster_paths\n",
    ")\n",
    "\n",
    "print(\"\\nAnalysis complete. All results are printed above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e109c4-c88a-488f-bca8-91281b94ce35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92fadf4-3eda-4531-80a0-9f170c321aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b091618d-2acf-4b83-810a-351b30e3a0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34652bfb-a144-48d1-a213-eec1052311f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2774cfa6-73ee-4cb0-b1c3-6731edb773dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 0. Necessary Imports and Setup ==================== #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import sys\n",
    "import pickle # Import the pickle library for saving objects\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "\n",
    "# Set a consistent seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Enable NumPy-like behavior in TensorFlow\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "# NOTE: This script assumes the following file paths are correct.\n",
    "try:\n",
    "    orig = pd.read_csv(\"../../data/WinterSeason1.csv\")\n",
    "    river_100 = pd.read_csv(\"../data/Samples_100W.csv\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Required data file not found. Please check your file paths.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters and Metadata ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDWW/*.tif\")\n",
    "\n",
    "# Get the pixel resolution from the first raster to set a uniform patch size\n",
    "try:\n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        pixel_size = src.transform.a\n",
    "except IndexError:\n",
    "    print(\"Error: No raster files found in the specified directories.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Create a dictionary to store raster metadata for fast access\n",
    "raster_metadata = {}\n",
    "for path in raster_paths:\n",
    "    with rasterio.open(path) as src:\n",
    "        raster_metadata[path] = {\n",
    "            'transform': src.transform,\n",
    "            'crs': src.crs,\n",
    "            'width': src.width,\n",
    "            'height': src.height\n",
    "        }\n",
    "\n",
    "# ==================== 3. Define a Custom Data Generator ==================== #\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Custom Keras Sequence for generating batches of data.\n",
    "    Handles three different input types: MLP features, GNN features,\n",
    "    and raster image patches, loading rasters on-the-fly to save memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, mlp_data, gnn_data, y, coords, raster_paths, buffer_radius_m, pixel_size, batch_size=4, shuffle=True):\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.coords = coords\n",
    "        self.raster_paths = raster_paths\n",
    "        # Calculate the uniform patch size in pixels based on the buffer radius and pixel size\n",
    "        # We need a square patch, so the size is 2 * radius / pixel_size\n",
    "        self.patch_size = int(round((2 * buffer_radius_m) / pixel_size))\n",
    "        # Ensure patch size is at least 1 and is an even number for easy centering\n",
    "        if self.patch_size % 2 != 0:\n",
    "            self.patch_size += 1\n",
    "        self.patch_size = max(self.patch_size, 2)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def get_raster_patches(self, coords_batch):\n",
    "        \"\"\"\n",
    "        Extracts a patch of raster data for each coordinate in the batch.\n",
    "        Loads rasters on-the-fly to save memory and robustly handles boundaries.\n",
    "        \"\"\"\n",
    "        patches_for_rasters = []\n",
    "        for path in self.raster_paths:\n",
    "            patches_for_this_raster = []\n",
    "            try:\n",
    "                with rasterio.open(path) as src:\n",
    "                    for lon, lat in coords_batch:\n",
    "                        # Get pixel coordinates\n",
    "                        row, col = src.index(lon, lat)\n",
    "                    \n",
    "                        # Define a window to read around the pixel, handling boundaries\n",
    "                        half_patch = self.patch_size // 2\n",
    "                        left = int(col - half_patch)\n",
    "                        top = int(row - half_patch)\n",
    "                        right = int(col + half_patch)\n",
    "                        bottom = int(row + half_patch)\n",
    "\n",
    "                        # Create a new, empty array for the final padded patch\n",
    "                        padded_patch = np.zeros((self.patch_size, self.patch_size), dtype='float32')\n",
    "\n",
    "                        # Calculate the window in the raster's coordinate space to read from\n",
    "                        # And the offset in the padded_patch to write to\n",
    "                        read_left = max(0, left)\n",
    "                        read_top = max(0, top)\n",
    "                        read_right = min(src.width, right)\n",
    "                        read_bottom = min(src.height, bottom)\n",
    "\n",
    "                        # Check if the calculated window has a valid size\n",
    "                        read_width = read_right - read_left\n",
    "                        read_height = read_bottom - read_top\n",
    "                    \n",
    "                        if read_width > 0 and read_height > 0:\n",
    "                            write_left = read_left - left\n",
    "                            write_top = read_top - top\n",
    "                            write_right = write_left + read_width\n",
    "                            write_bottom = write_top + read_height\n",
    "\n",
    "                            # Create the window object for rasterio to read from\n",
    "                            window = Window(read_left, read_top, read_width, read_height)\n",
    "\n",
    "                            # Read the data from the raster\n",
    "                            patch_data = src.read(1, window=window)\n",
    "                            # Place the read data into the padded patch\n",
    "                            padded_patch[write_top:write_bottom, write_left:write_right] = patch_data\n",
    "                    \n",
    "                        patches_for_this_raster.append(padded_patch)\n",
    "            \n",
    "                # Stack the patches for this raster\n",
    "                patches_for_rasters.append(np.stack(patches_for_this_raster, axis=0))\n",
    "            except Exception as e:\n",
    "                # This handles cases where a raster file might be missing or corrupted\n",
    "                patches_for_rasters.append(np.zeros((len(coords_batch), self.patch_size, self.patch_size), dtype='float32'))\n",
    "\n",
    "\n",
    "        # Stack all raster patches together\n",
    "        final_patches = np.stack(patches_for_rasters, axis=-1)\n",
    "        return final_patches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "        batch_y = self.y[batch_indices]\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        \n",
    "        # Get raster data for the current batch\n",
    "        batch_rasters = self.get_raster_patches(batch_coords)\n",
    "        \n",
    "        # Return a dictionary of inputs and the output\n",
    "        return {\"mlp_input\": batch_mlp, \"gnn_input\": batch_gnn, \"raster_input\": batch_rasters}, batch_y\n",
    "\n",
    "# ==================== 4. Define GNN-MLP-Raster Fusion Model ==================== #\n",
    "def build_fusion_model(mlp_dim, gnn_dim, raster_patch_size, num_rasters):\n",
    "    \"\"\"\n",
    "    Builds the multi-input Keras model with branches for MLP, GNN, and Rasters.\n",
    "    \"\"\"\n",
    "    # Inputs for all branches\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    raster_input = Input(shape=(raster_patch_size, raster_patch_size, num_rasters), name=\"raster_input\")\n",
    "\n",
    "    # --- MLP Branch ---\n",
    "    mlp_embedding = Dense(128, activation=\"relu\", name=\"mlp_embedding_dense1\")(mlp_input)\n",
    "    mlp_embedding = Dense(64, activation=\"relu\", name=\"mlp_embedding\")(mlp_embedding)\n",
    "\n",
    "    # --- GNN Branch ---\n",
    "    gnn_embedding = Dense(128, activation=\"relu\", name=\"gnn_embedding_dense1\")(gnn_input)\n",
    "    gnn_embedding = Dense(64, activation=\"relu\", name=\"gnn_embedding\")(gnn_embedding)\n",
    "    \n",
    "    # --- Raster Branch (using a simple CNN) ---\n",
    "    raster_conv = Conv2D(32, (3, 3), activation=\"relu\", name=\"raster_conv1\")(raster_input)\n",
    "    raster_pool = MaxPooling2D((2, 2))(raster_conv)\n",
    "    raster_conv = Conv2D(64, (3, 3), activation=\"relu\", name=\"raster_conv2\")(raster_pool)\n",
    "    raster_pool = MaxPooling2D((2, 2))(raster_conv)\n",
    "    raster_flatten = Flatten()(raster_pool)\n",
    "    raster_embedding = Dense(64, activation=\"relu\", name=\"raster_embedding\")(raster_flatten)\n",
    "\n",
    "    # --- Concatenate Embeddings ---\n",
    "    combined = Concatenate()([mlp_embedding, gnn_embedding, raster_embedding])\n",
    "    \n",
    "    # Final dense layers for prediction\n",
    "    f = Dense(128, activation=\"relu\")(combined)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[mlp_input, gnn_input, raster_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# ==================== 5. Define Evaluation & Importance Functions ==================== #\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    \"\"\"Calculates Symmetric Mean Absolute Percentage Error (SMAPE).\"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    # Avoid division by zero\n",
    "    mask = denominator == 0\n",
    "    smape_val = np.where(mask, 0, numerator / denominator)\n",
    "    return 100 * np.mean(smape_val)\n",
    "\n",
    "def evaluate_model(model, data_inputs, y_test, return_preds=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on given data and returns R², RMSE, MAE, and SMAPE.\n",
    "    Handles both Keras Generators and direct numpy arrays.\n",
    "    \"\"\"\n",
    "    if isinstance(data_inputs, DataGenerator):\n",
    "        y_pred = model.predict(data_inputs, verbose=0).flatten()\n",
    "    else:\n",
    "        y_pred = model.predict(data_inputs, verbose=0).flatten()\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        # Align true labels with predictions if using a generator\n",
    "        y_true_aligned = y_test[:len(y_pred)]\n",
    "        r2 = r2_score(y_true_aligned, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_aligned, y_pred))\n",
    "        mae = mean_absolute_error(y_true_aligned, y_pred)\n",
    "        smape = calculate_smape(y_true_aligned, y_pred)\n",
    "        return r2, rmse, mae, smape\n",
    "\n",
    "def calculate_permutation_importance(model, mlp_data, gnn_data, raster_data, y_true, mlp_features, raster_features):\n",
    "    \"\"\"\n",
    "    Calculates permutation feature importance for all individual features.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Permutation Feature Importance Analysis ---\")\n",
    "    \n",
    "    # Create the combined input for the model\n",
    "    initial_inputs = {\"mlp_input\": mlp_data, \"gnn_input\": gnn_data, \"raster_input\": raster_data}\n",
    "    \n",
    "    # Get baseline R² on the unshuffled data\n",
    "    baseline_r2, _, _, _ = evaluate_model(model, initial_inputs, y_true)\n",
    "    print(f\"Baseline R²: {baseline_r2:.4f}\")\n",
    "    \n",
    "    importance = {}\n",
    "    \n",
    "    # 1. Permute individual MLP features\n",
    "    print(\"Permuting MLP features...\")\n",
    "    for i, feature in enumerate(mlp_features):\n",
    "        shuffled_mlp_data = mlp_data.copy()\n",
    "        np.random.shuffle(shuffled_mlp_data[:, i])\n",
    "        shuffled_inputs = {\"mlp_input\": shuffled_mlp_data, \"gnn_input\": gnn_data, \"raster_input\": raster_data}\n",
    "        shuffled_r2, _, _, _ = evaluate_model(model, shuffled_inputs, y_true)\n",
    "        importance[f'MLP_{feature}'] = baseline_r2 - shuffled_r2\n",
    "    \n",
    "    # 2. Permute GNN input (as a single block)\n",
    "    print(\"Permuting GNN features...\")\n",
    "    shuffled_gnn_data = gnn_data.copy()\n",
    "    np.random.shuffle(shuffled_gnn_data)\n",
    "    shuffled_inputs = {\"mlp_input\": mlp_data, \"gnn_input\": shuffled_gnn_data, \"raster_input\": raster_data}\n",
    "    shuffled_r2, _, _, _ = evaluate_model(model, shuffled_inputs, y_true)\n",
    "    importance['GNN'] = baseline_r2 - shuffled_r2\n",
    "    \n",
    "    # 3. Permute Raster inputs (each raster band as a feature)\n",
    "    print(\"Permuting Raster features...\")\n",
    "    for i, feature in enumerate(raster_features):\n",
    "        shuffled_raster_data = raster_data.copy()\n",
    "        # Reshape the channel to a 2D array (samples, pixels) for easy shuffling\n",
    "        reshaped_channel = shuffled_raster_data[:, :, :, i].reshape(shuffled_raster_data.shape[0], -1)\n",
    "        # Shuffle each row independently to keep per-sample values\n",
    "        np.random.shuffle(reshaped_channel)\n",
    "        # Reshape back to the original shape\n",
    "        shuffled_raster_data[:, :, :, i] = reshaped_channel.reshape(shuffled_raster_data.shape[0], shuffled_raster_data.shape[1], shuffled_raster_data.shape[2])\n",
    "        shuffled_inputs = {\"mlp_input\": mlp_data, \"gnn_input\": gnn_data, \"raster_input\": shuffled_raster_data}\n",
    "        shuffled_r2, _, _, _ = evaluate_model(model, shuffled_inputs, y_true)\n",
    "        importance[f'Raster_{os.path.basename(feature)}'] = baseline_r2 - shuffled_r2\n",
    "        \n",
    "    return importance\n",
    "\n",
    "def calculate_intrinsic_importance(model, mlp_features, raster_features):\n",
    "    \"\"\"\n",
    "    Calculates intrinsic feature importance based on the weights of the model.\n",
    "    This approach is more robust to the previous TypeError.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Intrinsic Feature Importance Analysis ---\")\n",
    "\n",
    "    # === Branch-level Importance (L2 Norm of embedding layer weights) ===\n",
    "    # This gives a single score for the overall importance of each data type.\n",
    "    \n",
    "    print(\"\\nIntrinsic Importance (Branch-level):\")\n",
    "    \n",
    "    # MLP Branch\n",
    "    mlp_weights = model.get_layer(\"mlp_embedding\").get_weights()[0]\n",
    "    mlp_branch_importance = np.linalg.norm(mlp_weights)\n",
    "    print(f\"MLP Branch: {mlp_branch_importance.item():.4f}\")\n",
    "\n",
    "    # GNN Branch\n",
    "    gnn_weights = model.get_layer(\"gnn_embedding\").get_weights()[0]\n",
    "    gnn_branch_importance = np.linalg.norm(gnn_weights)\n",
    "    print(f\"GNN Branch: {gnn_branch_importance.item():.4f}\")\n",
    "    \n",
    "    # Raster Branch\n",
    "    raster_weights = model.get_layer(\"raster_embedding\").get_weights()[0]\n",
    "    raster_branch_importance = np.linalg.norm(raster_weights)\n",
    "    print(f\"Raster Branch: {raster_branch_importance.item():.4f}\")\n",
    "\n",
    "    # === Feature-level Importance (Sum of absolute weights from input to first dense layer) ===\n",
    "    # This provides a score for each individual feature within the MLP and Raster branches.\n",
    "    \n",
    "    print(\"\\nIntrinsic Importance (Feature-level):\")\n",
    "    \n",
    "    # MLP Features\n",
    "    mlp_input_weights = model.get_layer(\"mlp_embedding_dense1\").get_weights()[0]\n",
    "    # Sum the absolute weights for each input feature across all its connections to the next layer\n",
    "    mlp_feature_importance = np.sum(np.abs(mlp_input_weights), axis=1)\n",
    "    \n",
    "    print(\"\\nMLP Features:\")\n",
    "    for feature, score in zip(mlp_features, mlp_feature_importance):\n",
    "        print(f\"  {feature}: {score.item():.4f}\")\n",
    "        \n",
    "    # Raster Channels\n",
    "    # Get the weights from the first convolutional layer\n",
    "    raster_conv_weights = model.get_layer(\"raster_conv1\").get_weights()[0]\n",
    "    # Sum the absolute weights for each input channel\n",
    "    raster_channel_importance = np.sum(np.abs(raster_conv_weights), axis=(0, 1, 3))\n",
    "    \n",
    "    print(\"\\nRaster Channels:\")\n",
    "    for i, score in enumerate(raster_channel_importance):\n",
    "        feature_name = os.path.basename(raster_features[i])\n",
    "        print(f\"  Raster_{feature_name}: {score.item():.4f}\")\n",
    "    \n",
    "def calculate_lime_importance(model, test_mlp_data, test_gnn_data, test_raster_data, mlp_features, raster_features):\n",
    "    \"\"\"\n",
    "    Calculates LIME (Local Interpretable Model-agnostic Explanations) importance.\n",
    "    LIME is applied to a combined set of MLP and flattened raster features,\n",
    "    as GNN input is context-dependent and not suitable for LIME.\n",
    "    Note: LIME can be memory intensive, so we use a small number of samples.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting LIME Feature Importance Analysis ---\")\n",
    "    \n",
    "    # Flatten the raster data to a 2D array\n",
    "    flat_raster_data = test_raster_data.reshape(test_raster_data.shape[0], -1)\n",
    "    \n",
    "    # Combine MLP and flattened raster data for LIME\n",
    "    combined_data = np.hstack([test_mlp_data, flat_raster_data])\n",
    "    \n",
    "    # Create the full list of feature names for the combined data\n",
    "    raster_feature_names = [f\"Raster_{os.path.basename(path)}_{i}\" for path in raster_features for i in range(test_raster_data.shape[1] * test_raster_data.shape[2])]\n",
    "    feature_names = list(mlp_features) + raster_feature_names\n",
    "    \n",
    "    # Define a prediction function that LIME can use\n",
    "    def predict_fn(x):\n",
    "        # Unpack the combined features back to their original shapes\n",
    "        mlp_slice = x[:, :len(mlp_features)]\n",
    "        raster_slice = x[:, len(mlp_features):].reshape(x.shape[0], test_raster_data.shape[1], test_raster_data.shape[2], len(raster_features))\n",
    "        \n",
    "        # We need a dummy GNN input for the model prediction\n",
    "        dummy_gnn = np.zeros((x.shape[0], test_gnn_data.shape[1]))\n",
    "        \n",
    "        # Return the model's predictions (LIME expects a single value per sample)\n",
    "        return model.predict({\"mlp_input\": mlp_slice, \"gnn_input\": dummy_gnn, \"raster_input\": raster_slice}, verbose=0)\n",
    "    \n",
    "    # Initialize the LIME explainer\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=combined_data, \n",
    "        feature_names=feature_names, \n",
    "        class_names=[\"RI Prediction\"], \n",
    "        mode='regression'\n",
    "    )\n",
    "    \n",
    "    # Choose a few samples to explain\n",
    "    num_samples = 3 # Reduced to 3 to avoid memory issues\n",
    "    sample_indices = np.random.choice(range(len(test_mlp_data)), num_samples, replace=False)\n",
    "    \n",
    "    lime_importance_scores = {}\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        print(f\"Generating LIME explanation for sample {idx}...\")\n",
    "        explanation = explainer.explain_instance(\n",
    "            data_row=combined_data[idx], \n",
    "            predict_fn=predict_fn, \n",
    "            num_features=10 # Explain the top 10 most important features, as requested\n",
    "        )\n",
    "        for feature, weight in explanation.as_list():\n",
    "            if feature not in lime_importance_scores:\n",
    "                lime_importance_scores[feature] = []\n",
    "            lime_importance_scores[feature].append(abs(weight))\n",
    "            \n",
    "    # Aggregate and average the importance scores\n",
    "    avg_lime_importance = {\n",
    "        feature: np.mean(scores) for feature, scores in lime_importance_scores.items()\n",
    "    }\n",
    "    \n",
    "    # Sort and print the top 10 features\n",
    "    print(\"\\nTop 10 LIME Features (Average Absolute Weight):\")\n",
    "    sorted_lime = sorted(avg_lime_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "    for feature, score in sorted_lime[:10]:\n",
    "        print(f\"{feature}: {score:.4f}\")\n",
    "    \n",
    "    return avg_lime_importance\n",
    "\n",
    "# ==================== 6. Main Analysis without K-Fold CV ==================== #\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analyzing GNN-MLP-Raster Fusion Model (Single Run)\")\n",
    "print(f\"Using a uniform patch size of {int(round((2 * 500) / pixel_size))} pixels for a 500m buffer.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine all data\n",
    "full_data = pd.concat([orig, river_100], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "full_coords = full_data[['Long','Lat']].values\n",
    "full_y = full_data['RI'].values\n",
    "full_mlp_data = full_data[numeric_cols].values\n",
    "\n",
    "# Pre-process MLP data with StandardScaler\n",
    "scaler = StandardScaler()\n",
    "full_mlp_data = scaler.fit_transform(full_mlp_data)\n",
    "\n",
    "# Split data into training and testing sets (e.g., 80% train, 20% test)\n",
    "train_mlp, test_mlp, train_coords, test_coords, y_train, y_test = train_test_split(\n",
    "    full_mlp_data, full_coords, full_y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Prepare GNN input (adjacency matrix based on distances)\n",
    "dist_mat_train = distance_matrix(train_coords, train_coords)\n",
    "gnn_train = np.exp(-dist_mat_train / 10)\n",
    "    \n",
    "dist_mat_test_train = distance_matrix(test_coords, train_coords)\n",
    "gnn_test = np.exp(-dist_mat_test_train / 10)\n",
    "\n",
    "# Clean up memory\n",
    "del dist_mat_train, dist_mat_test_train\n",
    "gc.collect()\n",
    "\n",
    "# Define patch size and number of rasters\n",
    "buffer_radius_m = 500\n",
    "raster_patch_size = int(round((2 * buffer_radius_m) / pixel_size))\n",
    "if raster_patch_size % 2 != 0:\n",
    "    raster_patch_size += 1\n",
    "raster_patch_size = max(raster_patch_size, 2)\n",
    "num_rasters = len(raster_paths)\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_fusion_model(mlp_dim=train_mlp.shape[1], gnn_dim=gnn_train.shape[1], \n",
    "                             raster_patch_size=raster_patch_size, num_rasters=num_rasters)\n",
    "\n",
    "# Print model summary for inspection\n",
    "model.summary()\n",
    "    \n",
    "# Create data generators for training and testing\n",
    "train_generator = DataGenerator(\n",
    "    mlp_data=train_mlp, gnn_data=gnn_train, y=y_train, coords=train_coords,\n",
    "    raster_paths=raster_paths, buffer_radius_m=buffer_radius_m, pixel_size=pixel_size, batch_size=4, shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = DataGenerator(\n",
    "    mlp_data=test_mlp, gnn_data=gnn_test, y=y_test, coords=test_coords,\n",
    "    raster_paths=raster_paths, buffer_radius_m=buffer_radius_m, pixel_size=pixel_size, batch_size=4, shuffle=False\n",
    ")\n",
    "    \n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- Model Training ---\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=1,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=test_generator\n",
    ")\n",
    "\n",
    "# Evaluate on the test data\n",
    "r2_test, rmse_test, mae_test, smape_test = evaluate_model(model, test_generator, y_test)\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Final Model Performance on Test Set\")\n",
    "print(\"=\"*80)\n",
    "print(f\"R²: {r2_test:.4f}\")\n",
    "print(f\"RMSE: {rmse_test:.4f}\")\n",
    "print(f\"MAE: {mae_test:.4f}\")\n",
    "print(f\"SMAPE: {smape_test:.4f}%\")\n",
    "\n",
    "# ==================== 7. Feature Importance Analysis ==================== #\n",
    "\n",
    "# --- Prepare data for importance functions (needs to be full numpy arrays) ---\n",
    "# Get all test data from the generator\n",
    "test_mlp_full = test_generator.mlp_data\n",
    "test_gnn_full = test_generator.gnn_data\n",
    "test_y_full = test_generator.y\n",
    "test_coords_full = test_generator.coords\n",
    "test_rasters_full = test_generator.get_raster_patches(test_coords_full)\n",
    "\n",
    "# --- Permutation Importance ---\n",
    "permutation_importance_scores = calculate_permutation_importance(\n",
    "    model, \n",
    "    test_mlp_full, \n",
    "    test_gnn_full, \n",
    "    test_rasters_full, \n",
    "    test_y_full, \n",
    "    numeric_cols, \n",
    "    raster_paths\n",
    ")\n",
    "print(\"\\n--- Summary of Permutation Importance ---\")\n",
    "sorted_perm_importance = sorted(permutation_importance_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, score in sorted_perm_importance:\n",
    "    print(f\"{feature}: {score:.4f}\")\n",
    "\n",
    "# --- Intrinsic Importance ---\n",
    "calculate_intrinsic_importance(model, numeric_cols, raster_paths)\n",
    "\n",
    "# --- LIME Importance ---\n",
    "# This can be computationally intensive, so it's run on a sample\n",
    "lime_importance_scores = calculate_lime_importance(\n",
    "    model, \n",
    "    test_mlp_full, \n",
    "    test_gnn_full, \n",
    "    test_rasters_full, \n",
    "    numeric_cols, \n",
    "    raster_paths\n",
    ")\n",
    "\n",
    "print(\"\\nAnalysis complete. All results are printed above.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
