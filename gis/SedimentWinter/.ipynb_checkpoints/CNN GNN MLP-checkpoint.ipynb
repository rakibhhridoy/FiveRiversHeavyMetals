{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c814a-fe3e-4dca-8b1d-044aa01fe5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "# Define the buffer size in meters\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "river_100.drop(columns=\"Source\", inplace=True)\n",
    "\n",
    "drop_cols = ['Stations', 'River', 'Lat', 'Long', 'geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# --- IMPUTATION FIX: Fill NaN values with 0 before further processing ---\n",
    "orig.fillna(0, inplace=True)\n",
    "river_100.fillna(0, inplace=True)\n",
    "\n",
    "# Train-test split\n",
    "np.random.seed(42)\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(f\"Using {len(raster_paths)} raster layers for CNN input.\")\n",
    "for r in raster_paths:\n",
    "    print(\"  -\", os.path.basename(r))\n",
    "\n",
    "# ==================== 3. Create a Custom Data Generator ==================== #\n",
    "def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):\n",
    "    \"\"\"\n",
    "    Extracts a batch of patches from rasters for a given set of coordinates.\n",
    "    This function is optimized to be called by the data generator for each batch.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    # Loop through each coordinate pair in the batch\n",
    "    for lon, lat in coords:\n",
    "        channels = []\n",
    "        # Loop through each raster file to get a single patch for each raster\n",
    "        for rfile in raster_files:\n",
    "            with rasterio.open(rfile) as src:\n",
    "                try:\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)\n",
    "                    arr = src.read(1, window=win, boundless=True, fill_value=0)\n",
    "                    arr = arr.astype(np.float32)\n",
    "\n",
    "                    # --- NORMALIZATION FIX: Add a small epsilon to avoid division by zero ---\n",
    "                    max_val = np.nanmax(arr)\n",
    "                    if max_val != 0:\n",
    "                        arr /= max_val + 1e-8 # Add epsilon for stability\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {rfile} for coordinates ({lon}, {lat}): {e}\")\n",
    "                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)\n",
    "            channels.append(arr)\n",
    "        patches.append(np.stack(channels, axis=-1))\n",
    "    \n",
    "    return np.array(patches)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, batch_size=4, shuffle=True, buffer_meters=BUFFER_METERS, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.coords = coords\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.raster_paths = raster_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.buffer_meters = buffer_meters\n",
    "\n",
    "        # Pre-calculate patch size from the first raster\n",
    "        with rasterio.open(raster_paths[0]) as src:\n",
    "            res_x, res_y = src.res\n",
    "            self.buffer_pixels_x = int(self.buffer_meters / res_x)\n",
    "            self.buffer_pixels_y = int(self.buffer_meters / res_y)\n",
    "            self.patch_width = 2 * self.buffer_pixels_x\n",
    "            self.patch_height = 2 * self.buffer_pixels_y\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        \n",
    "        # Slice the GNN adjacency matrix for the current batch\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "\n",
    "        batch_y = self.y[batch_indices]\n",
    "\n",
    "        # Extract CNN patches for the current batch\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            self.raster_paths,\n",
    "            self.buffer_pixels_x,\n",
    "            self.buffer_pixels_y,\n",
    "            self.patch_width,\n",
    "            self.patch_height\n",
    "        )\n",
    "\n",
    "        # Return a tuple of inputs and the target, which Keras expects\n",
    "        return (batch_cnn, batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #\n",
    "coords_train = train_combined[['Long', 'Lat']].values\n",
    "coords_test = test_orig[['Long', 'Lat']].values\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_test_train = distance_matrix(coords_test, coords_train)\n",
    "gnn_test = np.exp(-dist_mat_test_train/10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# --- IMPUTATION FIX: Fill NaN in raw MLP data before scaling ---\n",
    "train_combined.fillna(0, inplace=True)\n",
    "test_orig.fillna(0, inplace=True)\n",
    "mlp_train = scaler.fit_transform(train_combined[numeric_cols])\n",
    "mlp_test = scaler.transform(test_orig[numeric_cols])\n",
    "y_train = train_combined['RI'].values\n",
    "y_test = test_orig['RI'].values\n",
    "\n",
    "# ==================== 5. Define Enhanced CNN–GNN–MLP Model ==================== #\n",
    "def build_fusion_model(patch_shape, gnn_dim, mlp_dim):\n",
    "    # CNN branch (for raster data)\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    x = Conv2D(32, (3,3), activation=\"relu\")(cnn_input)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(64, (3,3), activation=\"relu\")(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Flatten()(x)\n",
    "    cnn_out = Dense(128, activation=\"relu\", name=\"cnn_out\")(x)\n",
    "\n",
    "    # MLP branch (for numerical site features)\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    m = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_out = Dense(32, activation=\"relu\", name=\"mlp_out\")(m)\n",
    "\n",
    "    # GNN branch (for spatial connectivity)\n",
    "    # The GNN input dimension is now the number of training samples\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    g = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    gnn_out = Dense(32, activation=\"relu\", name=\"gnn_out\")(g)\n",
    "\n",
    "    # Fusion Layer\n",
    "    combined = Concatenate()([cnn_out, mlp_out, gnn_out])\n",
    "    f = Dense(128, activation=\"relu\")(combined)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# We need to determine the final GNN input dimension for the model\n",
    "# It's the total number of training samples\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "\n",
    "# Helper function to get CNN patch shape from rasters\n",
    "def get_cnn_patch_shape(raster_paths, buffer_meters):\n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, _ = src.res\n",
    "        buffer_pixels = int(buffer_meters / res_x)\n",
    "        return (2 * buffer_pixels, 2 * buffer_pixels, len(raster_paths))\n",
    "\n",
    "cnn_patch_shape = get_cnn_patch_shape(raster_paths, BUFFER_METERS)\n",
    "model = build_fusion_model(cnn_patch_shape, gnn_input_dim, mlp_train.shape[1])\n",
    "model.summary()\n",
    "\n",
    "# ==================== 6. Create Data Generators ==================== #\n",
    "# We create a separate generator for the validation data.\n",
    "train_generator = DataGenerator(\n",
    "    coords=coords_train,\n",
    "    mlp_data=mlp_train,\n",
    "    gnn_data=gnn_train,\n",
    "    y=y_train,\n",
    "    raster_paths=raster_paths,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    buffer_meters=BUFFER_METERS\n",
    ")\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(model, coords_test, mlp_test, gnn_test_matrix, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=4, return_preds=False):\n",
    "    num_samples = len(y_test)\n",
    "    y_pred_list = []\n",
    "    \n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, res_y = src.res\n",
    "        buffer_pixels_x = int(buffer_meters / res_x)\n",
    "        buffer_pixels_y = int(buffer_meters / res_y)\n",
    "        patch_width = 2 * buffer_pixels_x\n",
    "        patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_coords = coords_test[i:i+batch_size]\n",
    "        batch_mlp = mlp_test[i:i+batch_size]\n",
    "        \n",
    "        batch_gnn = gnn_test_matrix[i:i+batch_size, :]\n",
    "        batch_y = y_test[i:i+batch_size]\n",
    "\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            raster_paths,\n",
    "            buffer_pixels_x,\n",
    "            buffer_pixels_y,\n",
    "            patch_width,\n",
    "            patch_height\n",
    "        )\n",
    "        \n",
    "        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn), verbose=0).flatten())\n",
    "    \n",
    "    y_pred = np.concatenate(y_pred_list)\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        # --- NaN FIX: Ensure y_pred has no NaNs before calculating metrics ---\n",
    "        y_pred[np.isnan(y_pred)] = 0\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return r2, rmse\n",
    "\n",
    "\n",
    "# ==================== 7. Train Model ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing with CNN–GNN–MLP Model ({BUFFER_METERS}m)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=train_generator # Using the same generator for validation for this example\n",
    ")\n",
    "\n",
    "\n",
    "# ==================== 8. Evaluate ==================== #\n",
    "# Re-create a data generator without shuffling for evaluation on the training set\n",
    "train_eval_generator = DataGenerator(\n",
    "    coords=coords_train,\n",
    "    mlp_data=mlp_train,\n",
    "    gnn_data=gnn_train,\n",
    "    y=y_train,\n",
    "    raster_paths=raster_paths,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    buffer_meters=BUFFER_METERS\n",
    ")\n",
    "\n",
    "y_pred_train = model.predict(train_eval_generator, verbose=0).flatten()\n",
    "# --- NaN FIX: Ensure y_pred has no NaNs before calculating metrics ---\n",
    "y_pred_train[np.isnan(y_pred_train)] = 0\n",
    "r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))\n",
    "\n",
    "r2_test, rmse_test = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size)\n",
    "\n",
    "print(f\"\\n✅ CNN–GNN–MLP Model Performance ({BUFFER_METERS}m):\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# ==================== 9. Feature Importance Analysis ==================== #\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(f\"Feature Importance Analysis for {BUFFER_METERS}m\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# --- 9.1 Combined Feature Importance (by Model Branch) ---\n",
    "y_pred_baseline = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, return_preds=True)\n",
    "y_pred_baseline[np.isnan(y_pred_baseline)] = 0\n",
    "baseline_r2 = r2_score(y_test, y_pred_baseline)\n",
    "print(f\"\\nBaseline Performance on Test Set: R² = {baseline_r2:.4f}\")\n",
    "\n",
    "# Ablate CNN branch\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    buffer_pixels_y = int(BUFFER_METERS / res_y)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "cnn_test_ablated = np.zeros_like(extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "))\n",
    "y_pred_cnn_ablated = model.predict((cnn_test_ablated, mlp_test, gnn_test), verbose=0).flatten()\n",
    "y_pred_cnn_ablated[np.isnan(y_pred_cnn_ablated)] = 0\n",
    "r2_cnn_ablated = r2_score(y_test, y_pred_cnn_ablated)\n",
    "importance_cnn = baseline_r2 - r2_cnn_ablated\n",
    "\n",
    "# Ablate MLP branch\n",
    "mlp_test_ablated = np.zeros_like(mlp_test)\n",
    "y_pred_mlp_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test_ablated, gnn_test), verbose=0).flatten()\n",
    "y_pred_mlp_ablated[np.isnan(y_pred_mlp_ablated)] = 0\n",
    "r2_mlp_ablated = r2_score(y_test, y_pred_mlp_ablated)\n",
    "importance_mlp = baseline_r2 - r2_mlp_ablated\n",
    "\n",
    "# Ablate GNN branch\n",
    "gnn_test_ablated = np.zeros_like(gnn_test)\n",
    "y_pred_gnn_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test, gnn_test_ablated), verbose=0).flatten()\n",
    "y_pred_gnn_ablated[np.isnan(y_pred_gnn_ablated)] = 0\n",
    "r2_gnn_ablated = r2_score(y_test, y_pred_gnn_ablated)\n",
    "importance_gnn = baseline_r2 - r2_gnn_ablated\n",
    "\n",
    "print(\"\\n--- Combined Feature Importance (by Model Branch) ---\")\n",
    "print(f\"CNN Branch Importance (R² drop): {importance_cnn:.4f}\")\n",
    "print(f\"MLP Branch Importance (R² drop): {importance_mlp:.4f}\")\n",
    "print(f\"GNN Branch Importance (R² drop): {importance_gnn:.4f}\")\n",
    "\n",
    "# --- 9.2 MLP Feature Importance (Permutation-based) ---\n",
    "mlp_feature_importance = {}\n",
    "mlp_data_test_raw = test_orig[numeric_cols]\n",
    "for i, feature_name in enumerate(mlp_data_test_raw.columns):\n",
    "    mlp_test_shuffled = np.copy(mlp_test)\n",
    "    np.random.shuffle(mlp_test_shuffled[:, i])\n",
    "    \n",
    "    y_pred_shuffled = model.predict((extract_patch_for_generator(\n",
    "        coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "    ), mlp_test_shuffled, gnn_test), verbose=0).flatten()\n",
    "    y_pred_shuffled[np.isnan(y_pred_shuffled)] = 0\n",
    "    r2_shuffled = r2_score(y_test, y_pred_shuffled)\n",
    "    \n",
    "    importance = baseline_r2 - r2_shuffled\n",
    "    mlp_feature_importance[feature_name] = importance\n",
    "\n",
    "print(\"\\n--- MLP Feature Importance (Permutation-based) ---\")\n",
    "sorted_importance = sorted(mlp_feature_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, importance in sorted_importance:\n",
    "    print(f\"{feature:<20}: {importance:.4f}\")\n",
    "    \n",
    "# ==================== 10. Save Model and Data for Reproducibility ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Saving Model, Data, and Feature Importance Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create the single output directory\n",
    "output_dir = \"cnn_gnn_mlp\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the trained model in the Keras native format\n",
    "model_filename = os.path.join(output_dir, f\"fusion_model_{BUFFER_METERS}m.keras\")\n",
    "model.save(model_filename)\n",
    "print(f\"✅ Model saved to '{model_filename}'\")\n",
    "\n",
    "# Save the training history using pickle\n",
    "history_filename = os.path.join(output_dir, \"training_history.pkl\")\n",
    "with open(history_filename, 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(f\"✅ Training history saved to '{history_filename}'\")\n",
    "\n",
    "# --- New: Save Feature Importance Results ---\n",
    "feature_importance_results = {\n",
    "    \"mlp_feature_names\": test_orig[numeric_cols].columns.tolist(),\n",
    "    \"mlp_permutation_importance\": mlp_feature_importance,\n",
    "    \"cnn_ablation_importance\": importance_cnn,\n",
    "    \"mlp_ablation_importance\": importance_mlp,\n",
    "    \"gnn_ablation_importance\": importance_gnn\n",
    "}\n",
    "importance_filename = os.path.join(output_dir, \"feature_importance.pkl\")\n",
    "with open(importance_filename, 'wb') as f:\n",
    "    pickle.dump(feature_importance_results, f)\n",
    "print(f\"✅ Feature importance results saved to '{importance_filename}'\")\n",
    "\n",
    "# Save processed NumPy arrays for later use\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, \"processed_train_data.npz\"),\n",
    "    coords=coords_train,\n",
    "    mlp=mlp_train,\n",
    "    y=y_train\n",
    ")\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, \"processed_test_data.npz\"),\n",
    "    coords=coords_test,\n",
    "    mlp=mlp_test,\n",
    "    y=y_test\n",
    ")\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, \"gnn_data.npz\"),\n",
    "    gnn_train=gnn_train,\n",
    "    gnn_test=gnn_test\n",
    ")\n",
    "print(f\"✅ Processed data arrays saved to '{output_dir}'\")\n",
    "\n",
    "# Save the raw dataframes to CSV for easy inspection\n",
    "train_combined.to_csv(os.path.join(output_dir, \"train_combined.csv\"), index=False)\n",
    "test_orig.to_csv(os.path.join(output_dir, \"test_orig.csv\"), index=False)\n",
    "print(f\"✅ Raw dataframes saved to '{output_dir}'\")\n",
    "\n",
    "# Garbage collect to free up memory\n",
    "del model, history, train_generator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f26d0d-f2f2-4242-a428-32d475ad1e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11acdece-73a9-44d8-8b23-99f12e3044cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474a1ce6-26f3-499f-b35f-c24e1d28d443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b7ae458-39dc-40dc-9f7e-0bd5c65839c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 26 raster layers for CNN input.\n",
      "  - bui.tif\n",
      "  - ndsi.tif\n",
      "  - savi.tif\n",
      "  - ndbsi.tif\n",
      "  - ui.tif\n",
      "  - ndwi.tif\n",
      "  - ndbi.tif\n",
      "  - awei.tif\n",
      "  - evi.tif\n",
      "  - mndwi.tif\n",
      "  - ndvi.tif\n",
      "  - LULC2020.tif\n",
      "  - LULC2021.tif\n",
      "  - LULC2022.tif\n",
      "  - LULC2019.tif\n",
      "  - LULC2018.tif\n",
      "  - LULC2017.tif\n",
      "  - Pb_R.tif\n",
      "  - ClayR.tif\n",
      "  - SandR.tif\n",
      "  - CdR.tif\n",
      "  - CrR.tif\n",
      "  - AsR.tif\n",
      "  - SiltR.tif\n",
      "  - CuR.tif\n",
      "  - NiR.tif\n",
      "\n",
      "================================================================================\n",
      "Starting 5-Fold Cross-Validation for CNN–GNN–MLP Model (500m)\n",
      "================================================================================\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 127ms/step - loss: 68701.5547 - val_loss: 30439.4023\n",
      "Fold 1 - R²: -8.7400, MAE: 145.8612, RMSE: 170.8749, SMAPE: 141.6365\n",
      "   -> New best model found in Fold 1\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - loss: 60682.4023 - val_loss: 34040.3047\n",
      "Fold 2 - R²: -3.9505, MAE: 156.6918, RMSE: 179.3137, SMAPE: 157.6509\n",
      "   -> New best model found in Fold 2\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - loss: 33567.2148 - val_loss: 25414.5938\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x35c736ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Fold 3 - R²: -4.6450, MAE: 112.8952, RMSE: 162.7255, SMAPE: 51.5630\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 127ms/step - loss: 70166.5859 - val_loss: 47194.7617\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x34b077ba0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Fold 4 - R²: -10.3340, MAE: 176.2224, RMSE: 221.5624, SMAPE: 135.0725\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 136ms/step - loss: 101234.5625 - val_loss: 33094.9180\n",
      "Fold 5 - R²: -4.9418, MAE: 154.4285, RMSE: 176.7625, SMAPE: 127.3928\n",
      "\n",
      "================================================================================\n",
      "Cross-Validation Complete\n",
      "Best model from Fold 2 with Validation R²: -3.9505\n",
      "Loading the best model for final evaluation.\n",
      "================================================================================\n",
      "\n",
      "✅ CNN–GNN–MLP Model Final Performance on Test Set (500m):\n",
      "R² Test: -6.9760 | MAE Test: 120.4564 | RMSE Test: 135.2141 | SMAPE Test: 132.9413\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import pickle\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "# Remove 'Source' column if it exists in river_100 dataframe\n",
    "if 'Source' in river_100.columns:\n",
    "    river_100.drop(columns=\"Source\", inplace=True)\n",
    "\n",
    "drop_cols = ['Stations', 'River', 'Lat', 'Long', 'geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# --- IMPUTATION FIX: Fill NaN values with 0 before further processing ---\n",
    "orig.fillna(0, inplace=True)\n",
    "river_100.fillna(0, inplace=True)\n",
    "\n",
    "# --- Use an 80/20 train-test split for a larger test set ---\n",
    "np.random.seed(42)\n",
    "train_orig, test_orig = train_test_split(orig, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine the river data with the new training set\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(f\"Using {len(raster_paths)} raster layers for CNN input.\")\n",
    "for r in raster_paths:\n",
    "    print(\"  -\", os.path.basename(r))\n",
    "\n",
    "# ==================== 3. Create a Custom Data Generator ==================== #\n",
    "def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):\n",
    "    \"\"\"\n",
    "    Extracts a batch of patches from rasters for a given set of coordinates.\n",
    "    This function is optimized to be called by the data generator for each batch.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    # Loop through each coordinate pair in the batch\n",
    "    for lon, lat in coords:\n",
    "        channels = []\n",
    "        # Loop through each raster file to get a single patch for each raster\n",
    "        for rfile in raster_files:\n",
    "            with rasterio.open(rfile) as src:\n",
    "                try:\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)\n",
    "                    arr = src.read(1, window=win, boundless=True, fill_value=0)\n",
    "                    arr = arr.astype(np.float32)\n",
    "\n",
    "                    # --- NORMALIZATION FIX: Add a small epsilon to avoid division by zero ---\n",
    "                    max_val = np.nanmax(arr)\n",
    "                    if max_val != 0:\n",
    "                        arr /= max_val + 1e-8 # Add epsilon for stability\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {rfile} for coordinates ({lon}, {lat}): {e}\")\n",
    "                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)\n",
    "            channels.append(arr)\n",
    "        patches.append(np.stack(channels, axis=-1))\n",
    "    \n",
    "    return np.array(patches)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.coords = coords\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.raster_paths = raster_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.buffer_meters = buffer_meters\n",
    "\n",
    "        # Pre-calculate patch size from the first raster\n",
    "        with rasterio.open(raster_paths[0]) as src:\n",
    "            res_x, res_y = src.res\n",
    "            self.buffer_pixels_x = int(self.buffer_meters / res_x)\n",
    "            self.buffer_pixels_y = int(self.buffer_meters / res_y)\n",
    "            self.patch_width = 2 * self.buffer_pixels_x\n",
    "            self.patch_height = 2 * self.buffer_pixels_y\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        \n",
    "        # Slice the GNN adjacency matrix for the current batch\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "\n",
    "        batch_y = self.y[batch_indices]\n",
    "\n",
    "        # Extract CNN patches for the current batch\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            self.raster_paths,\n",
    "            self.buffer_pixels_x,\n",
    "            self.buffer_pixels_y,\n",
    "            self.patch_width,\n",
    "            self.patch_height\n",
    "        )\n",
    "\n",
    "        # Return a tuple of inputs and the target, which Keras expects\n",
    "        return (batch_cnn, batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #\n",
    "coords_train = train_combined[['Long', 'Lat']].values\n",
    "coords_test = test_orig[['Long', 'Lat']].values\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_test_train = distance_matrix(coords_test, coords_train)\n",
    "gnn_test = np.exp(-dist_mat_test_train/10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# --- IMPUTATION FIX: Fill NaN in raw MLP data before scaling ---\n",
    "train_combined.fillna(0, inplace=True)\n",
    "test_orig.fillna(0, inplace=True)\n",
    "mlp_train = scaler.fit_transform(train_combined[numeric_cols])\n",
    "mlp_test = scaler.transform(test_orig[numeric_cols])\n",
    "y_train = train_combined['RI'].values\n",
    "y_test = test_orig['RI'].values\n",
    "\n",
    "# ==================== 5. Define Enhanced CNN–GNN–MLP Model ==================== #\n",
    "def build_fusion_model(patch_shape, gnn_dim, mlp_dim):\n",
    "    # CNN branch (for raster data)\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    x = Conv2D(32, (3,3), activation=\"relu\")(cnn_input)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(64, (3,3), activation=\"relu\")(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Flatten()(x)\n",
    "    cnn_out = Dense(128, activation=\"relu\", name=\"cnn_out\")(x)\n",
    "\n",
    "    # MLP branch (for numerical site features)\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    m = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_out = Dense(32, activation=\"relu\", name=\"mlp_out\")(m)\n",
    "\n",
    "    # GNN branch (for spatial connectivity)\n",
    "    # The GNN input dimension is now the number of training samples\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    g = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    gnn_out = Dense(32, activation=\"relu\", name=\"gnn_out\")(g)\n",
    "\n",
    "    # Fusion Layer\n",
    "    combined = Concatenate()([cnn_out, mlp_out, gnn_out])\n",
    "    f = Dense(128, activation=\"relu\")(combined)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# We need to determine the final GNN input dimension for the model\n",
    "# It's the total number of training samples\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "\n",
    "# Helper function to get CNN patch shape from rasters\n",
    "def get_cnn_patch_shape(raster_paths, buffer_meters):\n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, _ = src.res\n",
    "        buffer_pixels = int(buffer_meters / res_x)\n",
    "        return (2 * buffer_pixels, 2 * buffer_pixels, len(raster_paths))\n",
    "\n",
    "cnn_patch_shape = get_cnn_patch_shape(raster_paths, BUFFER_METERS)\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "    \"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    \n",
    "    # Handle the case where both y_true and y_pred are zero to avoid division by zero\n",
    "    return np.mean(numerator / (denominator + 1e-8)) * 100\n",
    "\n",
    "\n",
    "def evaluate_model(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=4, return_preds=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a given dataset and returns the metrics or predictions.\n",
    "    \"\"\"\n",
    "    num_samples = len(y_true)\n",
    "    y_pred_list = []\n",
    "    \n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, res_y = src.res\n",
    "        buffer_pixels_x = int(buffer_meters / res_x)\n",
    "        buffer_pixels_y = int(buffer_meters / res_y)\n",
    "        patch_width = 2 * buffer_pixels_x\n",
    "        patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_coords = coords[i:i+batch_size]\n",
    "        batch_mlp = mlp_data[i:i+batch_size]\n",
    "        batch_gnn = gnn_data[i:i+batch_size, :]\n",
    "        \n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            raster_paths,\n",
    "            buffer_pixels_x,\n",
    "            buffer_pixels_y,\n",
    "            patch_width,\n",
    "            patch_height\n",
    "        )\n",
    "        \n",
    "        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn), verbose=0).flatten())\n",
    "        \n",
    "    y_pred = np.concatenate(y_pred_list)\n",
    "    \n",
    "    # --- NaN FIX: Ensure y_pred has no NaNs before calculating metrics ---\n",
    "    y_pred[np.isnan(y_pred)] = 0\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        smap = smape(y_true, y_pred)\n",
    "        return r2, mae, rmse, smap\n",
    "\n",
    "\n",
    "# ==================== 6. K-Fold Cross-Validation and Model Saving ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Starting 5-Fold Cross-Validation for CNN–GNN–MLP Model ({BUFFER_METERS}m)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create the directory to save the models\n",
    "output_dir = \"models/cnn_gnn_mlp\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Combine all training data for K-Fold splitting\n",
    "all_coords = coords_train\n",
    "all_mlp = mlp_train\n",
    "all_gnn = gnn_train\n",
    "all_y = y_train\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "best_r2_val = -np.inf\n",
    "best_model = None\n",
    "best_fold = -1\n",
    "fold_num = 1\n",
    "\n",
    "for train_index, val_index in kf.split(all_y):\n",
    "    print(f\"\\n--- Fold {fold_num}/{kf.n_splits} ---\")\n",
    "    \n",
    "    # Split the data for the current fold\n",
    "    fold_train_coords, fold_val_coords = all_coords[train_index], all_coords[val_index]\n",
    "    fold_train_mlp, fold_val_mlp = all_mlp[train_index], all_mlp[val_index]\n",
    "    # GNN matrix slicing needs to be handled carefully. The adjacency matrix depends on the training data.\n",
    "    fold_train_gnn = all_gnn[train_index, :]\n",
    "    fold_val_gnn = all_gnn[val_index, :]\n",
    "    fold_train_y, fold_val_y = all_y[train_index], all_y[val_index]\n",
    "    \n",
    "    # Create generators for the current fold's data\n",
    "    fold_train_generator = DataGenerator(\n",
    "        coords=fold_train_coords,\n",
    "        mlp_data=fold_train_mlp,\n",
    "        gnn_data=fold_train_gnn,\n",
    "        y=fold_train_y,\n",
    "        raster_paths=raster_paths,\n",
    "        buffer_meters=BUFFER_METERS,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Validation generator for evaluation\n",
    "    fold_val_generator = DataGenerator(\n",
    "        coords=fold_val_coords,\n",
    "        mlp_data=fold_val_mlp,\n",
    "        gnn_data=fold_val_gnn,\n",
    "        y=fold_val_y,\n",
    "        raster_paths=raster_paths,\n",
    "        buffer_meters=BUFFER_METERS,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Build and compile a new model for each fold\n",
    "    model = build_fusion_model(cnn_patch_shape, gnn_input_dim, mlp_train.shape[1])\n",
    "    \n",
    "    # Define a unique filename for each fold's best model\n",
    "    checkpoint_filepath = os.path.join(output_dir, f'best_model_fold_{fold_num}.keras')\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=False,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True\n",
    "    )\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        fold_train_generator,\n",
    "        epochs=100,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping, model_checkpoint_callback],\n",
    "        validation_data=fold_val_generator\n",
    "    )\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    val_r2, val_mae, val_rmse, val_smape = evaluate_model(\n",
    "        model, \n",
    "        fold_val_coords, \n",
    "        fold_val_mlp, \n",
    "        fold_val_gnn, \n",
    "        fold_val_y, \n",
    "        raster_paths, \n",
    "        BUFFER_METERS, \n",
    "        batch_size\n",
    "    )\n",
    "\n",
    "    print(f\"Fold {fold_num} - R²: {val_r2:.4f}, MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}, SMAPE: {val_smape:.4f}\")\n",
    "    fold_results.append({\n",
    "        'fold': fold_num,\n",
    "        'val_r2': val_r2,\n",
    "        'val_mae': val_mae,\n",
    "        'val_rmse': val_rmse,\n",
    "        'val_smape': val_smape\n",
    "    })\n",
    "    \n",
    "    # Check if this fold produced the best model so far\n",
    "    if val_r2 > best_r2_val:\n",
    "        best_r2_val = val_r2\n",
    "        best_fold = fold_num\n",
    "        best_model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "        print(f\"   -> New best model found in Fold {fold_num}\")\n",
    "    \n",
    "    fold_num += 1\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Cross-Validation Complete\")\n",
    "print(f\"Best model from Fold {best_fold} with Validation R²: {best_r2_val:.4f}\")\n",
    "print(\"Loading the best model for final evaluation.\")\n",
    "model = best_model\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ==================== 7. Final Evaluation on Test Set ==================== #\n",
    "r2_test, mae_test, rmse_test, smape_test = evaluate_model(\n",
    "    model, \n",
    "    coords_test, \n",
    "    mlp_test, \n",
    "    gnn_test, \n",
    "    y_test, \n",
    "    raster_paths, \n",
    "    buffer_meters=BUFFER_METERS, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ CNN–GNN–MLP Model Final Performance on Test Set ({BUFFER_METERS}m):\")\n",
    "print(f\"R² Test: {r2_test:.4f} | MAE Test: {mae_test:.4f} | RMSE Test: {rmse_test:.4f} | SMAPE Test: {smape_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f69ad0-670b-4963-b1bf-e8375de9bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 8. Save Final Results ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Saving Training and Evaluation Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save the fold results and final metrics\n",
    "results_filename = os.path.join(output_dir, \"training_results.pkl\")\n",
    "final_metrics = {\n",
    "    'test_r2': r2_test,\n",
    "    'test_rmse': rmse_test,\n",
    "    'kfold_results': fold_results\n",
    "}\n",
    "with open(results_filename, 'wb') as f:\n",
    "    pickle.dump(final_metrics, f)\n",
    "print(f\"✅ Training results saved to '{results_filename}'\")\n",
    "\n",
    "# Save processed NumPy arrays for later use\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, \"processed_train_data.npz\"),\n",
    "    coords=coords_train,\n",
    "    mlp=mlp_train,\n",
    "    y=y_train\n",
    ")\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, \"processed_test_data.npz\"),\n",
    "    coords=coords_test,\n",
    "    mlp=mlp_test,\n",
    "    y=y_test\n",
    ")\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, \"gnn_data.npz\"),\n",
    "    gnn_train=gnn_train,\n",
    "    gnn_test=gnn_test\n",
    ")\n",
    "print(f\"✅ Processed data arrays saved to '{output_dir}'\")\n",
    "\n",
    "# Save the raw dataframes to CSV for easy inspection\n",
    "train_combined.to_csv(os.path.join(output_dir, \"train_combined.csv\"), index=False)\n",
    "test_orig.to_csv(os.path.join(output_dir, \"test_orig.csv\"), index=False)\n",
    "print(f\"✅ Raw dataframes saved to '{output_dir}'\")\n",
    "\n",
    "# Garbage collect to free up memory\n",
    "del model\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7866d7d-3ed4-440d-9b07-5295a285e9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
