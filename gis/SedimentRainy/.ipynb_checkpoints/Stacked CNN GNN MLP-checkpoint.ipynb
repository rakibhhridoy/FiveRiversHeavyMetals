{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f21c33a-355d-40bc-9bf4-0623e0d270be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Concatenate,\n",
    "    Dropout,\n",
    "    Layer,\n",
    "    Lambda,\n",
    "    GlobalAveragePooling2D,\n",
    "    Reshape,\n",
    "    Multiply\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import sys\n",
    "from io import StringIO\n",
    "import pickle\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "# NOTE: The data loading logic remains the same.\n",
    "# Replace with your actual data paths if needed\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# Train-test split\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(f\"Using {len(raster_paths)} raster layers for CNN input.\")\n",
    "for r in raster_paths:\n",
    "    print(\"  -\", os.path.basename(r))\n",
    "\n",
    "# ==================== 3. Create a Custom Data Generator ==================== #\n",
    "def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):\n",
    "    \"\"\"\n",
    "    Extracts a batch of patches from rasters for a given set of coordinates.\n",
    "    This function is optimized to be called by the data generator for each batch.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    # Loop through each coordinate pair in the batch\n",
    "    for lon, lat in coords:\n",
    "        channels = []\n",
    "        # Loop through each raster file to get a single patch for each raster\n",
    "        for rfile in raster_files:\n",
    "            with rasterio.open(rfile) as src:\n",
    "                try:\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)\n",
    "                    arr = src.read(1, window=win, boundless=True, fill_value=0)\n",
    "                    arr = arr.astype(np.float32)\n",
    "\n",
    "                    if np.nanmax(arr) != 0:\n",
    "                        arr /= np.nanmax(arr)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {rfile} for coordinates ({lon}, {lat}): {e}\")\n",
    "                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)\n",
    "            channels.append(arr)\n",
    "        patches.append(np.stack(channels, axis=-1))\n",
    "    \n",
    "    return np.array(patches)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.coords = coords\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.raster_paths = raster_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.buffer_meters = buffer_meters\n",
    "\n",
    "        # Pre-calculate patch size from the first raster\n",
    "        with rasterio.open(raster_paths[0]) as src:\n",
    "            res_x, res_y = src.res\n",
    "            self.buffer_pixels_x = int(self.buffer_meters / res_x)\n",
    "            self.buffer_pixels_y = int(self.buffer_meters / res_y)\n",
    "            self.patch_width = 2 * self.buffer_pixels_x\n",
    "            self.patch_height = 2 * self.buffer_pixels_y\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "        batch_y = self.y[batch_indices]\n",
    "\n",
    "        # Extract CNN patches for the current batch\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            self.raster_paths,\n",
    "            self.buffer_pixels_x,\n",
    "            self.buffer_pixels_y,\n",
    "            self.patch_width,\n",
    "            self.patch_height\n",
    "        )\n",
    "\n",
    "        return (batch_cnn, batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #\n",
    "coords_train = train_combined[['Long','Lat']].values\n",
    "coords_test = test_orig[['Long','Lat']].values\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_test_train = distance_matrix(coords_test, coords_train)\n",
    "gnn_test = np.exp(-dist_mat_test_train/10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mlp_train = scaler.fit_transform(train_combined[numeric_cols])\n",
    "mlp_test = scaler.transform(test_orig[numeric_cols])\n",
    "y_train = train_combined['RI'].values\n",
    "y_test = test_orig['RI'].values\n",
    "\n",
    "# ==================== 5. Define Base Models ==================== #\n",
    "def build_cnn_mlp_model(patch_shape, mlp_dim):\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "\n",
    "    # CNN branch\n",
    "    cnn_branch = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_branch = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(cnn_branch)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_embedding = Flatten()(cnn_branch)\n",
    "\n",
    "    # MLP branch\n",
    "    mlp_embedding = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(32, activation=\"relu\")(mlp_embedding)\n",
    "\n",
    "    # Combine\n",
    "    combined = Concatenate()([cnn_embedding, mlp_embedding])\n",
    "    f = Dense(128, activation=\"relu\")(combined)\n",
    "    output = Dense(1, activation=\"linear\", name=\"cnn_mlp_output\")(f)\n",
    "    \n",
    "    model = Model(inputs=[cnn_input, mlp_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def build_gnn_mlp_model(gnn_dim, mlp_dim):\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "\n",
    "    # GNN branch\n",
    "    gnn_embedding = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    gnn_embedding = Dense(32, activation=\"relu\")(gnn_embedding)\n",
    "\n",
    "    # MLP branch\n",
    "    mlp_embedding = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(32, activation=\"relu\")(mlp_embedding)\n",
    "\n",
    "    # Combine\n",
    "    combined = Concatenate()([gnn_embedding, mlp_embedding])\n",
    "    f = Dense(64, activation=\"relu\")(combined)\n",
    "    output = Dense(1, activation=\"linear\", name=\"gnn_mlp_output\")(f)\n",
    "    \n",
    "    model = Model(inputs=[gnn_input, mlp_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def build_cnn_gnn_model(patch_shape, gnn_dim):\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "\n",
    "    # CNN branch\n",
    "    cnn_branch = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_branch = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(cnn_branch)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_embedding = Flatten()(cnn_branch)\n",
    "    \n",
    "    # GNN branch\n",
    "    gnn_embedding = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    gnn_embedding = Dense(32, activation=\"relu\")(gnn_embedding)\n",
    "\n",
    "    # Combine\n",
    "    combined = Concatenate()([cnn_embedding, gnn_embedding])\n",
    "    f = Dense(128, activation=\"relu\")(combined)\n",
    "    output = Dense(1, activation=\"linear\", name=\"cnn_gnn_output\")(f)\n",
    "    \n",
    "    model = Model(inputs=[cnn_input, gnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def build_meta_learner_model():\n",
    "    # Takes predictions from the 3 base models as input\n",
    "    pred1_input = Input(shape=(1,), name=\"pred1_input\")\n",
    "    pred2_input = Input(shape=(1,), name=\"pred2_input\")\n",
    "    pred3_input = Input(shape=(1,), name=\"pred3_input\")\n",
    "\n",
    "    # Concatenate the predictions\n",
    "    combined = Concatenate()([pred1_input, pred2_input, pred3_input])\n",
    "    \n",
    "    # Simple MLP as the meta-learner\n",
    "    f = Dense(32, activation=\"relu\")(combined)\n",
    "    f = Dense(16, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "    \n",
    "    model = Model(inputs=[pred1_input, pred2_input, pred3_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# ==================== 6. Create Data Generators for Base Models ==================== #\n",
    "# NOTE: We create generators that provide only the necessary inputs for each base model.\n",
    "class CNNDropoutGenerator(DataGenerator):\n",
    "    def __getitem__(self, index):\n",
    "        (batch_cnn, batch_mlp, batch_gnn), batch_y = super().__getitem__(index)\n",
    "        return (batch_cnn, batch_mlp), batch_y\n",
    "\n",
    "class GNNDropoutGenerator(DataGenerator):\n",
    "    def __getitem__(self, index):\n",
    "        (batch_cnn, batch_mlp, batch_gnn), batch_y = super().__getitem__(index)\n",
    "        return (batch_gnn, batch_mlp), batch_y\n",
    "\n",
    "class MLPDropoutGenerator(DataGenerator):\n",
    "    def __getitem__(self, index):\n",
    "        (batch_cnn, batch_mlp, batch_gnn), batch_y = super().__getitem__(index)\n",
    "        return (batch_cnn, batch_gnn), batch_y\n",
    "\n",
    "def get_base_model_predictions(model, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size):\n",
    "    num_samples = len(y)\n",
    "    y_pred_list = []\n",
    "    \n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, res_y = src.res\n",
    "        buffer_pixels_x = int(buffer_meters / res_x)\n",
    "        buffer_pixels_y = int(buffer_meters / res_y)\n",
    "        patch_width = 2 * buffer_pixels_x\n",
    "        patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_coords = coords[i:i+batch_size]\n",
    "        batch_mlp = mlp_data[i:i+batch_size]\n",
    "        batch_gnn = gnn_data[i:i+batch_size, :]\n",
    "        \n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "        )\n",
    "        \n",
    "        # Check which inputs the model expects and provide them\n",
    "        input_names = [inp.name for inp in model.inputs]\n",
    "        input_dict = {}\n",
    "        if 'cnn_input' in input_names:\n",
    "            input_dict['cnn_input'] = batch_cnn\n",
    "        if 'mlp_input' in input_names:\n",
    "            input_dict['mlp_input'] = batch_mlp\n",
    "        if 'gnn_input' in input_names:\n",
    "            input_dict['gnn_input'] = batch_gnn\n",
    "            \n",
    "        y_pred_list.append(model.predict(input_dict).flatten())\n",
    "            \n",
    "    return np.concatenate(y_pred_list)\n",
    "\n",
    "\n",
    "# ==================== Run the Analysis ==================== #\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing Stacked Deep Ensemble for BUFFER_METERS = {BUFFER_METERS}m\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "\n",
    "# Calculate CNN patch shape based on the current buffer size\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    cnn_patch_shape = (patch_width, patch_width, len(raster_paths))\n",
    "\n",
    "mlp_input_dim = mlp_train.shape[1]\n",
    "\n",
    "# --- Train Base Models ---\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- Training CNN-MLP Base Model ---\")\n",
    "cnn_mlp_model = build_cnn_mlp_model(cnn_patch_shape, mlp_input_dim)\n",
    "cnn_mlp_train_gen = CNNDropoutGenerator(\n",
    "    coords=coords_train, mlp_data=mlp_train, gnn_data=gnn_train, y=y_train,\n",
    "    raster_paths=raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "cnn_mlp_model.fit(cnn_mlp_train_gen, epochs=100, verbose=1, callbacks=[early_stopping], validation_data=cnn_mlp_train_gen)\n",
    "\n",
    "print(\"\\n--- Training GNN-MLP Base Model ---\")\n",
    "gnn_mlp_model = build_gnn_mlp_model(gnn_input_dim, mlp_input_dim)\n",
    "gnn_mlp_train_gen = GNNDropoutGenerator(\n",
    "    coords=coords_train, mlp_data=mlp_train, gnn_data=gnn_train, y=y_train,\n",
    "    raster_paths=raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "gnn_mlp_model.fit(gnn_mlp_train_gen, epochs=100, verbose=1, callbacks=[early_stopping], validation_data=gnn_mlp_train_gen)\n",
    "\n",
    "print(\"\\n--- Training CNN-GNN Base Model ---\")\n",
    "cnn_gnn_model = build_cnn_gnn_model(cnn_patch_shape, gnn_input_dim)\n",
    "cnn_gnn_train_gen = MLPDropoutGenerator(\n",
    "    coords=coords_train, mlp_data=mlp_train, gnn_data=gnn_train, y=y_train,\n",
    "    raster_paths=raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "cnn_gnn_model.fit(cnn_gnn_train_gen, epochs=100, verbose=1, callbacks=[early_stopping], validation_data=cnn_gnn_train_gen)\n",
    "\n",
    "# --- Generate predictions for meta-learner ---\n",
    "# Get predictions from base models on training data\n",
    "preds1_train = get_base_model_predictions(cnn_mlp_model, coords_train, mlp_train, gnn_train, y_train, raster_paths, BUFFER_METERS, batch_size)\n",
    "preds2_train = get_base_model_predictions(gnn_mlp_model, coords_train, mlp_train, gnn_train, y_train, raster_paths, BUFFER_METERS, batch_size)\n",
    "preds3_train = get_base_model_predictions(cnn_gnn_model, coords_train, mlp_train, gnn_train, y_train, raster_paths, BUFFER_METERS, batch_size)\n",
    "\n",
    "meta_train_inputs = (preds1_train.reshape(-1, 1), preds2_train.reshape(-1, 1), preds3_train.reshape(-1, 1))\n",
    "\n",
    "# --- Train Meta-Learner ---\n",
    "print(\"\\n--- Training Meta-Learner Model ---\")\n",
    "meta_model = build_meta_learner_model()\n",
    "meta_model.fit(meta_train_inputs, y_train, epochs=100, verbose=1, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "# --- Get predictions from base models on test data ---\n",
    "preds1_test = get_base_model_predictions(cnn_mlp_model, coords_test, mlp_test, gnn_test, y_test, raster_paths, BUFFER_METERS, batch_size)\n",
    "preds2_test = get_base_model_predictions(gnn_mlp_model, coords_test, mlp_test, gnn_test, y_test, raster_paths, BUFFER_METERS, batch_size)\n",
    "preds3_test = get_base_model_predictions(cnn_gnn_model, coords_test, mlp_test, gnn_test, y_test, raster_paths, BUFFER_METERS, batch_size)\n",
    "\n",
    "meta_test_inputs = (preds1_test.reshape(-1, 1), preds2_test.reshape(-1, 1), preds3_test.reshape(-1, 1))\n",
    "\n",
    "# --- Evaluate with Meta-Learner ---\n",
    "y_pred = meta_model.predict(meta_test_inputs).flatten()\n",
    "r2_test = r2_score(y_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"\\n Stacked Deep Ensemble Model Performance ({BUFFER_METERS}m):\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# --- NEW: Feature Importance for Meta-Learner ---\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(f\"Meta-Learner Feature Importance (Permutation-based)\")\n",
    "print(\"-\"*50)\n",
    "baseline_r2 = r2_test\n",
    "\n",
    "# Importance for CNN-MLP predictions\n",
    "preds1_test_shuffled = np.copy(preds1_test)\n",
    "np.random.shuffle(preds1_test_shuffled)\n",
    "shuffled_test_inputs = (preds1_test_shuffled.reshape(-1, 1), preds2_test.reshape(-1, 1), preds3_test.reshape(-1, 1))\n",
    "y_pred_shuffled = meta_model.predict(shuffled_test_inputs).flatten()\n",
    "r2_shuffled = r2_score(y_test, y_pred_shuffled)\n",
    "importance_cnn_mlp = baseline_r2 - r2_shuffled\n",
    "print(f\"Importance of CNN-MLP predictions (R² drop): {importance_cnn_mlp:.4f}\")\n",
    "\n",
    "# Importance for GNN-MLP predictions\n",
    "preds2_test_shuffled = np.copy(preds2_test)\n",
    "np.random.shuffle(preds2_test_shuffled)\n",
    "shuffled_test_inputs = (preds1_test.reshape(-1, 1), preds2_test_shuffled.reshape(-1, 1), preds3_test.reshape(-1, 1))\n",
    "y_pred_shuffled = meta_model.predict(shuffled_test_inputs).flatten()\n",
    "r2_shuffled = r2_score(y_test, y_pred_shuffled)\n",
    "importance_gnn_mlp = baseline_r2 - r2_shuffled\n",
    "print(f\"Importance of GNN-MLP predictions (R² drop): {importance_gnn_mlp:.4f}\")\n",
    "\n",
    "# Importance for CNN-GNN predictions\n",
    "preds3_test_shuffled = np.copy(preds3_test)\n",
    "np.random.shuffle(preds3_test_shuffled)\n",
    "shuffled_test_inputs = (preds1_test.reshape(-1, 1), preds2_test.reshape(-1, 1), preds3_test_shuffled.reshape(-1, 1))\n",
    "y_pred_shuffled = meta_model.predict(shuffled_test_inputs).flatten()\n",
    "r2_shuffled = r2_score(y_test, y_pred_shuffled)\n",
    "importance_cnn_gnn = baseline_r2 - r2_shuffled\n",
    "print(f\"Importance of CNN-GNN predictions (R² drop): {importance_cnn_gnn:.4f}\")\n",
    "\n",
    "\n",
    "# Save the feature importance results\n",
    "feature_importance = {\n",
    "    \"CNN-MLP_importance\": importance_cnn_mlp,\n",
    "    \"GNN-MLP_importance\": importance_gnn_mlp,\n",
    "    \"CNN-GNN_importance\": importance_cnn_gnn\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce5c72-a495-4f20-a5c9-2293eff669e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b3205d-9013-4492-b6d2-0f08679068bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751eb3eb-c9ec-4dea-9aed-cd5611beb3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00eccd56-d040-4597-ad75-baf2da1b12b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 26 raster layers for CNN input.\n",
      "  - bui.tif\n",
      "  - ndsi.tif\n",
      "  - savi.tif\n",
      "  - ndbsi.tif\n",
      "  - ui.tif\n",
      "  - ndwi.tif\n",
      "  - ndbi.tif\n",
      "  - awei.tif\n",
      "  - evi.tif\n",
      "  - mndwi.tif\n",
      "  - ndvi.tif\n",
      "  - LULC2020.tif\n",
      "  - LULC2021.tif\n",
      "  - LULC2022.tif\n",
      "  - LULC2019.tif\n",
      "  - LULC2018.tif\n",
      "  - LULC2017.tif\n",
      "  - Pb_R.tif\n",
      "  - ClayR.tif\n",
      "  - SandR.tif\n",
      "  - CdR.tif\n",
      "  - CrR.tif\n",
      "  - AsR.tif\n",
      "  - SiltR.tif\n",
      "  - CuR.tif\n",
      "  - NiR.tif\n",
      "\n",
      "================================================================================\n",
      "Analyzing Stacked Deep Ensemble with 5-Fold Cross-Validation for BUFFER_METERS = 500m\n",
      "================================================================================\n",
      "\n",
      "--- Starting Fold 1/5 ---\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 189ms/step - loss: nan - val_loss: nan\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: nan - val_loss: nan\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 188ms/step - loss: 41421.8867 - val_loss: 85178.5625\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 376\u001b[0m\n\u001b[1;32m    373\u001b[0m cnn_gnn_model\u001b[38;5;241m.\u001b[39mfit(cnn_gnn_train_gen, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping], validation_data\u001b[38;5;241m=\u001b[39mcnn_gnn_train_gen)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# --- Generate predictions for meta-learner ---\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m preds1_train \u001b[38;5;241m=\u001b[39m get_model_predictions(cnn_mlp_model, coords_train, mlp_train_scaled, gnn_train, raster_paths, BUFFER_METERS, batch_size)\n\u001b[1;32m    377\u001b[0m preds2_train \u001b[38;5;241m=\u001b[39m get_model_predictions(gnn_mlp_model, coords_train, mlp_train_scaled, gnn_train, raster_paths, BUFFER_METERS, batch_size)\n\u001b[1;32m    378\u001b[0m preds3_train \u001b[38;5;241m=\u001b[39m get_model_predictions(cnn_gnn_model, coords_train, mlp_train_scaled, gnn_train, raster_paths, BUFFER_METERS, batch_size)\n",
      "Cell \u001b[0;32mIn[6], line 281\u001b[0m, in \u001b[0;36mget_model_predictions\u001b[0;34m(model, data_coords, data_mlp, data_gnn, raster_paths, buffer_meters, batch_size)\u001b[0m\n\u001b[1;32m    278\u001b[0m batch_mlp \u001b[38;5;241m=\u001b[39m data_mlp[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m    279\u001b[0m batch_gnn \u001b[38;5;241m=\u001b[39m data_gnn[i:i\u001b[38;5;241m+\u001b[39mbatch_size, :]\n\u001b[0;32m--> 281\u001b[0m batch_cnn \u001b[38;5;241m=\u001b[39m extract_patch_for_generator(\n\u001b[1;32m    282\u001b[0m     batch_coords, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# Check which inputs the model expects and provide them\u001b[39;00m\n\u001b[1;32m    286\u001b[0m input_names \u001b[38;5;241m=\u001b[39m [inp\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39minputs]\n",
      "Cell \u001b[0;32mIn[6], line 93\u001b[0m, in \u001b[0;36mextract_patch_for_generator\u001b[0;34m(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height)\u001b[0m\n\u001b[1;32m     91\u001b[0m row, col \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mindex(lon, lat)\n\u001b[1;32m     92\u001b[0m win \u001b[38;5;241m=\u001b[39m Window(col \u001b[38;5;241m-\u001b[39m buffer_pixels_x, row \u001b[38;5;241m-\u001b[39m buffer_pixels_y, patch_width, patch_height)\n\u001b[0;32m---> 93\u001b[0m arr \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m1\u001b[39m, window\u001b[38;5;241m=\u001b[39mwin, boundless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     94\u001b[0m arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnanmax(arr) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32mrasterio/_io.pyx:678\u001b[0m, in \u001b[0;36mrasterio._io.DatasetReaderBase.read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/rasterio/vrt.py:260\u001b[0m, in \u001b[0;36m_boundless_vrt_doc\u001b[0;34m(src_dataset, nodata, background, hidenodata, width, height, transform, masked, resampling)\u001b[0m\n\u001b[1;32m    257\u001b[0m     dstrect\u001b[38;5;241m.\u001b[39mattrib[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxSize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(src_dataset\u001b[38;5;241m.\u001b[39mwidth)\n\u001b[1;32m    258\u001b[0m     dstrect\u001b[38;5;241m.\u001b[39mattrib[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mySize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(src_dataset\u001b[38;5;241m.\u001b[39mheight)\n\u001b[0;32m--> 260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ET\u001b[38;5;241m.\u001b[39mtostring(vrtdataset)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/xml/etree/ElementTree.py:1098\u001b[0m, in \u001b[0;36mtostring\u001b[0;34m(element, encoding, method, xml_declaration, default_namespace, short_empty_elements)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate string representation of XML element.\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m \n\u001b[1;32m   1086\u001b[0m \u001b[38;5;124;03mAll subelements are included.  If encoding is \"unicode\", a string\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \n\u001b[1;32m   1096\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m stream \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mStringIO() \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124municode\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m-> 1098\u001b[0m ElementTree(element)\u001b[38;5;241m.\u001b[39mwrite(stream, encoding,\n\u001b[1;32m   1099\u001b[0m                            xml_declaration\u001b[38;5;241m=\u001b[39mxml_declaration,\n\u001b[1;32m   1100\u001b[0m                            default_namespace\u001b[38;5;241m=\u001b[39mdefault_namespace,\n\u001b[1;32m   1101\u001b[0m                            method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m   1102\u001b[0m                            short_empty_elements\u001b[38;5;241m=\u001b[39mshort_empty_elements)\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/xml/etree/ElementTree.py:743\u001b[0m, in \u001b[0;36mElementTree.write\u001b[0;34m(self, file_or_filename, encoding, xml_declaration, default_namespace, method, short_empty_elements)\u001b[0m\n\u001b[1;32m    741\u001b[0m qnames, namespaces \u001b[38;5;241m=\u001b[39m _namespaces(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root, default_namespace)\n\u001b[1;32m    742\u001b[0m serialize \u001b[38;5;241m=\u001b[39m _serialize[method]\n\u001b[0;32m--> 743\u001b[0m serialize(write, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root, qnames, namespaces,\n\u001b[1;32m    744\u001b[0m           short_empty_elements\u001b[38;5;241m=\u001b[39mshort_empty_elements)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/xml/etree/ElementTree.py:906\u001b[0m, in \u001b[0;36m_serialize_xml\u001b[0;34m(write, elem, qnames, namespaces, short_empty_elements, **kwargs)\u001b[0m\n\u001b[1;32m    904\u001b[0m         write(_escape_cdata(text))\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m elem:\n\u001b[0;32m--> 906\u001b[0m         _serialize_xml(write, e, qnames, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    907\u001b[0m                        short_empty_elements\u001b[38;5;241m=\u001b[39mshort_empty_elements)\n\u001b[1;32m    908\u001b[0m     write(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m tag \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/xml/etree/ElementTree.py:900\u001b[0m, in \u001b[0;36m_serialize_xml\u001b[0;34m(write, elem, qnames, namespaces, short_empty_elements, **kwargs)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    899\u001b[0m             v \u001b[38;5;241m=\u001b[39m _escape_attrib(v)\n\u001b[0;32m--> 900\u001b[0m         write(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (qnames[k], v))\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(elem) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m short_empty_elements:\n\u001b[1;32m    902\u001b[0m     write(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<frozen codecs>:276\u001b[0m, in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Concatenate,\n",
    "    Layer,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import sys\n",
    "from io import StringIO\n",
    "import pickle\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "# NOTE: The data loading logic remains the same.\n",
    "# Replace with your actual data paths if needed\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "\n",
    "# Combine all data for cross-validation purposes\n",
    "all_data = pd.concat([river_100, orig], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry', \"Source\"]\n",
    "numeric_cols = all_data.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# Separate features and target for the full dataset\n",
    "coords_all = all_data[['Long','Lat']].values\n",
    "mlp_all = all_data[numeric_cols].values\n",
    "y_all = all_data['RI'].values\n",
    "gnn_all = distance_matrix(coords_all, coords_all)\n",
    "gnn_all = np.exp(-gnn_all/10)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(f\"Using {len(raster_paths)} raster layers for CNN input.\")\n",
    "for r in raster_paths:\n",
    "    print(\"  -\", os.path.basename(r))\n",
    "\n",
    "# ==================== 3. Define Metric Functions ==================== #\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Symmetric Mean Absolute Percentage Error (SMAPE).\n",
    "    \"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    \n",
    "    # Handle the case where denominator is zero to avoid division by zero errors\n",
    "    # Add a small epsilon to the denominator for robustness\n",
    "    mask = denominator == 0\n",
    "    denominator[mask] = 1e-8\n",
    "    \n",
    "    return np.mean(numerator / denominator) * 100\n",
    "\n",
    "# ==================== 4. Create a Custom Data Generator ==================== #\n",
    "def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):\n",
    "    \"\"\"\n",
    "    Extracts a batch of patches from rasters for a given set of coordinates.\n",
    "    This function is optimized to be called by the data generator for each batch.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    # Loop through each coordinate pair in the batch\n",
    "    for lon, lat in coords:\n",
    "        channels = []\n",
    "        # Loop through each raster file to get a single patch for each raster\n",
    "        for rfile in raster_files:\n",
    "            with rasterio.open(rfile) as src:\n",
    "                try:\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)\n",
    "                    arr = src.read(1, window=win, boundless=True, fill_value=0)\n",
    "                    arr = arr.astype(np.float32)\n",
    "\n",
    "                    if np.nanmax(arr) != 0:\n",
    "                        arr /= np.nanmax(arr)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {rfile} for coordinates ({lon}, {lat}): {e}\")\n",
    "                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)\n",
    "            channels.append(arr)\n",
    "        patches.append(np.stack(channels, axis=-1))\n",
    "    \n",
    "    return np.array(patches)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    # Added 'model_inputs' to specify which data types the generator should return\n",
    "    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, model_inputs, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.coords = coords\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.raster_paths = raster_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.buffer_meters = buffer_meters\n",
    "        # Store the list of inputs the model expects\n",
    "        self.model_inputs = model_inputs\n",
    "\n",
    "        # Pre-calculate patch size from the first raster\n",
    "        with rasterio.open(raster_paths[0]) as src:\n",
    "            res_x, res_y = src.res\n",
    "            self.buffer_pixels_x = int(self.buffer_meters / res_x)\n",
    "            self.buffer_pixels_y = int(self.buffer_meters / res_y)\n",
    "            self.patch_width = 2 * self.buffer_pixels_x\n",
    "            self.patch_height = 2 * self.buffer_pixels_y\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        batch_y = self.y[batch_indices]\n",
    "        \n",
    "        # New logic: Prepare the inputs as a dictionary based on 'model_inputs'\n",
    "        inputs = {}\n",
    "        if 'cnn' in self.model_inputs:\n",
    "            batch_cnn = extract_patch_for_generator(\n",
    "                batch_coords,\n",
    "                self.raster_paths,\n",
    "                self.buffer_pixels_x,\n",
    "                self.buffer_pixels_y,\n",
    "                self.patch_width,\n",
    "                self.patch_height\n",
    "            )\n",
    "            inputs['cnn_input'] = batch_cnn\n",
    "        \n",
    "        if 'mlp' in self.model_inputs:\n",
    "            batch_mlp = self.mlp_data[batch_indices]\n",
    "            inputs['mlp_input'] = batch_mlp\n",
    "\n",
    "        if 'gnn' in self.model_inputs:\n",
    "            batch_gnn = self.gnn_data[batch_indices, :]\n",
    "            inputs['gnn_input'] = batch_gnn\n",
    "\n",
    "        return inputs, batch_y\n",
    "\n",
    "# ==================== 5. Define Base Models ==================== #\n",
    "# Reduced learning rate and added gradient clipping to prevent NaN loss\n",
    "LEARNING_RATE = 0.0001\n",
    "def build_cnn_mlp_model(patch_shape, mlp_dim):\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "\n",
    "    # CNN branch\n",
    "    cnn_branch = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_branch = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(cnn_branch)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_embedding = Flatten()(cnn_branch)\n",
    "\n",
    "    # MLP branch\n",
    "    mlp_embedding = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(32, activation=\"relu\")(mlp_embedding)\n",
    "\n",
    "    # Combine\n",
    "    combined = Concatenate()([cnn_embedding, mlp_embedding])\n",
    "    f = Dense(128, activation=\"relu\")(combined)\n",
    "    output = Dense(1, activation=\"linear\", name=\"cnn_mlp_output\")(f)\n",
    "    \n",
    "    model = Model(inputs=[cnn_input, mlp_input], outputs=output)\n",
    "    # Added clipnorm to the Adam optimizer\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE, clipnorm=1.0), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def build_gnn_mlp_model(gnn_dim, mlp_dim):\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "\n",
    "    # GNN branch\n",
    "    gnn_embedding = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    gnn_embedding = Dense(32, activation=\"relu\")(gnn_embedding)\n",
    "\n",
    "    # MLP branch\n",
    "    mlp_embedding = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(32, activation=\"relu\")(mlp_embedding)\n",
    "\n",
    "    # Combine\n",
    "    combined = Concatenate()([gnn_embedding, mlp_embedding])\n",
    "    f = Dense(64, activation=\"relu\")(combined)\n",
    "    output = Dense(1, activation=\"linear\", name=\"gnn_mlp_output\")(f)\n",
    "    \n",
    "    model = Model(inputs=[gnn_input, mlp_input], outputs=output)\n",
    "    # Added clipnorm to the Adam optimizer\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE, clipnorm=1.0), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def build_cnn_gnn_model(patch_shape, gnn_dim):\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "\n",
    "    # CNN branch\n",
    "    cnn_branch = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_branch = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(cnn_branch)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_embedding = Flatten()(cnn_branch)\n",
    "    \n",
    "    # GNN branch\n",
    "    gnn_embedding = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    gnn_embedding = Dense(32, activation=\"relu\")(gnn_input)\n",
    "\n",
    "    # Combine\n",
    "    combined = Concatenate()([cnn_embedding, gnn_embedding])\n",
    "    f = Dense(128, activation=\"relu\")(combined)\n",
    "    output = Dense(1, activation=\"linear\", name=\"cnn_gnn_output\")(f)\n",
    "    \n",
    "    model = Model(inputs=[cnn_input, gnn_input], outputs=output)\n",
    "    # Added clipnorm to the Adam optimizer\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE, clipnorm=1.0), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def build_meta_learner_model():\n",
    "    # Takes predictions from the 3 base models as input\n",
    "    pred1_input = Input(shape=(1,), name=\"pred1_input\")\n",
    "    pred2_input = Input(shape=(1,), name=\"pred2_input\")\n",
    "    pred3_input = Input(shape=(1,), name=\"pred3_input\")\n",
    "\n",
    "    # Concatenate the predictions\n",
    "    combined = Concatenate()([pred1_input, pred2_input, pred3_input])\n",
    "    \n",
    "    # Simple MLP as the meta-learner\n",
    "    f = Dense(32, activation=\"relu\")(combined)\n",
    "    f = Dense(16, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "    \n",
    "    model = Model(inputs=[pred1_input, pred2_input, pred3_input], outputs=output)\n",
    "    # Added clipnorm to the Adam optimizer\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE, clipnorm=1.0), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# ==================== 6. Helper Function for Predictions ==================== #\n",
    "def get_model_predictions(model, data_coords, data_mlp, data_gnn, raster_paths, buffer_meters, batch_size):\n",
    "    num_samples = len(data_coords)\n",
    "    y_pred_list = []\n",
    "    \n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, res_y = src.res\n",
    "        buffer_pixels_x = int(buffer_meters / res_x)\n",
    "        buffer_pixels_y = int(buffer_meters / res_y)\n",
    "        patch_width = 2 * buffer_pixels_x\n",
    "        patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_coords = data_coords[i:i+batch_size]\n",
    "        batch_mlp = data_mlp[i:i+batch_size]\n",
    "        batch_gnn = data_gnn[i:i+batch_size, :]\n",
    "        \n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "        )\n",
    "        \n",
    "        # Check which inputs the model expects and provide them\n",
    "        input_names = [inp.name for inp in model.inputs]\n",
    "        input_dict = {}\n",
    "        if 'cnn_input' in input_names:\n",
    "            input_dict['cnn_input'] = batch_cnn\n",
    "        if 'mlp_input' in input_names:\n",
    "            input_dict['mlp_input'] = batch_mlp\n",
    "\n",
    "        if 'gnn_input' in input_names:\n",
    "            input_dict['gnn_input'] = batch_gnn\n",
    "            \n",
    "        y_pred_list.append(model.predict(input_dict).flatten())\n",
    "            \n",
    "    return np.concatenate(y_pred_list)\n",
    "\n",
    "# ==================== 7. Run the Analysis with K-Fold Cross-Validation ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing Stacked Deep Ensemble with 5-Fold Cross-Validation for BUFFER_METERS = {BUFFER_METERS}m\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Pre-calculate patch shape based on the current buffer size\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    cnn_patch_shape = (patch_width, patch_width, len(raster_paths))\n",
    "\n",
    "mlp_input_dim = mlp_all.shape[1]\n",
    "gnn_input_dim = gnn_all.shape[0]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "trained_models = {}\n",
    "\n",
    "# Increased patience to allow for more epochs to find a good minimum\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(all_data)):\n",
    "    print(f\"\\n--- Starting Fold {fold + 1}/5 ---\")\n",
    "    \n",
    "    # Split data for this fold\n",
    "    coords_train, coords_test = coords_all[train_idx], coords_all[test_idx]\n",
    "    mlp_train, mlp_test = mlp_all[train_idx], mlp_all[test_idx]\n",
    "    y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
    "    \n",
    "    # Scale MLP data\n",
    "    scaler = StandardScaler()\n",
    "    mlp_train_scaled = scaler.fit_transform(mlp_train)\n",
    "    mlp_test_scaled = scaler.transform(mlp_test)\n",
    "\n",
    "    # Prepare GNN data for the current fold\n",
    "    gnn_train = distance_matrix(coords_train, coords_train)\n",
    "    gnn_train = np.exp(-gnn_train/10)\n",
    "    gnn_test = distance_matrix(coords_test, coords_train)\n",
    "    gnn_test = np.exp(-gnn_test/10)\n",
    "    \n",
    "    # --- Train Base Models ---\n",
    "    batch_size = 4\n",
    "    \n",
    "    # CNN-MLP\n",
    "    cnn_mlp_model = build_cnn_mlp_model(cnn_patch_shape, mlp_input_dim)\n",
    "    cnn_mlp_train_gen = DataGenerator(\n",
    "        coords=coords_train, mlp_data=mlp_train_scaled, gnn_data=gnn_train, y=y_train,\n",
    "        raster_paths=raster_paths, buffer_meters=BUFFER_METERS, model_inputs=['cnn', 'mlp'], batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    # Increased epochs to give the model more time to train\n",
    "    cnn_mlp_model.fit(cnn_mlp_train_gen, epochs=1, verbose=1, callbacks=[early_stopping], validation_data=cnn_mlp_train_gen)\n",
    "\n",
    "    # GNN-MLP\n",
    "    gnn_mlp_model = build_gnn_mlp_model(gnn_train.shape[1], mlp_input_dim)\n",
    "    gnn_mlp_train_gen = DataGenerator(\n",
    "        coords=coords_train, mlp_data=mlp_train_scaled, gnn_data=gnn_train, y=y_train,\n",
    "        raster_paths=raster_paths, buffer_meters=BUFFER_METERS, model_inputs=['gnn', 'mlp'], batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    # Increased epochs to give the model more time to train\n",
    "    gnn_mlp_model.fit(gnn_mlp_train_gen, epochs=1, verbose=1, callbacks=[early_stopping], validation_data=gnn_mlp_train_gen)\n",
    "    \n",
    "    # CNN-GNN\n",
    "    cnn_gnn_model = build_cnn_gnn_model(cnn_patch_shape, gnn_train.shape[1])\n",
    "    cnn_gnn_train_gen = DataGenerator(\n",
    "        coords=coords_train, mlp_data=mlp_train_scaled, gnn_data=gnn_train, y=y_train,\n",
    "        raster_paths=raster_paths, buffer_meters=BUFFER_METERS, model_inputs=['cnn', 'gnn'], batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    # Increased epochs to give the model more time to train\n",
    "    cnn_gnn_model.fit(cnn_gnn_train_gen, epochs=1, verbose=1, callbacks=[early_stopping], validation_data=cnn_gnn_train_gen)\n",
    "\n",
    "    # --- Generate predictions for meta-learner ---\n",
    "    preds1_train = get_model_predictions(cnn_mlp_model, coords_train, mlp_train_scaled, gnn_train, raster_paths, BUFFER_METERS, batch_size)\n",
    "    preds2_train = get_model_predictions(gnn_mlp_model, coords_train, mlp_train_scaled, gnn_train, raster_paths, BUFFER_METERS, batch_size)\n",
    "    preds3_train = get_model_predictions(cnn_gnn_model, coords_train, mlp_train_scaled, gnn_train, raster_paths, BUFFER_METERS, batch_size)\n",
    "    meta_train_inputs = (preds1_train.reshape(-1, 1), preds2_train.reshape(-1, 1), preds3_train.reshape(-1, 1))\n",
    "\n",
    "    # --- Train Meta-Learner ---\n",
    "    meta_model = build_meta_learner_model()\n",
    "    meta_model.fit(meta_train_inputs, y_train, epochs=1, verbose=0, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "    # --- Evaluate on test data of the fold ---\n",
    "    preds1_test = get_model_predictions(cnn_mlp_model, coords_test, mlp_test_scaled, gnn_test, raster_paths, BUFFER_METERS, batch_size)\n",
    "    preds2_test = get_model_predictions(gnn_mlp_model, coords_test, mlp_test_scaled, gnn_test, raster_paths, BUFFER_METERS, batch_size)\n",
    "    preds3_test = get_model_predictions(cnn_gnn_model, coords_test, mlp_test_scaled, gnn_test, raster_paths, BUFFER_METERS, batch_size)\n",
    "    meta_test_inputs = (preds1_test.reshape(-1, 1), preds2_test.reshape(-1, 1), preds3_test.reshape(-1, 1))\n",
    "\n",
    "    y_pred = meta_model.predict(meta_test_inputs).flatten()\n",
    "    r2_test = r2_score(y_test, y_pred)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae_test = mean_absolute_error(y_test, y_pred)\n",
    "    smape_test = smape(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Performance:\")\n",
    "    print(f\"  R²: {r2_test:.4f} | RMSE: {rmse_test:.4f} | MAE: {mae_test:.4f} | SMAPE: {smape_test:.4f}%\")\n",
    "    \n",
    "    fold_results.append({\n",
    "        'r2': r2_test,\n",
    "        'rmse': rmse_test,\n",
    "        'mae': mae_test,\n",
    "        'smape': smape_test,\n",
    "    })\n",
    "    \n",
    "    # Store models from the final fold for the feature importance analysis\n",
    "    if fold == kf.get_n_splits() - 1:\n",
    "        # Create CNN patches for the last fold's test set\n",
    "        cnn_test = extract_patch_for_generator(coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height)\n",
    "\n",
    "        trained_models = {\n",
    "            'cnn_mlp': cnn_mlp_model,\n",
    "            'gnn_mlp': gnn_mlp_model,\n",
    "            'cnn_gnn': cnn_gnn_model,\n",
    "            'meta': meta_model,\n",
    "            'test_data': {\n",
    "                'coords': coords_test,\n",
    "                'mlp_scaled': mlp_test_scaled,\n",
    "                'gnn': gnn_test,\n",
    "                'y_true': y_test,\n",
    "                'cnn': cnn_test\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    # Free up memory\n",
    "    del cnn_mlp_model, gnn_mlp_model, cnn_gnn_model, meta_model\n",
    "    gc.collect()\n",
    "\n",
    "# --- Print Final Cross-Validation Summary ---\n",
    "avg_r2 = np.mean([r['r2'] for r in fold_results])\n",
    "std_r2 = np.std([r['r2'] for r in fold_results])\n",
    "avg_rmse = np.mean([r['rmse'] for r in fold_results])\n",
    "std_rmse = np.std([r['rmse'] for r in fold_results])\n",
    "avg_mae = np.mean([r['mae'] for r in fold_results])\n",
    "std_mae = np.std([r['mae'] for r in fold_results])\n",
    "avg_smape = np.mean([r['smape'] for r in fold_results])\n",
    "std_smape = np.std([r['smape'] for r in fold_results])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Final Cross-Validation Performance (Mean ± Std Dev)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"R²: {avg_r2:.4f} ± {std_r2:.4f}\")\n",
    "print(f\"RMSE: {avg_rmse:.4f} ± {std_rmse:.4f}\")\n",
    "print(f\"MAE: {avg_mae:.4f} ± {std_mae:.4f}\")\n",
    "print(f\"SMAPE: {avg_smape:.4f} ± {std_smape:.4f}%\")\n",
    "\n",
    "# ==================== 8. Full Feature Importance Analysis ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Running Full Feature Importance (Permutation-based) on Last Fold's Test Set\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_full_pipeline_predictions(models, coords, mlp_scaled, gnn, cnn_input, raster_paths, buffer_meters, batch_size, permute_mlp_idx=None, permute_raster_idx=None):\n",
    "    \"\"\"Gets predictions from the full stacked ensemble pipeline with an optional feature permutation.\"\"\"\n",
    "    \n",
    "    # Get CNN patches, with optional channel permutation\n",
    "    if permute_raster_idx is not None:\n",
    "        cnn_input_permuted = np.copy(cnn_input)\n",
    "        np.random.shuffle(cnn_input_permuted[:, :, :, permute_raster_idx].ravel())\n",
    "    else:\n",
    "        cnn_input_permuted = cnn_input\n",
    "\n",
    "    # Get MLP data, with optional column permutation\n",
    "    if permute_mlp_idx is not None:\n",
    "        mlp_scaled_permuted = np.copy(mlp_scaled)\n",
    "        np.random.shuffle(mlp_scaled_permuted[:, permute_mlp_idx])\n",
    "    else:\n",
    "        mlp_scaled_permuted = mlp_scaled\n",
    "\n",
    "    # Run data through base models\n",
    "    preds1 = models['cnn_mlp'].predict({'cnn_input': cnn_input_permuted, 'mlp_input': mlp_scaled_permuted}).flatten()\n",
    "    preds2 = models['gnn_mlp'].predict({'gnn_input': gnn, 'mlp_input': mlp_scaled_permuted}).flatten()\n",
    "    preds3 = models['cnn_gnn'].predict({'cnn_input': cnn_input_permuted, 'gnn_input': gnn}).flatten()\n",
    "    \n",
    "    # Run predictions through meta-learner\n",
    "    meta_inputs = (preds1.reshape(-1, 1), preds2.reshape(-1, 1), preds3.reshape(-1, 1))\n",
    "    y_pred = models['meta'].predict(meta_inputs).flatten()\n",
    "    return y_pred\n",
    "\n",
    "# Get baseline performance\n",
    "models = trained_models\n",
    "test_data = models['test_data']\n",
    "y_test = test_data['y_true']\n",
    "y_pred_baseline = get_full_pipeline_predictions(models, test_data['coords'], test_data['mlp_scaled'], test_data['gnn'], test_data['cnn'], raster_paths, BUFFER_METERS, 4)\n",
    "baseline_r2 = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "importance_scores = {}\n",
    "\n",
    "# Importance for MLP features\n",
    "print(\"\\n--- Importance of MLP Features ---\")\n",
    "for i, feature_name in enumerate(numeric_cols):\n",
    "    y_pred_shuffled = get_full_pipeline_predictions(models, test_data['coords'], test_data['mlp_scaled'], test_data['gnn'], test_data['cnn'], raster_paths, BUFFER_METERS, 4, permute_mlp_idx=i)\n",
    "    shuffled_r2 = r2_score(y_test, y_pred_shuffled)\n",
    "    importance = baseline_r2 - shuffled_r2\n",
    "    importance_scores[feature_name] = importance\n",
    "    print(f\"  {feature_name:<20}: R² drop = {importance:.4f}\")\n",
    "\n",
    "# Importance for CNN (Raster) features\n",
    "print(\"\\n--- Importance of CNN (Raster) Features ---\")\n",
    "for i, raster_path in enumerate(raster_paths):\n",
    "    raster_name = os.path.basename(raster_path)\n",
    "    y_pred_shuffled = get_full_pipeline_predictions(models, test_data['coords'], test_data['mlp_scaled'], test_data['gnn'], test_data['cnn'], raster_paths, BUFFER_METERS, 4, permute_raster_idx=i)\n",
    "    shuffled_r2 = r2_score(y_test, y_pred_shuffled)\n",
    "    importance = baseline_r2 - shuffled_r2\n",
    "    importance_scores[raster_name] = importance\n",
    "    print(f\"  {raster_name:<20}: R² drop = {importance:.4f}\")\n",
    "\n",
    "# Importance for GNN features (by shuffling GNN input)\n",
    "print(\"\\n--- Importance of GNN Features ---\")\n",
    "gnn_test_permuted = np.copy(test_data['gnn'])\n",
    "np.random.shuffle(gnn_test_permuted.ravel())\n",
    "preds1_shuffled_gnn = models['cnn_mlp'].predict({'cnn_input': test_data['cnn'], 'mlp_input': test_data['mlp_scaled']}).flatten()\n",
    "preds2_shuffled_gnn = models['gnn_mlp'].predict({'gnn_input': gnn_test_permuted, 'mlp_input': test_data['mlp_scaled']}).flatten()\n",
    "preds3_shuffled_gnn = models['cnn_gnn'].predict({'cnn_input': test_data['cnn'], 'gnn_input': gnn_test_permuted}).flatten()\n",
    "meta_inputs_shuffled_gnn = (preds1_shuffled_gnn.reshape(-1, 1), preds2_shuffled_gnn.reshape(-1, 1), preds3_shuffled_gnn.reshape(-1, 1))\n",
    "y_pred_shuffled_gnn = models['meta'].predict(meta_inputs_shuffled_gnn).flatten()\n",
    "shuffled_r2_gnn = r2_score(y_test, y_pred_shuffled_gnn)\n",
    "importance_gnn = baseline_r2 - shuffled_r2_gnn\n",
    "importance_scores['GNN_distance_matrix'] = importance_gnn\n",
    "print(f\"  GNN_distance_matrix: R² drop = {importance_gnn:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d54743-f0f6-454e-a5e2-5909f110cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Save all info to a folder ==================== #\n",
    "output_folder = \"stacked_ensemble_kfold\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(f\"\\nCreating folder: '{output_folder}' and saving results...\")\n",
    "\n",
    "# Save all four models from the last fold\n",
    "cnn_mlp_model_path = os.path.join(output_folder, \"cnn_mlp_model.keras\")\n",
    "models['cnn_mlp'].save(cnn_mlp_model_path)\n",
    "print(f\"CNN-MLP model saved to: {cnn_mlp_model_path}\")\n",
    "\n",
    "gnn_mlp_model_path = os.path.join(output_folder, \"gnn_mlp_model.keras\")\n",
    "models['gnn_mlp'].save(gnn_mlp_model_path)\n",
    "print(f\"GNN-MLP model saved to: {gnn_mlp_model_path}\")\n",
    "\n",
    "cnn_gnn_model_path = os.path.join(output_folder, \"cnn_gnn_model.keras\")\n",
    "models['cnn_gnn'].save(cnn_gnn_model_path)\n",
    "print(f\"CNN-GNN model saved to: {cnn_gnn_model_path}\")\n",
    "\n",
    "meta_model_path = os.path.join(output_folder, \"meta_learner.keras\")\n",
    "models['meta'].save(meta_model_path)\n",
    "print(f\"Meta-learner model saved to: {meta_model_path}\")\n",
    "\n",
    "# Save the metrics and feature importance\n",
    "metrics_path = os.path.join(output_folder, \"kfold_metrics.pkl\")\n",
    "with open(metrics_path, 'wb') as f:\n",
    "    pickle.dump(fold_results, f)\n",
    "print(f\"K-Fold performance metrics saved to: {metrics_path}\")\n",
    "\n",
    "importance_path = os.path.join(output_folder, \"full_feature_importance.pkl\")\n",
    "with open(importance_path, 'wb') as f:\n",
    "    pickle.dump(importance_scores, f)\n",
    "print(f\"Full feature importance scores saved to: {importance_path}\")\n",
    "\n",
    "print(\"All information successfully saved.\")\n",
    "\n",
    "# Garbage collect to free up memory\n",
    "del models\n",
    "gc.collect()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
