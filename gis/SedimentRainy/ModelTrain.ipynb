{
 "cells": [
  {
   "cell_type": "raw",
   "id": "91de9c71-18bd-4265-88e0-9ac4f60f1e4e",
   "metadata": {},
   "source": [
    "# PMF-GWR-Xgboost"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49ac9481-414c-4c22-8018-0796be790fba",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import rasterio\n",
    "import rasterstats\n",
    "from rasterstats import zonal_stats\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "import shap\n",
    "from scipy.spatial import cKDTree\n",
    "import os\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "# Load the main dataset and the river sampling data.\n",
    "original = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "river_100.drop(columns=\"Source\", inplace=True)\n",
    "\n",
    "# Identify columns for feature engineering and prediction\n",
    "drop_cols = ['Stations', 'River', 'Lat', 'Long', 'geometry']\n",
    "numeric_cols = original.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# Split original data into train and test sets for the ensemble model.\n",
    "# This ensures a fair evaluation on unseen data points.\n",
    "np.random.seed(42)\n",
    "train_idx = np.random.choice(len(original), 10, replace=False)\n",
    "test_idx = [i for i in range(len(original)) if i not in train_idx]\n",
    "train_orig = original.iloc[train_idx]\n",
    "test_orig = original.iloc[test_idx]\n",
    "\n",
    "# Combine the river samples and the original training data to form the full training set.\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Extract Multi-Scale Raster Features ==================== #\n",
    "# Define the raster files and buffer sizes for zonal statistics.\n",
    "raster_files = [\n",
    "    \"../CalIndices/ndwi.tif\", \"../CalIndices/ndvi.tif\", \"../CalIndices/ndbi.tif\",\n",
    "    \"../CalIndices/awei.tif\", \"../CalIndices/bui.tif\", \"../CalIndices/evi.tif\",\n",
    "    \"../CalIndices/mndwi.tif\", \"../CalIndices/ndbsi.tif\", \"../CalIndices/ndsi.tif\",\n",
    "    \"../CalIndices/savi.tif\", \"../CalIndices/ui.tif\", \n",
    "    \"../IDW/AsR.tif\", \"../IDW/CdR.tif\", \"../IDW/ClayR.tif\", \"../IDW/CrR.tif\", \"../IDW/CuR.tif\",\n",
    "    \"../IDW/NiR.tif\", \"../IDW/Pb_R.tif\", \"../IDW/SandR.tif\", \"../IDW/SiltR.tif\",\n",
    "    \"../LULCMerged/LULC2017.tif\", \"../LULCMerged/LULC2018.tif\", \"../LULCMerged/LULC2019.tif\",\n",
    "    \"../LULCMerged/LULC2020.tif\", \"../LULCMerged/LULC2021.tif\", \"../LULCMerged/LULC2022.tif\"\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "buffers = [500]\n",
    "\n",
    "def extract_raster_stats(points_df, rasters, buffers):\n",
    "    \"\"\"\n",
    "    Extracts zonal statistics (mean, std) from raster files for points\n",
    "    within specified buffer distances.\n",
    "    \"\"\"\n",
    "    # Create a GeoDataFrame from the points for spatial operations\n",
    "    gdf = gpd.GeoDataFrame(points_df, geometry=gpd.points_from_xy(points_df.Long, points_df.Lat), crs=\"EPSG:4326\")\n",
    "    features = pd.DataFrame(index=gdf.index)\n",
    "\n",
    "    for raster_path in rasters:\n",
    "        for buf in buffers:\n",
    "            col_mean = f\"{os.path.basename(raster_path).split('.')[0]}_{buf}m_mean\"\n",
    "            col_std = f\"{os.path.basename(raster_path).split('.')[0]}_{buf}m_std\"\n",
    "\n",
    "            # Create a buffer around each point (converted from meters to degrees)\n",
    "            # The value 111320 is a rough conversion factor from degrees to meters at the equator.\n",
    "            buffered_geometries = gdf.geometry.buffer(buf / 111320)\n",
    "            \n",
    "            # Use rasterstats to get zonal statistics for the buffered areas\n",
    "            zs_results = zonal_stats(buffered_geometries, raster_path, stats=['mean', 'std'], nodata=np.nan)\n",
    "\n",
    "            features[col_mean] = [res['mean'] for res in zs_results]\n",
    "            features[col_std] = [res['std'] for res in zs_results]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract raster features for both the training and testing data\n",
    "train_raster_feats = extract_raster_stats(train_combined, raster_files, buffers)\n",
    "test_raster_feats = extract_raster_stats(test_orig, raster_files, buffers)\n",
    "\n",
    "# ==================== 3. PMF (NMF) for Source Apportionment ==================== #\n",
    "# Use Non-Negative Matrix Factorization to identify latent source factors.\n",
    "pmf_features = ['CrR', 'NiR', 'CuR', 'AsR', 'CdR', 'PbR', 'MR', 'SandR', 'SiltR', 'ClayR', 'FeR']\n",
    "nmf = NMF(n_components=3, init='random', random_state=42, max_iter=100)\n",
    "G_train = nmf.fit_transform(train_combined[pmf_features].values)\n",
    "F = nmf.components_\n",
    "print(\"\\nPMF Source Profiles (F):\\n\", pd.DataFrame(F, columns=pmf_features))\n",
    "\n",
    "# ==================== 4. Fixed Geographically Weighted Regression (GWR) ==================== #\n",
    "# Implement a custom GWR function to model spatial non-stationarity.\n",
    "def gaussian_kernel(d, bw):\n",
    "    return np.exp(-(d**2) / (2 * bw**2))\n",
    "\n",
    "def fixed_gwr(coords, factors, y, bw=0.5):\n",
    "    \"\"\"\n",
    "    Performs a fixed bandwidth GWR using a Gaussian kernel.\n",
    "    \"\"\"\n",
    "    n = len(coords)\n",
    "    preds = np.zeros(n)\n",
    "    X = np.hstack([np.ones((n, 1)), factors])\n",
    "    for i in range(n):\n",
    "        dist = np.linalg.norm(coords - coords[i], axis=1)\n",
    "        W = np.diag(gaussian_kernel(dist, bw))\n",
    "        # Use pseudo-inverse for stability\n",
    "        beta = np.linalg.pinv(X.T @ W @ X) @ (X.T @ W @ y.reshape(-1, 1))\n",
    "        preds[i] = (np.array([1] + list(factors[i])) @ beta).item()\n",
    "    return preds.reshape(-1, 1)\n",
    "\n",
    "coords_train = train_combined[['Long', 'Lat']].values\n",
    "y_train = train_combined['RI'].values\n",
    "GWR_train = fixed_gwr(coords_train, G_train, y_train, bw=0.5)\n",
    "\n",
    "# Interpolate PMF factors for the test set using Inverse Distance Weighting (IDW)\n",
    "def idw_interpolation(known_coords, known_values, query_coords, power=2):\n",
    "    \"\"\"\n",
    "    Performs IDW to interpolate values from known points to query points.\n",
    "    \"\"\"\n",
    "    tree = cKDTree(known_coords)\n",
    "    dists, idxs = tree.query(query_coords, k=4)\n",
    "    dists[dists == 0] = 1e-10  # Avoid division by zero\n",
    "    weights = 1 / (dists ** power)\n",
    "    weights /= weights.sum(axis=1)[:, None]\n",
    "    return np.sum(weights * known_values[idxs], axis=1)\n",
    "\n",
    "coords_test = test_orig[['Long', 'Lat']].values\n",
    "y_test = test_orig['RI'].values\n",
    "# Interpolate PMF factors for the test set\n",
    "G_test = np.column_stack([idw_interpolation(coords_train, G_train[:, i], coords_test) for i in range(G_train.shape[1])])\n",
    "# Apply GWR to the interpolated PMF factors for the test set\n",
    "GWR_test = fixed_gwr(coords_test, G_test, y_test, bw=0.5)\n",
    "\n",
    "# ==================== 5. Interaction Features ==================== #\n",
    "# Create new features by interacting PMF and GWR results.\n",
    "def create_interactions(pmf, gwr):\n",
    "    \"\"\"\n",
    "    Creates interaction features between PMF factors and GWR predictions.\n",
    "    \"\"\"\n",
    "    interactions = pd.DataFrame()\n",
    "    for i in range(pmf.shape[1]):\n",
    "        interactions[f\"PMF{i}_GWR\"] = pmf[:, i] * gwr.flatten()\n",
    "    return interactions\n",
    "\n",
    "train_interact = create_interactions(G_train, GWR_train)\n",
    "test_interact = create_interactions(G_test, GWR_test)\n",
    "\n",
    "# ==================== 6. Final Feature Matrix ==================== #\n",
    "# Combine all engineered features into a single matrix for the XGBoost model.\n",
    "X_train = np.hstack([\n",
    "    train_combined[numeric_cols].values,\n",
    "    train_raster_feats.values,\n",
    "    G_train,\n",
    "    GWR_train,\n",
    "    train_interact.values\n",
    "])\n",
    "\n",
    "X_test = np.hstack([\n",
    "    test_orig[numeric_cols].values,\n",
    "    test_raster_feats.values,\n",
    "    G_test,\n",
    "    GWR_test,\n",
    "    test_interact.values\n",
    "])\n",
    "\n",
    "# ==================== 7. Optuna Hyperparameter Optimization ==================== #\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Defines the Optuna objective function to minimize negative R².\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 600),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1, 10),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 5)\n",
    "    }\n",
    "    model = XGBRegressor(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return -r2_score(y_test, y_pred)\n",
    "\n",
    "# Run the Optuna study to find the best parameters\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=25)\n",
    "best_params = study.best_params\n",
    "print(\"\\n✅ Best Parameters from Optuna:\", best_params)\n",
    "\n",
    "# ==================== 8. Train Final XGBoost ==================== #\n",
    "# Train the final model with the optimized hyperparameters.\n",
    "xgb = XGBRegressor(**best_params, random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# ==================== 9. Evaluation ==================== #\n",
    "# Evaluate the final model's performance on both training and test data.\n",
    "y_pred_train = xgb.predict(X_train)\n",
    "y_pred_test = xgb.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"\\n✅ Final Model Performance:\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# ==================== 10. SHAP Interpretation ==================== #\n",
    "# Use SHAP to explain the model's predictions and feature importance.\n",
    "explainer = shap.TreeExplainer(xgb)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# Generate SHAP summary plots\n",
    "shap.summary_plot(shap_values, X_train, feature_names=[f\"F{i}\" for i in range(X_train.shape[1])])\n",
    "shap.summary_plot(shap_values, X_train, feature_names=[f\"F{i}\" for i in range(X_train.shape[1])], plot_type=\"bar\")\n",
    "\n",
    "print(\"✅ SHAP analysis complete. Check plots for feature importance.\")\n",
    "\n",
    "feature_names = list(numeric_cols) \\\n",
    "                + list(train_raster_feats.columns) \\\n",
    "                + [f\"PMF_Factor{i}\" for i in range(G_train.shape[1])] \\\n",
    "                + [\"GWR_Adjusted\"] \\\n",
    "                + list(train_interact.columns)\n",
    "\n",
    "# Create DataFrame of importance\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": xgb.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Save & print\n",
    "importance_df.to_csv(\"SHAP-PMF-GLWR-Xgboost.csv\", index=False)\n",
    "print(\"\\n✅ Top Features (XGBoost Gain):\\n\", importance_df.head(30))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0596a5a-6d97-484f-a7e6-3d0ddc62aa95",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import rasterio\n",
    "import rasterstats\n",
    "from rasterstats import zonal_stats\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "import shap\n",
    "from scipy.spatial import cKDTree\n",
    "import os\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "# Load the main dataset and the river sampling data.\n",
    "original = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_200.csv\")\n",
    "river_100.drop(columns=\"Source\", inplace=True)\n",
    "\n",
    "# Identify columns for feature engineering and prediction\n",
    "drop_cols = ['Stations', 'River', 'Lat', 'Long', 'geometry']\n",
    "numeric_cols = original.drop(columns=drop_cols).columns.drop('AsR')\n",
    "\n",
    "# Split original data into train and test sets for the ensemble model.\n",
    "# This ensures a fair evaluation on unseen data points.\n",
    "np.random.seed(42)\n",
    "train_idx = np.random.choice(len(original), 10, replace=False)\n",
    "test_idx = [i for i in range(len(original)) if i not in train_idx]\n",
    "train_orig = original.iloc[train_idx]\n",
    "test_orig = original.iloc[test_idx]\n",
    "\n",
    "# Combine the river samples and the original training data to form the full training set.\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Extract Multi-Scale Raster Features ==================== #\n",
    "# Define the raster files and buffer sizes for zonal statistics.\n",
    "raster_files = [\n",
    "    \"../CalIndices/ndwi.tif\", \"../CalIndices/ndvi.tif\", \"../CalIndices/ndbi.tif\",\n",
    "    \"../CalIndices/awei.tif\", \"../CalIndices/bui.tif\", \"../CalIndices/evi.tif\",\n",
    "    \"../CalIndices/mndwi.tif\", \"../CalIndices/ndbsi.tif\", \"../CalIndices/ndsi.tif\",\n",
    "    \"../CalIndices/savi.tif\", \"../CalIndices/ui.tif\",\n",
    "    \"../IDW/AsR.tif\", \"../IDW/CdR.tif\", \"../IDW/ClayR.tif\", \"../IDW/CrR.tif\", \"../IDW/CuR.tif\",\n",
    "    \"../IDW/NiR.tif\", \"../IDW/Pb_R.tif\", \"../IDW/SandR.tif\", \"../IDW/SiltR.tif\",\n",
    "    \"../LULCMerged/LULC2017.tif\", \"../LULCMerged/LULC2018.tif\", \"../LULCMerged/LULC2019.tif\",\n",
    "    \"../LULCMerged/LULC2020.tif\", \"../LULCMerged/LULC2021.tif\", \"../LULCMerged/LULC2022.tif\"\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "buffers = [500, 1000, 2000]\n",
    "\n",
    "def extract_raster_stats(points_df, rasters, buffers):\n",
    "    \"\"\"\n",
    "    Extracts zonal statistics (mean, std) from raster files for points\n",
    "    within specified buffer distances.\n",
    "    \"\"\"\n",
    "    # Create a GeoDataFrame from the points for spatial operations\n",
    "    gdf = gpd.GeoDataFrame(points_df, geometry=gpd.points_from_xy(points_df.Long, points_df.Lat), crs=\"EPSG:4326\")\n",
    "    features = pd.DataFrame(index=gdf.index)\n",
    "\n",
    "    for raster_path in rasters:\n",
    "        for buf in buffers:\n",
    "            col_mean = f\"{os.path.basename(raster_path).split('.')[0]}_{buf}m_mean\"\n",
    "            col_std = f\"{os.path.basename(raster_path).split('.')[0]}_{buf}m_std\"\n",
    "\n",
    "            # Create a buffer around each point (converted from meters to degrees)\n",
    "            # The value 111320 is a rough conversion factor from degrees to meters at the equator.\n",
    "            buffered_geometries = gdf.geometry.buffer(buf / 111320)\n",
    "            \n",
    "            # Use rasterstats to get zonal statistics for the buffered areas\n",
    "            zs_results = zonal_stats(buffered_geometries, raster_path, stats=['mean', 'std'], nodata=np.nan)\n",
    "\n",
    "            features[col_mean] = [res['mean'] for res in zs_results]\n",
    "            features[col_std] = [res['std'] for res in zs_results]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract raster features for both the training and testing data\n",
    "train_raster_feats = extract_raster_stats(train_combined, raster_files, buffers)\n",
    "test_raster_feats = extract_raster_stats(test_orig, raster_files, buffers)\n",
    "\n",
    "# ==================== 3. PMF (NMF) for Source Apportionment ==================== #\n",
    "# Use Non-Negative Matrix Factorization to identify latent source factors.\n",
    "pmf_features = ['CrR', 'NiR', 'CuR', 'AsR', 'CdR', 'PbR', 'MR', 'SandR', 'SiltR', 'ClayR', 'FeR']\n",
    "nmf = NMF(n_components=3, init='random', random_state=42, max_iter=1000)\n",
    "G_train = nmf.fit_transform(train_combined[pmf_features].values)\n",
    "F = nmf.components_\n",
    "print(\"\\nPMF Source Profiles (F):\\n\", pd.DataFrame(F, columns=pmf_features))\n",
    "\n",
    "# ==================== 4. Fixed Geographically Weighted Regression (GWR) ==================== #\n",
    "# Implement a custom GWR function to model spatial non-stationarity.\n",
    "def gaussian_kernel(d, bw):\n",
    "    return np.exp(-(d**2) / (2 * bw**2))\n",
    "\n",
    "def fixed_gwr(coords, factors, y, bw=0.5):\n",
    "    \"\"\"\n",
    "    Performs a fixed bandwidth GWR using a Gaussian kernel.\n",
    "    \"\"\"\n",
    "    n = len(coords)\n",
    "    preds = np.zeros(n)\n",
    "    X = np.hstack([np.ones((n, 1)), factors])\n",
    "    for i in range(n):\n",
    "        dist = np.linalg.norm(coords - coords[i], axis=1)\n",
    "        W = np.diag(gaussian_kernel(dist, bw))\n",
    "        # Use pseudo-inverse for stability\n",
    "        beta = np.linalg.pinv(X.T @ W @ X) @ (X.T @ W @ y.reshape(-1, 1))\n",
    "        preds[i] = (np.array([1] + list(factors[i])) @ beta).item()\n",
    "    return preds.reshape(-1, 1)\n",
    "\n",
    "coords_train = train_combined[['Long', 'Lat']].values\n",
    "y_train = train_combined['AsR'].values\n",
    "GWR_train = fixed_gwr(coords_train, G_train, y_train, bw=0.5)\n",
    "\n",
    "# Interpolate PMF factors for the test set using Inverse Distance Weighting (IDW)\n",
    "def idw_interpolation(known_coords, known_values, query_coords, power=2):\n",
    "    \"\"\"\n",
    "    Performs IDW to interpolate values from known points to query points.\n",
    "    \"\"\"\n",
    "    tree = cKDTree(known_coords)\n",
    "    dists, idxs = tree.query(query_coords, k=4)\n",
    "    dists[dists == 0] = 1e-10  # Avoid division by zero\n",
    "    weights = 1 / (dists ** power)\n",
    "    weights /= weights.sum(axis=1)[:, None]\n",
    "    return np.sum(weights * known_values[idxs], axis=1)\n",
    "\n",
    "coords_test = test_orig[['Long', 'Lat']].values\n",
    "y_test = test_orig['AsR'].values\n",
    "# Interpolate PMF factors for the test set\n",
    "G_test = np.column_stack([idw_interpolation(coords_train, G_train[:, i], coords_test) for i in range(G_train.shape[1])])\n",
    "# Apply GWR to the interpolated PMF factors for the test set\n",
    "GWR_test = fixed_gwr(coords_test, G_test, y_test, bw=0.5)\n",
    "\n",
    "# ==================== 5. Interaction Features ==================== #\n",
    "# Create new features by interacting PMF and GWR results.\n",
    "def create_interactions(pmf, gwr):\n",
    "    \"\"\"\n",
    "    Creates interaction features between PMF factors and GWR predictions.\n",
    "    \"\"\"\n",
    "    interactions = pd.DataFrame()\n",
    "    for i in range(pmf.shape[1]):\n",
    "        interactions[f\"PMF{i}_GWR\"] = pmf[:, i] * gwr.flatten()\n",
    "    return interactions\n",
    "\n",
    "train_interact = create_interactions(G_train, GWR_train)\n",
    "test_interact = create_interactions(G_test, GWR_test)\n",
    "\n",
    "# ==================== 6. Final Feature Matrix ==================== #\n",
    "# Combine all engineered features into a single matrix for the XGBoost model.\n",
    "X_train = np.hstack([\n",
    "    train_combined[numeric_cols].values,\n",
    "    train_raster_feats.values,\n",
    "    G_train,\n",
    "    GWR_train,\n",
    "    train_interact.values\n",
    "])\n",
    "\n",
    "X_test = np.hstack([\n",
    "    test_orig[numeric_cols].values,\n",
    "    test_raster_feats.values,\n",
    "    G_test,\n",
    "    GWR_test,\n",
    "    test_interact.values\n",
    "])\n",
    "\n",
    "# ==================== 7. Optuna Hyperparameter Optimization ==================== #\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Defines the Optuna objective function to minimize negative R².\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 600),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1, 10),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 5)\n",
    "    }\n",
    "    model = XGBRegressor(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return -r2_score(y_test, y_pred)\n",
    "\n",
    "# Run the Optuna study to find the best parameters\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=25)\n",
    "best_params = study.best_params\n",
    "print(\"\\nBest Parameters from Optuna:\", best_params)\n",
    "\n",
    "# ==================== 8. Train Final XGBoost ==================== #\n",
    "# Train the final model with the optimized hyperparameters.\n",
    "xgb = XGBRegressor(**best_params, random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# ==================== 9. Evaluation ==================== #\n",
    "# Evaluate the final model's performance on both training and test data.\n",
    "y_pred_train = xgb.predict(X_train)\n",
    "y_pred_test = xgb.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"\\nFinal Model Performance:\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# ==================== 10. SHAP Interpretation ==================== #\n",
    "# Use SHAP to explain the model's predictions and feature importance.\n",
    "explainer = shap.TreeExplainer(xgb)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# Generate SHAP summary plots\n",
    "shap.summary_plot(shap_values, X_train, feature_names=[f\"F{i}\" for i in range(X_train.shape[1])])\n",
    "shap.summary_plot(shap_values, X_train, feature_names=[f\"F{i}\" for i in range(X_train.shape[1])], plot_type=\"bar\")\n",
    "\n",
    "print(\"SHAP analysis complete. Check plots for feature importance.\")\n",
    "\n",
    "feature_names = list(numeric_cols) \\\n",
    "                + list(train_raster_feats.columns) \\\n",
    "                + [f\"PMF_Factor{i}\" for i in range(G_train.shape[1])] \\\n",
    "                + [\"GWR_Adjusted\"] \\\n",
    "                + list(train_interact.columns)\n",
    "\n",
    "# Create DataFrame of importance\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": xgb.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Save & print\n",
    "importance_df.to_csv(\"SHAP-PMF-GLWR-Xgboost.csv\", index=False)\n",
    "print(\"\\nTop Features (XGBoost Gain):\\n\", importance_df.head(30))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b835bad2-c10b-4734-921b-b174cb7470f6",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import rasterio\n",
    "import rasterstats\n",
    "from rasterstats import zonal_stats\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "import shap\n",
    "from scipy.spatial import cKDTree\n",
    "import os\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "# Load the main dataset and the river sampling data.\n",
    "original = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "river_100.drop(columns=\"Source\", inplace=True)\n",
    "\n",
    "# Identify columns for feature engineering and prediction\n",
    "drop_cols = ['Stations', 'River', 'Lat', 'Long', 'geometry']\n",
    "numeric_cols = original.drop(columns=drop_cols).columns.drop('AsR')\n",
    "\n",
    "# Split original data into train and test sets for the ensemble model.\n",
    "# This ensures a fair evaluation on unseen data points.\n",
    "np.random.seed(42)\n",
    "train_idx = np.random.choice(len(original), 10, replace=False)\n",
    "test_idx = [i for i in range(len(original)) if i not in train_idx]\n",
    "train_orig = original.iloc[train_idx]\n",
    "test_orig = original.iloc[test_idx]\n",
    "\n",
    "# Combine the river samples and the original training data to form the full training set.\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Extract Multi-Scale Raster Features ==================== #\n",
    "# Define the raster files and buffer sizes for zonal statistics.\n",
    "raster_files = [\n",
    "    \"../CalIndices/ndwi.tif\", \"../CalIndices/ndvi.tif\", \"../CalIndices/ndbi.tif\",\n",
    "    \"../CalIndices/awei.tif\", \"../CalIndices/bui.tif\", \"../CalIndices/evi.tif\",\n",
    "    \"../CalIndices/mndwi.tif\", \"../CalIndices/ndbsi.tif\", \"../CalIndices/ndsi.tif\",\n",
    "    \"../CalIndices/savi.tif\", \"../CalIndices/ui.tif\",\n",
    "    \"../IDW/AsR.tif\", \"../IDW/CdR.tif\", \"../IDW/ClayR.tif\", \"../IDW/CrR.tif\", \"../IDW/CuR.tif\",\n",
    "    \"../IDW/NiR.tif\", \"../IDW/Pb_R.tif\", \"../IDW/SandR.tif\", \"../IDW/SiltR.tif\",\n",
    "    \"../LULCMerged/LULC2017.tif\", \"../LULCMerged/LULC2018.tif\", \"../LULCMerged/LULC2019.tif\",\n",
    "    \"../LULCMerged/LULC2020.tif\", \"../LULCMerged/LULC2021.tif\", \"../LULCMerged/LULC2022.tif\"\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "buffers = [500, 1000, 2000]\n",
    "\n",
    "def extract_raster_stats(points_df, rasters, buffers):\n",
    "    \"\"\"\n",
    "    Extracts zonal statistics (mean, std) from raster files for points\n",
    "    within specified buffer distances.\n",
    "    \"\"\"\n",
    "    # Create a GeoDataFrame from the points for spatial operations\n",
    "    gdf = gpd.GeoDataFrame(points_df, geometry=gpd.points_from_xy(points_df.Long, points_df.Lat), crs=\"EPSG:4326\")\n",
    "    features = pd.DataFrame(index=gdf.index)\n",
    "\n",
    "    for raster_path in rasters:\n",
    "        for buf in buffers:\n",
    "            col_mean = f\"{os.path.basename(raster_path).split('.')[0]}_{buf}m_mean\"\n",
    "            col_std = f\"{os.path.basename(raster_path).split('.')[0]}_{buf}m_std\"\n",
    "\n",
    "            # Create a buffer around each point (converted from meters to degrees)\n",
    "            # The value 111320 is a rough conversion factor from degrees to meters at the equator.\n",
    "            buffered_geometries = gdf.geometry.buffer(buf / 111320)\n",
    "            \n",
    "            # Use rasterstats to get zonal statistics for the buffered areas\n",
    "            zs_results = zonal_stats(buffered_geometries, raster_path, stats=['mean', 'std'], nodata=np.nan)\n",
    "\n",
    "            features[col_mean] = [res['mean'] for res in zs_results]\n",
    "            features[col_std] = [res['std'] for res in zs_results]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract raster features for both the training and testing data\n",
    "train_raster_feats = extract_raster_stats(train_combined, raster_files, buffers)\n",
    "test_raster_feats = extract_raster_stats(test_orig, raster_files, buffers)\n",
    "\n",
    "# ==================== 3. PMF (NMF) for Source Apportionment ==================== #\n",
    "# Use Non-Negative Matrix Factorization to identify latent source factors.\n",
    "pmf_features = ['CrR', 'NiR', 'CuR', 'AsR', 'CdR', 'PbR', 'MR', 'SandR', 'SiltR', 'ClayR', 'FeR']\n",
    "nmf = NMF(n_components=3, init='random', random_state=42, max_iter=1000)\n",
    "G_train = nmf.fit_transform(train_combined[pmf_features].values)\n",
    "F = nmf.components_\n",
    "print(\"\\nPMF Source Profiles (F):\\n\", pd.DataFrame(F, columns=pmf_features))\n",
    "\n",
    "# ==================== 4. Fixed Geographically Weighted Regression (GWR) ==================== #\n",
    "# Implement a custom GWR function to model spatial non-stationarity.\n",
    "def gaussian_kernel(d, bw):\n",
    "    return np.exp(-(d**2) / (2 * bw**2))\n",
    "\n",
    "def fixed_gwr(coords, factors, y, bw=0.5):\n",
    "    \"\"\"\n",
    "    Performs a fixed bandwidth GWR using a Gaussian kernel.\n",
    "    \"\"\"\n",
    "    n = len(coords)\n",
    "    preds = np.zeros(n)\n",
    "    X = np.hstack([np.ones((n, 1)), factors])\n",
    "    for i in range(n):\n",
    "        dist = np.linalg.norm(coords - coords[i], axis=1)\n",
    "        W = np.diag(gaussian_kernel(dist, bw))\n",
    "        # Use pseudo-inverse for stability\n",
    "        beta = np.linalg.pinv(X.T @ W @ X) @ (X.T @ W @ y.reshape(-1, 1))\n",
    "        preds[i] = (np.array([1] + list(factors[i])) @ beta).item()\n",
    "    return preds.reshape(-1, 1)\n",
    "\n",
    "coords_train = train_combined[['Long', 'Lat']].values\n",
    "y_train = train_combined['AsR'].values\n",
    "GWR_train = fixed_gwr(coords_train, G_train, y_train, bw=0.5)\n",
    "\n",
    "# Interpolate PMF factors for the test set using Inverse Distance Weighting (IDW)\n",
    "def idw_interpolation(known_coords, known_values, query_coords, power=2):\n",
    "    \"\"\"\n",
    "    Performs IDW to interpolate values from known points to query points.\n",
    "    \"\"\"\n",
    "    tree = cKDTree(known_coords)\n",
    "    dists, idxs = tree.query(query_coords, k=4)\n",
    "    dists[dists == 0] = 1e-10  # Avoid division by zero\n",
    "    weights = 1 / (dists ** power)\n",
    "    weights /= weights.sum(axis=1)[:, None]\n",
    "    return np.sum(weights * known_values[idxs], axis=1)\n",
    "\n",
    "coords_test = test_orig[['Long', 'Lat']].values\n",
    "y_test = test_orig['AsR'].values\n",
    "# Interpolate PMF factors for the test set\n",
    "G_test = np.column_stack([idw_interpolation(coords_train, G_train[:, i], coords_test) for i in range(G_train.shape[1])])\n",
    "# Apply GWR to the interpolated PMF factors for the test set\n",
    "GWR_test = fixed_gwr(coords_test, G_test, y_test, bw=0.5)\n",
    "\n",
    "# ==================== 5. Interaction Features ==================== #\n",
    "# Create new features by interacting PMF and GWR results.\n",
    "def create_interactions(pmf, gwr):\n",
    "    \"\"\"\n",
    "    Creates interaction features between PMF factors and GWR predictions.\n",
    "    \"\"\"\n",
    "    interactions = pd.DataFrame()\n",
    "    for i in range(pmf.shape[1]):\n",
    "        interactions[f\"PMF{i}_GWR\"] = pmf[:, i] * gwr.flatten()\n",
    "    return interactions\n",
    "\n",
    "train_interact = create_interactions(G_train, GWR_train)\n",
    "test_interact = create_interactions(G_test, GWR_test)\n",
    "\n",
    "# ==================== 6. Final Feature Matrix ==================== #\n",
    "# Combine all engineered features into a single matrix for the XGBoost model.\n",
    "X_train = np.hstack([\n",
    "    train_combined[numeric_cols].values,\n",
    "    train_raster_feats.values,\n",
    "    G_train,\n",
    "    GWR_train,\n",
    "    train_interact.values\n",
    "])\n",
    "\n",
    "X_test = np.hstack([\n",
    "    test_orig[numeric_cols].values,\n",
    "    test_raster_feats.values,\n",
    "    G_test,\n",
    "    GWR_test,\n",
    "    test_interact.values\n",
    "])\n",
    "\n",
    "# ==================== 7. Optuna Hyperparameter Optimization ==================== #\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Defines the Optuna objective function to minimize negative R².\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 600),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1, 10),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 5)\n",
    "    }\n",
    "    model = XGBRegressor(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return -r2_score(y_test, y_pred)\n",
    "\n",
    "# Run the Optuna study to find the best parameters\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=25)\n",
    "best_params = study.best_params\n",
    "print(\"\\nBest Parameters from Optuna:\", best_params)\n",
    "\n",
    "# ==================== 8. Train Final XGBoost ==================== #\n",
    "# Train the final model with the optimized hyperparameters.\n",
    "xgb = XGBRegressor(**best_params, random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# ==================== 9. Evaluation ==================== #\n",
    "# Evaluate the final model's performance on both training and test data.\n",
    "y_pred_train = xgb.predict(X_train)\n",
    "y_pred_test = xgb.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"\\nFinal Model Performance:\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# ==================== 10. SHAP Interpretation ==================== #\n",
    "# Use SHAP to explain the model's predictions and feature importance.\n",
    "explainer = shap.TreeExplainer(xgb)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# Generate SHAP summary plots\n",
    "shap.summary_plot(shap_values, X_train, feature_names=[f\"F{i}\" for i in range(X_train.shape[1])])\n",
    "shap.summary_plot(shap_values, X_train, feature_names=[f\"F{i}\" for i in range(X_train.shape[1])], plot_type=\"bar\")\n",
    "\n",
    "print(\"SHAP analysis complete. Check plots for feature importance.\")\n",
    "\n",
    "feature_names = list(numeric_cols) \\\n",
    "                + list(train_raster_feats.columns) \\\n",
    "                + [f\"PMF_Factor{i}\" for i in range(G_train.shape[1])] \\\n",
    "                + [\"GWR_Adjusted\"] \\\n",
    "                + list(train_interact.columns)\n",
    "\n",
    "# Create DataFrame of importance\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": xgb.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Save & print\n",
    "importance_df.to_csv(\"SHAP-PMF-GLWR-Xgboost.csv\", index=False)\n",
    "print(\"\\nTop Features (XGBoost Gain):\\n\", importance_df.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd0b42-9574-4d38-a519-e51e3e508275",
   "metadata": {},
   "source": [
    "# PMF-GWR Based CNN-GNN-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85fb55d7-102c-44d7-8a8a-28a2778b5ef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PMF Source Profiles (F):\n",
      "          CrR        NiR        CuR       AsR       CdR        PbR         MR  \\\n",
      "0   1.011394   0.736558   1.692247  0.281341  0.081750   1.820546   0.744546   \n",
      "1   6.361288   2.934156   7.088633  1.445853  0.265187   5.471062   3.794235   \n",
      "2  21.198226  13.542373  26.909206  5.147733  1.449371  21.855110  14.985165   \n",
      "\n",
      "       SandR      SiltR      ClayR           FeR  \n",
      "0   0.626923   0.857056   0.708029    811.577541  \n",
      "1   4.450678   3.851341   2.797802   3334.739331  \n",
      "2  16.238105  14.757198  12.760148  12485.904320  \n",
      "\n",
      "Using 26 raster layers for CNN input.\n",
      "  - bui.tif\n",
      "  - ndsi.tif\n",
      "  - savi.tif\n",
      "  - ndbsi.tif\n",
      "  - ui.tif\n",
      "  - ndwi.tif\n",
      "  - ndbi.tif\n",
      "  - awei.tif\n",
      "  - evi.tif\n",
      "  - mndwi.tif\n",
      "  - ndvi.tif\n",
      "  - LULC2020.tif\n",
      "  - LULC2021.tif\n",
      "  - LULC2022.tif\n",
      "  - LULC2019.tif\n",
      "  - LULC2018.tif\n",
      "  - LULC2017.tif\n",
      "  - Pb_R.tif\n",
      "  - ClayR.tif\n",
      "  - SandR.tif\n",
      "  - CdR.tif\n",
      "  - CrR.tif\n",
      "  - AsR.tif\n",
      "  - SiltR.tif\n",
      "  - CuR.tif\n",
      "  - NiR.tif\n",
      "\n",
      "================================================================================\n",
      "Analyzing with Enhanced CNN–GNN–MLP Model (500m)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_38\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_38\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ cnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,520</span> │ cnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">20,832</span> │ cnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">40,800</span> │ cnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_41    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_43    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_45    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_41… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> │ max_pooling2d_43… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">100,416</span> │ max_pooling2d_45… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_42    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_44    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_46    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_21          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40000</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_42… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_22          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40000</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_44… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_23          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40000</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_46… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_combined        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120000</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ flatten_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ flatten_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_125 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,472</span> │ mlp_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_126 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,104</span> │ gnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15,360,128</span> │ cnn_combined[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense_125[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense_126[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_25      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cnn_out[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ mlp_out[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ gnn_out[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">691,840</span> │ concatenate_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_129 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ transformer_bloc… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_33          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_129[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_130 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_130[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ cnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m26\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_46 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │      \u001b[38;5;34m7,520\u001b[0m │ cnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_48 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │     \u001b[38;5;34m20,832\u001b[0m │ cnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_50 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │     \u001b[38;5;34m40,800\u001b[0m │ cnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_41    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_43    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_45    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_47 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_41… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_49 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    │     \u001b[38;5;34m51,264\u001b[0m │ max_pooling2d_43… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_51 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    │    \u001b[38;5;34m100,416\u001b[0m │ max_pooling2d_45… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_42    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_44    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_46    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_21          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40000\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_42… │\n",
       "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_22          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40000\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_44… │\n",
       "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_23          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40000\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_46… │\n",
       "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m110\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_combined        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120000\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ flatten_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ flatten_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ flatten_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_125 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m1,472\u001b[0m │ mlp_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_126 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m7,104\u001b[0m │ gnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_out (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │ \u001b[38;5;34m15,360,128\u001b[0m │ cnn_combined[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_out (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dense_125[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_out (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dense_126[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_25      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ cnn_out[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ mlp_out[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ gnn_out[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │    \u001b[38;5;34m691,840\u001b[0m │ concatenate_25[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_129 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m24,704\u001b[0m │ transformer_bloc… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_33          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_129[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_130 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_130[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,337,057</span> (62.32 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,337,057\u001b[0m (62.32 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,337,057</span> (62.32 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,337,057\u001b[0m (62.32 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 294ms/step - loss: 34723.8008 - val_loss: 23331.9414\n",
      "Epoch 2/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 280ms/step - loss: 22914.8848 - val_loss: 10303.9258\n",
      "Epoch 3/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 284ms/step - loss: 9632.6924 - val_loss: 4718.3516\n",
      "Epoch 4/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 281ms/step - loss: 6271.0171 - val_loss: 5185.2856\n",
      "Epoch 5/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 280ms/step - loss: 6240.5991 - val_loss: 4652.2095\n",
      "Epoch 6/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 278ms/step - loss: 5114.9873 - val_loss: 4708.8525\n",
      "Epoch 7/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 300ms/step - loss: 4945.1816 - val_loss: 4530.8457\n",
      "Epoch 8/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 279ms/step - loss: 5727.6230 - val_loss: 3625.7747\n",
      "Epoch 9/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 283ms/step - loss: 2996.6450 - val_loss: 2689.4751\n",
      "Epoch 10/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 278ms/step - loss: 3152.1204 - val_loss: 1318.2704\n",
      "Epoch 11/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 282ms/step - loss: 1478.1138 - val_loss: 824.2883\n",
      "Epoch 12/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 287ms/step - loss: 1203.4611 - val_loss: 892.1137\n",
      "Epoch 13/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 289ms/step - loss: 838.8353 - val_loss: 1155.0319\n",
      "Epoch 14/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 287ms/step - loss: 983.5536 - val_loss: 584.0251\n",
      "Epoch 15/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 285ms/step - loss: 778.6987 - val_loss: 562.7603\n",
      "Epoch 16/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 305ms/step - loss: 817.7516 - val_loss: 327.6863\n",
      "Epoch 17/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 281ms/step - loss: 744.0341 - val_loss: 399.9274\n",
      "Epoch 18/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 288ms/step - loss: 994.4224 - val_loss: 589.5069\n",
      "Epoch 19/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 289ms/step - loss: 680.7864 - val_loss: 368.8808\n",
      "Epoch 20/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 282ms/step - loss: 840.3652 - val_loss: 408.6573\n",
      "Epoch 21/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 286ms/step - loss: 710.5457 - val_loss: 339.5006\n",
      "Epoch 22/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 279ms/step - loss: 1212.3990 - val_loss: 519.6331\n",
      "Epoch 23/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 289ms/step - loss: 1137.7543 - val_loss: 589.1194\n",
      "Epoch 24/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 285ms/step - loss: 810.2007 - val_loss: 327.2043\n",
      "Epoch 25/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 288ms/step - loss: 679.5660 - val_loss: 301.2451\n",
      "Epoch 26/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 283ms/step - loss: 481.8894 - val_loss: 374.2705\n",
      "Epoch 27/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 283ms/step - loss: 656.6066 - val_loss: 443.4462\n",
      "Epoch 28/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 288ms/step - loss: 905.3644 - val_loss: 249.7935\n",
      "Epoch 29/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 287ms/step - loss: 475.8770 - val_loss: 177.6548\n",
      "Epoch 30/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 283ms/step - loss: 468.1436 - val_loss: 107.3018\n",
      "Epoch 31/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 287ms/step - loss: 267.5375 - val_loss: 193.5201\n",
      "Epoch 32/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 282ms/step - loss: 364.9856 - val_loss: 213.4180\n",
      "Epoch 33/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 285ms/step - loss: 608.8947 - val_loss: 172.6354\n",
      "Epoch 34/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 293ms/step - loss: 462.0377 - val_loss: 124.4818\n",
      "Epoch 35/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 291ms/step - loss: 361.2413 - val_loss: 259.8395\n",
      "Epoch 36/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 303ms/step - loss: 641.2083 - val_loss: 142.1660\n",
      "Epoch 37/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 300ms/step - loss: 573.2639 - val_loss: 244.7065\n",
      "Epoch 38/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 289ms/step - loss: 427.8917 - val_loss: 228.0634\n",
      "Epoch 39/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 293ms/step - loss: 591.1084 - val_loss: 649.3901\n",
      "Epoch 40/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 291ms/step - loss: 823.0609 - val_loss: 341.9646\n",
      "\n",
      "✅ Enhanced CNN–GNN–MLP Model Performance (500m):\n",
      "R² Train: -0.9433 | RMSE Train: 95.1857\n",
      "R² Test: 0.9168 | RMSE Test: 22.8137\n",
      "\n",
      "--------------------------------------------------\n",
      "Feature Importance Analysis for 500m\n",
      "--------------------------------------------------\n",
      "\n",
      "Baseline Performance on Test Set: R² = 0.9168\n",
      "\n",
      "--- Combined Feature Importance (by Model Branch) ---\n",
      "CNN Branch Importance (R² drop): 0.4542\n",
      "MLP Branch Importance (R² drop): 0.2628\n",
      "GNN Branch Importance (R² drop): -0.0179\n",
      "\n",
      "--- MLP Feature Importance (Permutation-based) ---\n",
      "PMF0_GWR            : 0.0254\n",
      "RI                  : 0.0215\n",
      "GWR_Adjusted        : 0.0194\n",
      "NiR                 : 0.0116\n",
      "num_industry        : 0.0101\n",
      "ClayR               : 0.0098\n",
      "PMF_Factor0         : 0.0094\n",
      "FeR                 : 0.0056\n",
      "CuR                 : 0.0034\n",
      "CdR                 : 0.0031\n",
      "PMF_Factor2         : 0.0028\n",
      "SandR               : 0.0015\n",
      "PbR                 : 0.0008\n",
      "MR                  : 0.0005\n",
      "hydro_dist_brick    : 0.0000\n",
      "hydro_dist_ind      : 0.0000\n",
      "PMF_Factor1         : -0.0001\n",
      "PMF2_GWR            : -0.0007\n",
      "CrR                 : -0.0008\n",
      "SiltR               : -0.0015\n",
      "PMF1_GWR            : -0.0029\n",
      "num_brick_field     : -0.0131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11364"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Concatenate,\n",
    "    Dropout,\n",
    "    Layer,\n",
    "    LayerNormalization\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "\n",
    "# Define the single buffer size to use for CNN patches\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data & Preprocessing ==================== #\n",
    "# Load the main dataset and the river sampling data.\n",
    "original = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "river_100.drop(columns=\"Source\", inplace=True)\n",
    "\n",
    "# Identify columns for feature engineering and prediction\n",
    "drop_cols = ['Stations', 'River', 'Lat', 'Long', 'geometry']\n",
    "numeric_cols = original.drop(columns=drop_cols).columns.drop('AsR')\n",
    "pmf_features = ['CrR', 'NiR', 'CuR', 'AsR', 'CdR', 'PbR', 'MR', 'SandR', 'SiltR', 'ClayR', 'FeR']\n",
    "\n",
    "# --- IMPUTATION FIX: Fill NaN values with 0 before further processing ---\n",
    "original.fillna(0, inplace=True)\n",
    "river_100.fillna(0, inplace=True)\n",
    "\n",
    "# Split original data into train and test sets for the ensemble model.\n",
    "np.random.seed(42)\n",
    "train_orig = original.sample(10, random_state=42)\n",
    "test_orig = original.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# Define the coordinates and target variables\n",
    "coords_train = train_combined[['Long', 'Lat']].values\n",
    "coords_test = test_orig[['Long', 'Lat']].values\n",
    "y_train = train_combined['RI'].values\n",
    "y_test = test_orig['RI'].values\n",
    "\n",
    "# ==================== 2. Feature Engineering from Model 1 ==================== #\n",
    "\n",
    "# --- 2.1 PMF (NMF) for Source Apportionment ---\n",
    "nmf = NMF(n_components=3, init='random', random_state=42, max_iter=1000)\n",
    "# Ensure data for NMF does not contain NaN or negative values\n",
    "G_train = nmf.fit_transform(np.maximum(train_combined[pmf_features].values, 0))\n",
    "F = nmf.components_\n",
    "print(\"\\nPMF Source Profiles (F):\\n\", pd.DataFrame(F, columns=pmf_features))\n",
    "\n",
    "# --- 2.2 Fixed Geographically Weighted Regression (GWR) ---\n",
    "def gaussian_kernel(d, bw):\n",
    "    return np.exp(-(d**2) / (2 * bw**2))\n",
    "\n",
    "def fixed_gwr(coords, factors, y, bw=0.5):\n",
    "    \"\"\"Performs a fixed bandwidth GWR using a Gaussian kernel.\"\"\"\n",
    "    n = len(coords)\n",
    "    preds = np.zeros(n)\n",
    "    X = np.hstack([np.ones((n, 1)), factors])\n",
    "    for i in range(n):\n",
    "        dist = np.linalg.norm(coords - coords[i], axis=1)\n",
    "        W = np.diag(gaussian_kernel(dist, bw))\n",
    "        try:\n",
    "            beta = np.linalg.pinv(X.T @ W @ X) @ (X.T @ W @ y.reshape(-1, 1))\n",
    "            preds[i] = (np.array([1] + list(factors[i])) @ beta).item()\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Handle singular matrix by using a simpler model\n",
    "            preds[i] = y.mean()\n",
    "    return preds.reshape(-1, 1)\n",
    "\n",
    "GWR_train = fixed_gwr(coords_train, G_train, y_train, bw=0.5)\n",
    "\n",
    "# --- 2.3 Interpolate PMF factors for the test set ---\n",
    "def idw_interpolation(known_coords, known_values, query_coords, power=2):\n",
    "    \"\"\"Performs IDW to interpolate values from known points to query points.\"\"\"\n",
    "    tree = cKDTree(known_coords)\n",
    "    dists, idxs = tree.query(query_coords, k=4)\n",
    "    dists[dists == 0] = 1e-10  # Avoid division by zero\n",
    "    weights = 1 / (dists ** power)\n",
    "    weights /= weights.sum(axis=1)[:, None]\n",
    "    return np.sum(weights * known_values[idxs], axis=1)\n",
    "\n",
    "G_test = np.column_stack([idw_interpolation(coords_train, G_train[:, i], coords_test) for i in range(G_train.shape[1])])\n",
    "\n",
    "# --- 2.4 Apply GWR to the interpolated PMF factors for the test set ---\n",
    "GWR_test = fixed_gwr(coords_test, G_test, y_test, bw=0.5)\n",
    "\n",
    "# --- 2.5 Interaction Features ---\n",
    "def create_interactions(pmf, gwr):\n",
    "    \"\"\"Creates interaction features between PMF factors and GWR predictions.\"\"\"\n",
    "    interactions = pd.DataFrame()\n",
    "    for i in range(pmf.shape[1]):\n",
    "        interactions[f\"PMF{i}_GWR\"] = pmf[:, i] * gwr.flatten()\n",
    "    return interactions\n",
    "\n",
    "train_interact = create_interactions(G_train, GWR_train)\n",
    "test_interact = create_interactions(G_test, GWR_test)\n",
    "\n",
    "# ==================== 3. Prepare GNN & MLP Input ==================== #\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_test_train = distance_matrix(coords_test, coords_train)\n",
    "gnn_test = np.exp(-dist_mat_test_train/10)\n",
    "\n",
    "mlp_data_train_raw = pd.DataFrame(\n",
    "    np.hstack([\n",
    "        train_combined[numeric_cols].values,\n",
    "        G_train,\n",
    "        GWR_train,\n",
    "        train_interact.values\n",
    "    ]),\n",
    "    columns=list(numeric_cols) + [f\"PMF_Factor{i}\" for i in range(G_train.shape[1])] + [\"GWR_Adjusted\"] + list(train_interact.columns)\n",
    ")\n",
    "\n",
    "mlp_data_test_raw = pd.DataFrame(\n",
    "    np.hstack([\n",
    "        test_orig[numeric_cols].values,\n",
    "        G_test,\n",
    "        GWR_test,\n",
    "        test_interact.values\n",
    "    ]),\n",
    "    columns=list(numeric_cols) + [f\"PMF_Factor{i}\" for i in range(G_test.shape[1])] + [\"GWR_Adjusted\"] + list(test_interact.columns)\n",
    ")\n",
    "\n",
    "# --- IMPUTATION FIX: Fill NaN in raw MLP data before scaling ---\n",
    "mlp_data_train_raw.fillna(0, inplace=True)\n",
    "mlp_data_test_raw.fillna(0, inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mlp_train = scaler.fit_transform(mlp_data_train_raw)\n",
    "mlp_test = scaler.transform(mlp_data_test_raw)\n",
    "\n",
    "# ==================== 4. Collect ALL Rasters for CNN ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(f\"\\nUsing {len(raster_paths)} raster layers for CNN input.\")\n",
    "for r in raster_paths:\n",
    "    print(\"  -\", os.path.basename(r))\n",
    "\n",
    "# ==================== 5. Create a Custom Data Generator ==================== #\n",
    "def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):\n",
    "    \"\"\"\n",
    "    Extracts a batch of patches from rasters for a given set of coordinates.\n",
    "    This function is optimized to be called by the data generator for each batch.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    for lon, lat in coords:\n",
    "        channels = []\n",
    "        for rfile in raster_files:\n",
    "            with rasterio.open(rfile) as src:\n",
    "                try:\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)\n",
    "                    arr = src.read(1, window=win, boundless=True, fill_value=0)\n",
    "                    arr = arr.astype(np.float32)\n",
    "\n",
    "                    # --- NORMALIZATION FIX: Add a small epsilon to avoid division by zero ---\n",
    "                    max_val = np.nanmax(arr)\n",
    "                    if max_val != 0:\n",
    "                        arr /= max_val + 1e-8 # Add epsilon for stability\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {rfile} for coordinates ({lon}, {lat}): {e}\")\n",
    "                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)\n",
    "            channels.append(arr)\n",
    "        patches.append(np.stack(channels, axis=-1))\n",
    "    \n",
    "    return np.array(patches)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.coords = coords\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.raster_paths = raster_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.buffer_meters = buffer_meters\n",
    "\n",
    "        with rasterio.open(raster_paths[0]) as src:\n",
    "            res_x, res_y = src.res\n",
    "            self.buffer_pixels_x = int(self.buffer_meters / res_x)\n",
    "            self.buffer_pixels_y = int(self.buffer_meters / res_y)\n",
    "            self.patch_width = 2 * self.buffer_pixels_x\n",
    "            self.patch_height = 2 * self.buffer_pixels_y\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "        batch_y = self.y[batch_indices]\n",
    "\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            self.raster_paths,\n",
    "            self.buffer_pixels_x,\n",
    "            self.buffer_pixels_y,\n",
    "            self.patch_width,\n",
    "            self.patch_height\n",
    "        )\n",
    "\n",
    "        return (batch_cnn, batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "# ==================== 6. Define Custom Transformer Layer ==================== #\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.expand_dims(inputs, axis=1)\n",
    "        attn_output = self.att(x, x)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return tf.squeeze(out2, axis=1)\n",
    "\n",
    "# ==================== 7. Define the New Fusion Model ==================== #\n",
    "def build_fusion_model(patch_shape, gnn_dim, mlp_dim):\n",
    "    # CNN input\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    cnn_3x3 = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_3x3 = MaxPooling2D((2,2))(cnn_3x3)\n",
    "    cnn_3x3 = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(cnn_3x3)\n",
    "    cnn_3x3 = MaxPooling2D((2,2))(cnn_3x3)\n",
    "    cnn_3x3 = Flatten()(cnn_3x3)\n",
    "\n",
    "    cnn_5x5 = Conv2D(32, (5,5), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_5x5 = MaxPooling2D((2,2))(cnn_5x5)\n",
    "    cnn_5x5 = Conv2D(64, (5,5), activation=\"relu\", padding=\"same\")(cnn_5x5)\n",
    "    cnn_5x5 = MaxPooling2D((2,2))(cnn_5x5)\n",
    "    cnn_5x5 = Flatten()(cnn_5x5)\n",
    "\n",
    "    cnn_7x7 = Conv2D(32, (7,7), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_7x7 = MaxPooling2D((2,2))(cnn_7x7)\n",
    "    cnn_7x7 = Conv2D(64, (7,7), activation=\"relu\", padding=\"same\")(cnn_7x7)\n",
    "    cnn_7x7 = MaxPooling2D((2,2))(cnn_7x7)\n",
    "    cnn_7x7 = Flatten()(cnn_7x7)\n",
    "\n",
    "    cnn_combined = Concatenate(name=\"cnn_combined\")([cnn_3x3, cnn_5x5, cnn_7x7])\n",
    "    cnn_out = Dense(128, activation=\"relu\", name=\"cnn_out\")(cnn_combined)\n",
    "\n",
    "    # MLP branch (for numerical site features)\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    m = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_out = Dense(32, activation=\"relu\", name=\"mlp_out\")(m)\n",
    "\n",
    "    # GNN branch (for spatial connectivity)\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    g = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    gnn_out = Dense(32, activation=\"relu\", name=\"gnn_out\")(g)\n",
    "\n",
    "    # Meta-learner (Transformer Block)\n",
    "    pre_transformer_features = Concatenate()([cnn_out, mlp_out, gnn_out])\n",
    "    \n",
    "    # Calculate the new embedding dimension\n",
    "    embed_dim = pre_transformer_features.shape[1]\n",
    "    \n",
    "    transformer_out = TransformerBlock(\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=4,\n",
    "        ff_dim=256\n",
    "    )(pre_transformer_features)\n",
    "    \n",
    "    # Final Fusion Layer\n",
    "    f = Dense(128, activation=\"relu\")(transformer_out)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(model, coords_test, mlp_test, gnn_test_matrix, y_test, raster_paths, buffer_meters, batch_size=4, return_preds=False):\n",
    "    num_samples = len(y_test)\n",
    "    y_pred_list = []\n",
    "    \n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, res_y = src.res\n",
    "        buffer_pixels_x = int(buffer_meters / res_x)\n",
    "        buffer_pixels_y = int(buffer_meters / res_y)\n",
    "        patch_width = 2 * buffer_pixels_x\n",
    "        patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_coords = coords_test[i:i+batch_size]\n",
    "        batch_mlp = mlp_test[i:i+batch_size]\n",
    "        \n",
    "        batch_gnn = gnn_test_matrix[i:i+batch_size, :]\n",
    "        batch_y = y_test[i:i+batch_size]\n",
    "\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            raster_paths,\n",
    "            buffer_pixels_x,\n",
    "            buffer_pixels_y,\n",
    "            patch_width,\n",
    "            patch_height\n",
    "        )\n",
    "        \n",
    "        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn), verbose=0).flatten())\n",
    "    \n",
    "    y_pred = np.concatenate(y_pred_list)\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        # --- NaN FIX: Ensure y_pred has no NaNs before calculating metrics ---\n",
    "        y_pred[np.isnan(y_pred)] = 0 # Replace NaNs with 0\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return r2, rmse\n",
    "\n",
    "# ==================== 8. Run the Analysis ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing with Enhanced CNN–GNN–MLP Model ({BUFFER_METERS}m)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "mlp_input_dim = mlp_train.shape[1]\n",
    "\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    cnn_patch_shape = (patch_width, patch_width, len(raster_paths))\n",
    "\n",
    "model = build_fusion_model(cnn_patch_shape, gnn_input_dim, mlp_input_dim)\n",
    "model.summary()\n",
    "\n",
    "# Create data generators\n",
    "train_generator = DataGenerator(\n",
    "    coords=coords_train,\n",
    "    mlp_data=mlp_train,\n",
    "    gnn_data=gnn_train,\n",
    "    y=y_train,\n",
    "    raster_paths=raster_paths,\n",
    "    buffer_meters=BUFFER_METERS,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=train_generator\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_train = model.predict(train_generator, verbose=0).flatten()\n",
    "# --- NaN FIX: Ensure y_pred has no NaNs before calculating metrics ---\n",
    "y_pred_train[np.isnan(y_pred_train)] = 0\n",
    "r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))\n",
    "\n",
    "r2_test, rmse_test = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size)\n",
    "\n",
    "print(f\"\\n✅ Enhanced CNN–GNN–MLP Model Performance ({BUFFER_METERS}m):\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# ==================== 9. Feature Importance Analysis ==================== #\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(f\"Feature Importance Analysis for {BUFFER_METERS}m\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# --- 9.1 Combined Feature Importance (by Model Branch) ---\n",
    "y_pred_baseline = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, return_preds=True)\n",
    "y_pred_baseline[np.isnan(y_pred_baseline)] = 0\n",
    "baseline_r2 = r2_score(y_test, y_pred_baseline)\n",
    "print(f\"\\nBaseline Performance on Test Set: R² = {baseline_r2:.4f}\")\n",
    "\n",
    "# Ablate CNN branch\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    buffer_pixels_y = int(BUFFER_METERS / res_y)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "cnn_test_ablated = np.zeros_like(extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "))\n",
    "y_pred_cnn_ablated = model.predict((cnn_test_ablated, mlp_test, gnn_test), verbose=0).flatten()\n",
    "y_pred_cnn_ablated[np.isnan(y_pred_cnn_ablated)] = 0\n",
    "r2_cnn_ablated = r2_score(y_test, y_pred_cnn_ablated)\n",
    "importance_cnn = baseline_r2 - r2_cnn_ablated\n",
    "\n",
    "# Ablate MLP branch\n",
    "mlp_test_ablated = np.zeros_like(mlp_test)\n",
    "y_pred_mlp_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test_ablated, gnn_test), verbose=0).flatten()\n",
    "y_pred_mlp_ablated[np.isnan(y_pred_mlp_ablated)] = 0\n",
    "r2_mlp_ablated = r2_score(y_test, y_pred_mlp_ablated)\n",
    "importance_mlp = baseline_r2 - r2_mlp_ablated\n",
    "\n",
    "# Ablate GNN branch\n",
    "gnn_test_ablated = np.zeros_like(gnn_test)\n",
    "y_pred_gnn_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test, gnn_test_ablated), verbose=0).flatten()\n",
    "y_pred_gnn_ablated[np.isnan(y_pred_gnn_ablated)] = 0\n",
    "r2_gnn_ablated = r2_score(y_test, y_pred_gnn_ablated)\n",
    "importance_gnn = baseline_r2 - r2_gnn_ablated\n",
    "\n",
    "print(\"\\n--- Combined Feature Importance (by Model Branch) ---\")\n",
    "print(f\"CNN Branch Importance (R² drop): {importance_cnn:.4f}\")\n",
    "print(f\"MLP Branch Importance (R² drop): {importance_mlp:.4f}\")\n",
    "print(f\"GNN Branch Importance (R² drop): {importance_gnn:.4f}\")\n",
    "\n",
    "# --- 9.2 MLP Feature Importance (Permutation-based) ---\n",
    "mlp_feature_importance = {}\n",
    "for i, feature_name in enumerate(mlp_data_train_raw.columns):\n",
    "    mlp_test_shuffled = np.copy(mlp_test)\n",
    "    np.random.shuffle(mlp_test_shuffled[:, i])\n",
    "    \n",
    "    y_pred_shuffled = model.predict((extract_patch_for_generator(\n",
    "        coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "    ), mlp_test_shuffled, gnn_test), verbose=0).flatten()\n",
    "    y_pred_shuffled[np.isnan(y_pred_shuffled)] = 0\n",
    "    r2_shuffled = r2_score(y_test, y_pred_shuffled)\n",
    "    \n",
    "    importance = baseline_r2 - r2_shuffled\n",
    "    mlp_feature_importance[feature_name] = importance\n",
    "\n",
    "print(\"\\n--- MLP Feature Importance (Permutation-based) ---\")\n",
    "sorted_importance = sorted(mlp_feature_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, importance in sorted_importance:\n",
    "    print(f\"{feature:<20}: {importance:.4f}\")\n",
    "    \n",
    "# Garbage collect to free up memory\n",
    "del model, history, train_generator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dac9f54e-9cca-4278-a228-ce7304dc2578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_37\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_37\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ cnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,520</span> │ cnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">20,832</span> │ cnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">40,800</span> │ cnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_38    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_40    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_42    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_38… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> │ max_pooling2d_40… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">100,416</span> │ max_pooling2d_42… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_39    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_41    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_43    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_21          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40000</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_39… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_22          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40000</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_41… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_23          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40000</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_43… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_combined        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120000</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ flatten_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ flatten_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_143 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,472</span> │ mlp_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_144 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,104</span> │ gnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15,360,128</span> │ cnn_combined[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense_143[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense_144[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_19      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cnn_out[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ mlp_out[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ gnn_out[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block_1 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">691,840</span> │ concatenate_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_147 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ transformer_bloc… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_81          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_147[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_148 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_148[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ cnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m26\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_41 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │      \u001b[38;5;34m7,520\u001b[0m │ cnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_43 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │     \u001b[38;5;34m20,832\u001b[0m │ cnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_45 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │     \u001b[38;5;34m40,800\u001b[0m │ cnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_38    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_40    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_42    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_42 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_38… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_44 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    │     \u001b[38;5;34m51,264\u001b[0m │ max_pooling2d_40… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_46 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    │    \u001b[38;5;34m100,416\u001b[0m │ max_pooling2d_42… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_39    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_41    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_43    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_21          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40000\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_39… │\n",
       "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_22          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40000\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_41… │\n",
       "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_23          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40000\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_43… │\n",
       "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m110\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_combined        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120000\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ flatten_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ flatten_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ flatten_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_143 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m1,472\u001b[0m │ mlp_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_144 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m7,104\u001b[0m │ gnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_out (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │ \u001b[38;5;34m15,360,128\u001b[0m │ cnn_combined[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_out (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dense_143[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_out (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dense_144[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_19      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ cnn_out[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ mlp_out[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ gnn_out[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block_1 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │    \u001b[38;5;34m691,840\u001b[0m │ concatenate_19[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_147 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m24,704\u001b[0m │ transformer_bloc… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_81          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_147[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_148 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_81[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_148[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,337,057</span> (62.32 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,337,057\u001b[0m (62.32 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,337,057</span> (62.32 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,337,057\u001b[0m (62.32 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "15097"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Concatenate,\n",
    "    Dropout,\n",
    "    Layer,\n",
    "    LayerNormalization\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "# Define the single buffer size to use for CNN patches\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data & Preprocessing ==================== #\n",
    "# Load the main dataset and the river sampling data.\n",
    "original = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "river_100.drop(columns=\"Source\", inplace=True)\n",
    "\n",
    "# Identify columns for feature engineering and prediction\n",
    "drop_cols = ['Stations', 'River', 'Lat', 'Long', 'geometry']\n",
    "numeric_cols = original.drop(columns=drop_cols).columns.drop('AsR')\n",
    "pmf_features = ['CrR', 'NiR', 'CuR', 'AsR', 'CdR', 'PbR', 'MR', 'SandR', 'SiltR', 'ClayR', 'FeR']\n",
    "\n",
    "# --- IMPUTATION FIX: Fill NaN values with 0 before further processing ---\n",
    "original.fillna(0, inplace=True)\n",
    "river_100.fillna(0, inplace=True)\n",
    "\n",
    "# Split original data into train and test sets for the ensemble model.\n",
    "np.random.seed(42)\n",
    "train_orig = original.sample(10, random_state=42)\n",
    "test_orig = original.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# Define the coordinates and target variables\n",
    "coords_train = train_combined[['Long', 'Lat']].values\n",
    "coords_test = test_orig[['Long', 'Lat']].values\n",
    "y_train = train_combined['RI'].values\n",
    "y_test = test_orig['RI'].values\n",
    "\n",
    "# ==================== 2. Feature Engineering from Model 1 ==================== #\n",
    "\n",
    "# --- 2.1 PMF (NMF) for Source Apportionment ---\n",
    "nmf = NMF(n_components=3, init='random', random_state=42, max_iter=1000)\n",
    "# Ensure data for NMF does not contain NaN or negative values\n",
    "G_train = nmf.fit_transform(np.maximum(train_combined[pmf_features].values, 0))\n",
    "F = nmf.components_\n",
    "print(\"\\nPMF Source Profiles (F):\\n\", pd.DataFrame(F, columns=pmf_features))\n",
    "\n",
    "# --- 2.2 Fixed Geographically Weighted Regression (GWR) ---\n",
    "def gaussian_kernel(d, bw):\n",
    "    return np.exp(-(d**2) / (2 * bw**2))\n",
    "\n",
    "def fixed_gwr(coords, factors, y, bw=0.5):\n",
    "    \"\"\"Performs a fixed bandwidth GWR using a Gaussian kernel.\"\"\"\n",
    "    n = len(coords)\n",
    "    preds = np.zeros(n)\n",
    "    X = np.hstack([np.ones((n, 1)), factors])\n",
    "    for i in range(n):\n",
    "        dist = np.linalg.norm(coords - coords[i], axis=1)\n",
    "        W = np.diag(gaussian_kernel(dist, bw))\n",
    "        try:\n",
    "            beta = np.linalg.pinv(X.T @ W @ X) @ (X.T @ W @ y.reshape(-1, 1))\n",
    "            preds[i] = (np.array([1] + list(factors[i])) @ beta).item()\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Handle singular matrix by using a simpler model\n",
    "            preds[i] = y.mean()\n",
    "    return preds.reshape(-1, 1)\n",
    "\n",
    "GWR_train = fixed_gwr(coords_train, G_train, y_train, bw=0.5)\n",
    "\n",
    "# --- 2.3 Interpolate PMF factors for the test set ---\n",
    "def idw_interpolation(known_coords, known_values, query_coords, power=2):\n",
    "    \"\"\"Performs IDW to interpolate values from known points to query points.\"\"\"\n",
    "    tree = cKDTree(known_coords)\n",
    "    dists, idxs = tree.query(query_coords, k=4)\n",
    "    dists[dists == 0] = 1e-10  # Avoid division by zero\n",
    "    weights = 1 / (dists ** power)\n",
    "    weights /= weights.sum(axis=1)[:, None]\n",
    "    return np.sum(weights * known_values[idxs], axis=1)\n",
    "\n",
    "G_test = np.column_stack([idw_interpolation(coords_train, G_train[:, i], coords_test) for i in range(G_train.shape[1])])\n",
    "\n",
    "# --- 2.4 Apply GWR to the interpolated PMF factors for the test set ---\n",
    "GWR_test = fixed_gwr(coords_test, G_test, y_test, bw=0.5)\n",
    "\n",
    "# --- 2.5 Interaction Features ---\n",
    "def create_interactions(pmf, gwr):\n",
    "    \"\"\"Creates interaction features between PMF factors and GWR predictions.\"\"\"\n",
    "    interactions = pd.DataFrame()\n",
    "    for i in range(pmf.shape[1]):\n",
    "        interactions[f\"PMF{i}_GWR\"] = pmf[:, i] * gwr.flatten()\n",
    "    return interactions\n",
    "\n",
    "train_interact = create_interactions(G_train, GWR_train)\n",
    "test_interact = create_interactions(G_test, GWR_test)\n",
    "\n",
    "# ==================== 3. Prepare GNN & MLP Input ==================== #\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_test_train = distance_matrix(coords_test, coords_train)\n",
    "gnn_test = np.exp(-dist_mat_test_train/10)\n",
    "\n",
    "mlp_data_train_raw = pd.DataFrame(\n",
    "    np.hstack([\n",
    "        train_combined[numeric_cols].values,\n",
    "        G_train,\n",
    "        GWR_train,\n",
    "        train_interact.values\n",
    "    ]),\n",
    "    columns=list(numeric_cols) + [f\"PMF_Factor{i}\" for i in range(G_train.shape[1])] + [\"GWR_Adjusted\"] + list(train_interact.columns)\n",
    ")\n",
    "\n",
    "mlp_data_test_raw = pd.DataFrame(\n",
    "    np.hstack([\n",
    "        test_orig[numeric_cols].values,\n",
    "        G_test,\n",
    "        GWR_test,\n",
    "        test_interact.values\n",
    "    ]),\n",
    "    columns=list(numeric_cols) + [f\"PMF_Factor{i}\" for i in range(G_test.shape[1])] + [\"GWR_Adjusted\"] + list(test_interact.columns)\n",
    ")\n",
    "\n",
    "# --- IMPUTATION FIX: Fill NaN in raw MLP data before scaling ---\n",
    "mlp_data_train_raw.fillna(0, inplace=True)\n",
    "mlp_data_test_raw.fillna(0, inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mlp_train = scaler.fit_transform(mlp_data_train_raw)\n",
    "mlp_test = scaler.transform(mlp_data_test_raw)\n",
    "\n",
    "# ==================== 4. Collect ALL Rasters for CNN ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(f\"\\nUsing {len(raster_paths)} raster layers for CNN input.\")\n",
    "for r in raster_paths:\n",
    "    print(\"  -\", os.path.basename(r))\n",
    "\n",
    "# ==================== 5. Create a Custom Data Generator ==================== #\n",
    "def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):\n",
    "    \"\"\"\n",
    "    Extracts a batch of patches from rasters for a given set of coordinates.\n",
    "    This function is optimized to be called by the data generator for each batch.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    for lon, lat in coords:\n",
    "        channels = []\n",
    "        for rfile in raster_files:\n",
    "            with rasterio.open(rfile) as src:\n",
    "                try:\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)\n",
    "                    arr = src.read(1, window=win, boundless=True, fill_value=0)\n",
    "                    arr = arr.astype(np.float32)\n",
    "\n",
    "                    # --- NORMALIZATION FIX: Add a small epsilon to avoid division by zero ---\n",
    "                    max_val = np.nanmax(arr)\n",
    "                    if max_val != 0:\n",
    "                        arr /= max_val + 1e-8 # Add epsilon for stability\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {rfile} for coordinates ({lon}, {lat}): {e}\")\n",
    "                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)\n",
    "            channels.append(arr)\n",
    "        patches.append(np.stack(channels, axis=-1))\n",
    "    \n",
    "    return np.array(patches)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.coords = coords\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.raster_paths = raster_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.buffer_meters = buffer_meters\n",
    "\n",
    "        with rasterio.open(raster_paths[0]) as src:\n",
    "            res_x, res_y = src.res\n",
    "            self.buffer_pixels_x = int(self.buffer_meters / res_x)\n",
    "            self.buffer_pixels_y = int(self.buffer_meters / res_y)\n",
    "            self.patch_width = 2 * self.buffer_pixels_x\n",
    "            self.patch_height = 2 * self.buffer_pixels_y\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "        batch_y = self.y[batch_indices]\n",
    "\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            self.raster_paths,\n",
    "            self.buffer_pixels_x,\n",
    "            self.buffer_pixels_y,\n",
    "            self.patch_width,\n",
    "            self.patch_height\n",
    "        )\n",
    "\n",
    "        return (batch_cnn, batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "# ==================== 6. Define Custom Transformer Layer ==================== #\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.expand_dims(inputs, axis=1)\n",
    "        attn_output = self.att(x, x)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return tf.squeeze(out2, axis=1)\n",
    "\n",
    "# ==================== 7. Define the New Fusion Model ==================== #\n",
    "def build_fusion_model(patch_shape, gnn_dim, mlp_dim):\n",
    "    # CNN input\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    cnn_3x3 = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_3x3 = MaxPooling2D((2,2))(cnn_3x3)\n",
    "    cnn_3x3 = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(cnn_3x3)\n",
    "    cnn_3x3 = MaxPooling2D((2,2))(cnn_3x3)\n",
    "    cnn_3x3 = Flatten()(cnn_3x3)\n",
    "\n",
    "    cnn_5x5 = Conv2D(32, (5,5), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_5x5 = MaxPooling2D((2,2))(cnn_5x5)\n",
    "    cnn_5x5 = Conv2D(64, (5,5), activation=\"relu\", padding=\"same\")(cnn_5x5)\n",
    "    cnn_5x5 = MaxPooling2D((2,2))(cnn_5x5)\n",
    "    cnn_5x5 = Flatten()(cnn_5x5)\n",
    "\n",
    "    cnn_7x7 = Conv2D(32, (7,7), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_7x7 = MaxPooling2D((2,2))(cnn_7x7)\n",
    "    cnn_7x7 = Conv2D(64, (7,7), activation=\"relu\", padding=\"same\")(cnn_7x7)\n",
    "    cnn_7x7 = MaxPooling2D((2,2))(cnn_7x7)\n",
    "    cnn_7x7 = Flatten()(cnn_7x7)\n",
    "\n",
    "    cnn_combined = Concatenate(name=\"cnn_combined\")([cnn_3x3, cnn_5x5, cnn_7x7])\n",
    "    cnn_out = Dense(128, activation=\"relu\", name=\"cnn_out\")(cnn_combined)\n",
    "\n",
    "    # MLP branch (for numerical site features)\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    m = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_out = Dense(32, activation=\"relu\", name=\"mlp_out\")(m)\n",
    "\n",
    "    # GNN branch (for spatial connectivity)\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    g = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    gnn_out = Dense(32, activation=\"relu\", name=\"gnn_out\")(g)\n",
    "\n",
    "    # Meta-learner (Transformer Block)\n",
    "    pre_transformer_features = Concatenate()([cnn_out, mlp_out, gnn_out])\n",
    "    \n",
    "    # Calculate the new embedding dimension\n",
    "    embed_dim = pre_transformer_features.shape[1]\n",
    "    \n",
    "    transformer_out = TransformerBlock(\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=4,\n",
    "        ff_dim=256\n",
    "    )(pre_transformer_features)\n",
    "    \n",
    "    # Final Fusion Layer\n",
    "    f = Dense(128, activation=\"relu\")(transformer_out)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(model, coords_test, mlp_test, gnn_test_matrix, y_test, raster_paths, buffer_meters, batch_size=4, return_preds=False):\n",
    "    num_samples = len(y_test)\n",
    "    y_pred_list = []\n",
    "    \n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, res_y = src.res\n",
    "        buffer_pixels_x = int(buffer_meters / res_x)\n",
    "        buffer_pixels_y = int(buffer_meters / res_y)\n",
    "        patch_width = 2 * buffer_pixels_x\n",
    "        patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_coords = coords_test[i:i+batch_size]\n",
    "        batch_mlp = mlp_test[i:i+batch_size]\n",
    "        \n",
    "        batch_gnn = gnn_test_matrix[i:i+batch_size, :]\n",
    "        batch_y = y_test[i:i+batch_size]\n",
    "\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            raster_paths,\n",
    "            buffer_pixels_x,\n",
    "            buffer_pixels_y,\n",
    "            patch_width,\n",
    "            patch_height\n",
    "        )\n",
    "        \n",
    "        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn), verbose=0).flatten())\n",
    "    \n",
    "    y_pred = np.concatenate(y_pred_list)\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        # --- NaN FIX: Ensure y_pred has no NaNs before calculating metrics ---\n",
    "        y_pred[np.isnan(y_pred)] = 0 # Replace NaNs with 0\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return r2, rmse\n",
    "\n",
    "# ==================== 8. Run the Analysis ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing with Enhanced CNN–GNN–MLP Model ({BUFFER_METERS}m)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "mlp_input_dim = mlp_train.shape[1]\n",
    "\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    cnn_patch_shape = (patch_width, patch_width, len(raster_paths))\n",
    "\n",
    "model = build_fusion_model(cnn_patch_shape, gnn_input_dim, mlp_input_dim)\n",
    "model.summary()\n",
    "\n",
    "# Create data generators\n",
    "train_generator = DataGenerator(\n",
    "    coords=coords_train,\n",
    "    mlp_data=mlp_train,\n",
    "    gnn_data=gnn_train,\n",
    "    y=y_train,\n",
    "    raster_paths=raster_paths,\n",
    "    buffer_meters=BUFFER_METERS,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=train_generator\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_train = model.predict(train_generator, verbose=0).flatten()\n",
    "# --- NaN FIX: Ensure y_pred has no NaNs before calculating metrics ---\n",
    "y_pred_train[np.isnan(y_pred_train)] = 0\n",
    "r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))\n",
    "\n",
    "r2_test, rmse_test = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size)\n",
    "\n",
    "print(f\"\\n✅ Enhanced CNN–GNN–MLP Model Performance ({BUFFER_METERS}m):\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# ==================== 9. Feature Importance Analysis ==================== #\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(f\"Feature Importance Analysis for {BUFFER_METERS}m\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# --- 9.1 Combined Feature Importance (by Model Branch) ---\n",
    "y_pred_baseline = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, return_preds=True)\n",
    "y_pred_baseline[np.isnan(y_pred_baseline)] = 0\n",
    "baseline_r2 = r2_score(y_test, y_pred_baseline)\n",
    "print(f\"\\nBaseline Performance on Test Set: R² = {baseline_r2:.4f}\")\n",
    "\n",
    "# Ablate CNN branch\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    buffer_pixels_y = int(BUFFER_METERS / res_y)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "cnn_test_ablated = np.zeros_like(extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "))\n",
    "y_pred_cnn_ablated = model.predict((cnn_test_ablated, mlp_test, gnn_test), verbose=0).flatten()\n",
    "y_pred_cnn_ablated[np.isnan(y_pred_cnn_ablated)] = 0\n",
    "r2_cnn_ablated = r2_score(y_test, y_pred_cnn_ablated)\n",
    "importance_cnn = baseline_r2 - r2_cnn_ablated\n",
    "\n",
    "# Ablate MLP branch\n",
    "mlp_test_ablated = np.zeros_like(mlp_test)\n",
    "y_pred_mlp_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test_ablated, gnn_test), verbose=0).flatten()\n",
    "y_pred_mlp_ablated[np.isnan(y_pred_mlp_ablated)] = 0\n",
    "r2_mlp_ablated = r2_score(y_test, y_pred_mlp_ablated)\n",
    "importance_mlp = baseline_r2 - r2_mlp_ablated\n",
    "\n",
    "# Ablate GNN branch\n",
    "gnn_test_ablated = np.zeros_like(gnn_test)\n",
    "y_pred_gnn_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test, gnn_test_ablated), verbose=0).flatten()\n",
    "y_pred_gnn_ablated[np.isnan(y_pred_gnn_ablated)] = 0\n",
    "r2_gnn_ablated = r2_score(y_test, y_pred_gnn_ablated)\n",
    "importance_gnn = baseline_r2 - r2_gnn_ablated\n",
    "\n",
    "print(\"\\n--- Combined Feature Importance (by Model Branch) ---\")\n",
    "print(f\"CNN Branch Importance (R² drop): {importance_cnn:.4f}\")\n",
    "print(f\"MLP Branch Importance (R² drop): {importance_mlp:.4f}\")\n",
    "print(f\"GNN Branch Importance (R² drop): {importance_gnn:.4f}\")\n",
    "\n",
    "# --- 9.2 MLP Feature Importance (Permutation-based) ---\n",
    "mlp_feature_importance = {}\n",
    "for i, feature_name in enumerate(mlp_data_train_raw.columns):\n",
    "    mlp_test_shuffled = np.copy(mlp_test)\n",
    "    np.random.shuffle(mlp_test_shuffled[:, i])\n",
    "    \n",
    "    y_pred_shuffled = model.predict((extract_patch_for_generator(\n",
    "        coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "    ), mlp_test_shuffled, gnn_test), verbose=0).flatten()\n",
    "    y_pred_shuffled[np.isnan(y_pred_shuffled)] = 0\n",
    "    r2_shuffled = r2_score(y_test, y_pred_shuffled)\n",
    "    \n",
    "    importance = baseline_r2 - r2_shuffled\n",
    "    mlp_feature_importance[feature_name] = importance\n",
    "\n",
    "print(\"\\n--- MLP Feature Importance (Permutation-based) ---\")\n",
    "sorted_importance = sorted(mlp_feature_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, importance in sorted_importance:\n",
    "    print(f\"{feature:<20}: {importance:.4f}\")\n",
    "    \n",
    "# ==================== 10. Save Model and Data for Reproducibility ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Saving Model, Data, and Feature Importance Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create the single output directory\n",
    "output_dir = \"cnn_gnn_mlp_pg\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the trained model in the Keras native format\n",
    "model_filename = os.path.join(output_dir, f\"fusion_model_{BUFFER_METERS}m.keras\")\n",
    "model.save(model_filename)\n",
    "print(f\"✅ Model saved to '{model_filename}'\")\n",
    "\n",
    "# Save the training history using pickle\n",
    "history_filename = os.path.join(output_dir, \"training_history.pkl\")\n",
    "with open(history_filename, 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(f\"✅ Training history saved to '{history_filename}'\")\n",
    "\n",
    "# --- New: Save Feature Importance Results ---\n",
    "feature_importance_results = {\n",
    "    \"mlp_feature_names\": mlp_data_train_raw.columns.tolist(),\n",
    "    \"mlp_permutation_importance\": mlp_feature_importance,\n",
    "    \"cnn_ablation_importance\": importance_cnn,\n",
    "    \"mlp_ablation_importance\": importance_mlp,\n",
    "    \"gnn_ablation_importance\": importance_gnn\n",
    "}\n",
    "importance_filename = os.path.join(output_dir, \"feature_importance.pkl\")\n",
    "with open(importance_filename, 'wb') as f:\n",
    "    pickle.dump(feature_importance_results, f)\n",
    "print(f\"✅ Feature importance results saved to '{importance_filename}'\")\n",
    "\n",
    "# Save processed NumPy arrays for later use\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, \"processed_train_data.npz\"),\n",
    "    coords=coords_train,\n",
    "    mlp=mlp_train,\n",
    "    y=y_train\n",
    ")\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, \"processed_test_data.npz\"),\n",
    "    coords=coords_test,\n",
    "    mlp=mlp_test,\n",
    "    y=y_test\n",
    ")\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, \"gnn_data.npz\"),\n",
    "    gnn_train=gnn_train,\n",
    "    gnn_test=gnn_test\n",
    ")\n",
    "print(f\"✅ Processed data arrays saved to '{output_dir}'\")\n",
    "\n",
    "# Save the raw dataframes to CSV for easy inspection\n",
    "train_combined.to_csv(os.path.join(output_dir, \"train_combined.csv\"), index=False)\n",
    "test_orig.to_csv(os.path.join(output_dir, \"test_orig.csv\"), index=False)\n",
    "print(f\"✅ Raw dataframes saved to '{output_dir}'\")\n",
    "\n",
    "# Garbage collect to free up memory\n",
    "del model, history, train_generator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a6c37-f3f6-460b-928b-c96cf6a875fd",
   "metadata": {},
   "source": [
    "# CNN GNN MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f773f64d-634f-4d43-b5be-bee199f66e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_38\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_38\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ cnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,520</span> │ cnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_44    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_44… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_45    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_24          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33856</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_45… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_149 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ mlp_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_150 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,104</span> │ gnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,333,696</span> │ flatten_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense_149[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense_150[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_20      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cnn_out[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ mlp_out[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ gnn_out[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_151 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ concatenate_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_82          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_151[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_152 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_152[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ cnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m26\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_47 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m98\u001b[0m,    │      \u001b[38;5;34m7,520\u001b[0m │ cnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_44    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m49\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_48 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m47\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_44… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_45    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m110\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_24          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33856\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_45… │\n",
       "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_149 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m1,024\u001b[0m │ mlp_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_150 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m7,104\u001b[0m │ gnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_out (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m4,333,696\u001b[0m │ flatten_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_out (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dense_149[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_out (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dense_150[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_20      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ cnn_out[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ mlp_out[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ gnn_out[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_151 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m24,704\u001b[0m │ concatenate_20[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_82          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_151[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_152 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_82[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_152[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,405,025</span> (16.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,405,025\u001b[0m (16.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,405,025</span> (16.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,405,025\u001b[0m (16.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "21309"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "# Define the buffer size in meters\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "river_100.drop(columns=\"Source\", inplace=True)\n",
    "\n",
    "drop_cols = ['Stations', 'River', 'Lat', 'Long', 'geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# --- IMPUTATION FIX: Fill NaN values with 0 before further processing ---\n",
    "orig.fillna(0, inplace=True)\n",
    "river_100.fillna(0, inplace=True)\n",
    "\n",
    "# Train-test split\n",
    "np.random.seed(42)\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(f\"Using {len(raster_paths)} raster layers for CNN input.\")\n",
    "for r in raster_paths:\n",
    "    print(\"  -\", os.path.basename(r))\n",
    "\n",
    "# ==================== 3. Create a Custom Data Generator ==================== #\n",
    "def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):\n",
    "    \"\"\"\n",
    "    Extracts a batch of patches from rasters for a given set of coordinates.\n",
    "    This function is optimized to be called by the data generator for each batch.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    # Loop through each coordinate pair in the batch\n",
    "    for lon, lat in coords:\n",
    "        channels = []\n",
    "        # Loop through each raster file to get a single patch for each raster\n",
    "        for rfile in raster_files:\n",
    "            with rasterio.open(rfile) as src:\n",
    "                try:\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)\n",
    "                    arr = src.read(1, window=win, boundless=True, fill_value=0)\n",
    "                    arr = arr.astype(np.float32)\n",
    "\n",
    "                    # --- NORMALIZATION FIX: Add a small epsilon to avoid division by zero ---\n",
    "                    max_val = np.nanmax(arr)\n",
    "                    if max_val != 0:\n",
    "                        arr /= max_val + 1e-8 # Add epsilon for stability\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {rfile} for coordinates ({lon}, {lat}): {e}\")\n",
    "                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)\n",
    "            channels.append(arr)\n",
    "        patches.append(np.stack(channels, axis=-1))\n",
    "    \n",
    "    return np.array(patches)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, batch_size=4, shuffle=True, buffer_meters=BUFFER_METERS, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.coords = coords\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.raster_paths = raster_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.buffer_meters = buffer_meters\n",
    "\n",
    "        # Pre-calculate patch size from the first raster\n",
    "        with rasterio.open(raster_paths[0]) as src:\n",
    "            res_x, res_y = src.res\n",
    "            self.buffer_pixels_x = int(self.buffer_meters / res_x)\n",
    "            self.buffer_pixels_y = int(self.buffer_meters / res_y)\n",
    "            self.patch_width = 2 * self.buffer_pixels_x\n",
    "            self.patch_height = 2 * self.buffer_pixels_y\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        \n",
    "        # Slice the GNN adjacency matrix for the current batch\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "\n",
    "        batch_y = self.y[batch_indices]\n",
    "\n",
    "        # Extract CNN patches for the current batch\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            self.raster_paths,\n",
    "            self.buffer_pixels_x,\n",
    "            self.buffer_pixels_y,\n",
    "            self.patch_width,\n",
    "            self.patch_height\n",
    "        )\n",
    "\n",
    "        # Return a tuple of inputs and the target, which Keras expects\n",
    "        return (batch_cnn, batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #\n",
    "coords_train = train_combined[['Long', 'Lat']].values\n",
    "coords_test = test_orig[['Long', 'Lat']].values\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_test_train = distance_matrix(coords_test, coords_train)\n",
    "gnn_test = np.exp(-dist_mat_test_train/10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# --- IMPUTATION FIX: Fill NaN in raw MLP data before scaling ---\n",
    "train_combined.fillna(0, inplace=True)\n",
    "test_orig.fillna(0, inplace=True)\n",
    "mlp_train = scaler.fit_transform(train_combined[numeric_cols])\n",
    "mlp_test = scaler.transform(test_orig[numeric_cols])\n",
    "y_train = train_combined['RI'].values\n",
    "y_test = test_orig['RI'].values\n",
    "\n",
    "# ==================== 5. Define Enhanced CNN–GNN–MLP Model ==================== #\n",
    "def build_fusion_model(patch_shape, gnn_dim, mlp_dim):\n",
    "    # CNN branch (for raster data)\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    x = Conv2D(32, (3,3), activation=\"relu\")(cnn_input)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(64, (3,3), activation=\"relu\")(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Flatten()(x)\n",
    "    cnn_out = Dense(128, activation=\"relu\", name=\"cnn_out\")(x)\n",
    "\n",
    "    # MLP branch (for numerical site features)\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    m = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_out = Dense(32, activation=\"relu\", name=\"mlp_out\")(m)\n",
    "\n",
    "    # GNN branch (for spatial connectivity)\n",
    "    # The GNN input dimension is now the number of training samples\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    g = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    gnn_out = Dense(32, activation=\"relu\", name=\"gnn_out\")(g)\n",
    "\n",
    "    # Fusion Layer\n",
    "    combined = Concatenate()([cnn_out, mlp_out, gnn_out])\n",
    "    f = Dense(128, activation=\"relu\")(combined)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# We need to determine the final GNN input dimension for the model\n",
    "# It's the total number of training samples\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "\n",
    "# Helper function to get CNN patch shape from rasters\n",
    "def get_cnn_patch_shape(raster_paths, buffer_meters):\n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, _ = src.res\n",
    "        buffer_pixels = int(buffer_meters / res_x)\n",
    "        return (2 * buffer_pixels, 2 * buffer_pixels, len(raster_paths))\n",
    "\n",
    "cnn_patch_shape = get_cnn_patch_shape(raster_paths, BUFFER_METERS)\n",
    "model = build_fusion_model(cnn_patch_shape, gnn_input_dim, mlp_train.shape[1])\n",
    "model.summary()\n",
    "\n",
    "# ==================== 6. Create Data Generators ==================== #\n",
    "# We create a separate generator for the validation data.\n",
    "train_generator = DataGenerator(\n",
    "    coords=coords_train,\n",
    "    mlp_data=mlp_train,\n",
    "    gnn_data=gnn_train,\n",
    "    y=y_train,\n",
    "    raster_paths=raster_paths,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    buffer_meters=BUFFER_METERS\n",
    ")\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(model, coords_test, mlp_test, gnn_test_matrix, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=4, return_preds=False):\n",
    "    num_samples = len(y_test)\n",
    "    y_pred_list = []\n",
    "    \n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, res_y = src.res\n",
    "        buffer_pixels_x = int(buffer_meters / res_x)\n",
    "        buffer_pixels_y = int(buffer_meters / res_y)\n",
    "        patch_width = 2 * buffer_pixels_x\n",
    "        patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_coords = coords_test[i:i+batch_size]\n",
    "        batch_mlp = mlp_test[i:i+batch_size]\n",
    "        \n",
    "        batch_gnn = gnn_test_matrix[i:i+batch_size, :]\n",
    "        batch_y = y_test[i:i+batch_size]\n",
    "\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            raster_paths,\n",
    "            buffer_pixels_x,\n",
    "            buffer_pixels_y,\n",
    "            patch_width,\n",
    "            patch_height\n",
    "        )\n",
    "        \n",
    "        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn), verbose=0).flatten())\n",
    "    \n",
    "    y_pred = np.concatenate(y_pred_list)\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        # --- NaN FIX: Ensure y_pred has no NaNs before calculating metrics ---\n",
    "        y_pred[np.isnan(y_pred)] = 0\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return r2, rmse\n",
    "\n",
    "\n",
    "# ==================== 7. Train Model ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing with CNN–GNN–MLP Model ({BUFFER_METERS}m)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=train_generator # Using the same generator for validation for this example\n",
    ")\n",
    "\n",
    "\n",
    "# ==================== 8. Evaluate ==================== #\n",
    "# Re-create a data generator without shuffling for evaluation on the training set\n",
    "train_eval_generator = DataGenerator(\n",
    "    coords=coords_train,\n",
    "    mlp_data=mlp_train,\n",
    "    gnn_data=gnn_train,\n",
    "    y=y_train,\n",
    "    raster_paths=raster_paths,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    buffer_meters=BUFFER_METERS\n",
    ")\n",
    "\n",
    "y_pred_train = model.predict(train_eval_generator, verbose=0).flatten()\n",
    "# --- NaN FIX: Ensure y_pred has no NaNs before calculating metrics ---\n",
    "y_pred_train[np.isnan(y_pred_train)] = 0\n",
    "r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))\n",
    "\n",
    "r2_test, rmse_test = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size)\n",
    "\n",
    "print(f\"\\n✅ CNN–GNN–MLP Model Performance ({BUFFER_METERS}m):\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# ==================== 9. Feature Importance Analysis ==================== #\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(f\"Feature Importance Analysis for {BUFFER_METERS}m\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# --- 9.1 Combined Feature Importance (by Model Branch) ---\n",
    "y_pred_baseline = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, return_preds=True)\n",
    "y_pred_baseline[np.isnan(y_pred_baseline)] = 0\n",
    "baseline_r2 = r2_score(y_test, y_pred_baseline)\n",
    "print(f\"\\nBaseline Performance on Test Set: R² = {baseline_r2:.4f}\")\n",
    "\n",
    "# Ablate CNN branch\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    buffer_pixels_y = int(BUFFER_METERS / res_y)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "cnn_test_ablated = np.zeros_like(extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "))\n",
    "y_pred_cnn_ablated = model.predict((cnn_test_ablated, mlp_test, gnn_test), verbose=0).flatten()\n",
    "y_pred_cnn_ablated[np.isnan(y_pred_cnn_ablated)] = 0\n",
    "r2_cnn_ablated = r2_score(y_test, y_pred_cnn_ablated)\n",
    "importance_cnn = baseline_r2 - r2_cnn_ablated\n",
    "\n",
    "# Ablate MLP branch\n",
    "mlp_test_ablated = np.zeros_like(mlp_test)\n",
    "y_pred_mlp_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test_ablated, gnn_test), verbose=0).flatten()\n",
    "y_pred_mlp_ablated[np.isnan(y_pred_mlp_ablated)] = 0\n",
    "r2_mlp_ablated = r2_score(y_test, y_pred_mlp_ablated)\n",
    "importance_mlp = baseline_r2 - r2_mlp_ablated\n",
    "\n",
    "# Ablate GNN branch\n",
    "gnn_test_ablated = np.zeros_like(gnn_test)\n",
    "y_pred_gnn_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test, gnn_test_ablated), verbose=0).flatten()\n",
    "y_pred_gnn_ablated[np.isnan(y_pred_gnn_ablated)] = 0\n",
    "r2_gnn_ablated = r2_score(y_test, y_pred_gnn_ablated)\n",
    "importance_gnn = baseline_r2 - r2_gnn_ablated\n",
    "\n",
    "print(\"\\n--- Combined Feature Importance (by Model Branch) ---\")\n",
    "print(f\"CNN Branch Importance (R² drop): {importance_cnn:.4f}\")\n",
    "print(f\"MLP Branch Importance (R² drop): {importance_mlp:.4f}\")\n",
    "print(f\"GNN Branch Importance (R² drop): {importance_gnn:.4f}\")\n",
    "\n",
    "# --- 9.2 MLP Feature Importance (Permutation-based) ---\n",
    "mlp_feature_importance = {}\n",
    "mlp_data_test_raw = test_orig[numeric_cols]\n",
    "for i, feature_name in enumerate(mlp_data_test_raw.columns):\n",
    "    mlp_test_shuffled = np.copy(mlp_test)\n",
    "    np.random.shuffle(mlp_test_shuffled[:, i])\n",
    "    \n",
    "    y_pred_shuffled = model.predict((extract_patch_for_generator(\n",
    "        coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "    ), mlp_test_shuffled, gnn_test), verbose=0).flatten()\n",
    "    y_pred_shuffled[np.isnan(y_pred_shuffled)] = 0\n",
    "    r2_shuffled = r2_score(y_test, y_pred_shuffled)\n",
    "    \n",
    "    importance = baseline_r2 - r2_shuffled\n",
    "    mlp_feature_importance[feature_name] = importance\n",
    "\n",
    "print(\"\\n--- MLP Feature Importance (Permutation-based) ---\")\n",
    "sorted_importance = sorted(mlp_feature_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, importance in sorted_importance:\n",
    "    print(f\"{feature:<20}: {importance:.4f}\")\n",
    "    \n",
    "# ==================== 10. Save Model and Data for Reproducibility ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Saving Model, Data, and Feature Importance Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create the single output directory\n",
    "output_dir = \"cnn_gnn_mlp\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the trained model in the Keras native format\n",
    "model_filename = os.path.join(output_dir, f\"fusion_model_{BUFFER_METERS}m.keras\")\n",
    "model.save(model_filename)\n",
    "print(f\"✅ Model saved to '{model_filename}'\")\n",
    "\n",
    "# Save the training history using pickle\n",
    "history_filename = os.path.join(output_dir, \"training_history.pkl\")\n",
    "with open(history_filename, 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(f\"✅ Training history saved to '{history_filename}'\")\n",
    "\n",
    "# --- New: Save Feature Importance Results ---\n",
    "feature_importance_results = {\n",
    "    \"mlp_feature_names\": test_orig[numeric_cols].columns.tolist(),\n",
    "    \"mlp_permutation_importance\": mlp_feature_importance,\n",
    "    \"cnn_ablation_importance\": importance_cnn,\n",
    "    \"mlp_ablation_importance\": importance_mlp,\n",
    "    \"gnn_ablation_importance\": importance_gnn\n",
    "}\n",
    "importance_filename = os.path.join(output_dir, \"feature_importance.pkl\")\n",
    "with open(importance_filename, 'wb') as f:\n",
    "    pickle.dump(feature_importance_results, f)\n",
    "print(f\"✅ Feature importance results saved to '{importance_filename}'\")\n",
    "\n",
    "# Save processed NumPy arrays for later use\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, \"processed_train_data.npz\"),\n",
    "    coords=coords_train,\n",
    "    mlp=mlp_train,\n",
    "    y=y_train\n",
    ")\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, \"processed_test_data.npz\"),\n",
    "    coords=coords_test,\n",
    "    mlp=mlp_test,\n",
    "    y=y_test\n",
    ")\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, \"gnn_data.npz\"),\n",
    "    gnn_train=gnn_train,\n",
    "    gnn_test=gnn_test\n",
    ")\n",
    "print(f\"✅ Processed data arrays saved to '{output_dir}'\")\n",
    "\n",
    "# Save the raw dataframes to CSV for easy inspection\n",
    "train_combined.to_csv(os.path.join(output_dir, \"train_combined.csv\"), index=False)\n",
    "test_orig.to_csv(os.path.join(output_dir, \"test_orig.csv\"), index=False)\n",
    "print(f\"✅ Raw dataframes saved to '{output_dir}'\")\n",
    "\n",
    "# Garbage collect to free up memory\n",
    "del model, history, train_generator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f402e81-2dbb-49b1-9514-e07b2cecb8c4",
   "metadata": {},
   "source": [
    "# ==================== 9. Feature Importance Analysis ==================== #\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"9. Feature Importance Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- 9.1 Combined Feature Importance (by Model Branch) ---\n",
    "# This method measures the importance of each model branch (CNN, MLP, GNN)\n",
    "# by temporarily 'ablating' it and measuring the drop in model performance (R²).\n",
    "\n",
    "# Calculate baseline performance on the test set\n",
    "y_pred_baseline = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, return_preds=True)\n",
    "baseline_r2 = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(f\"\\nBaseline Performance on Test Set: R² = {baseline_r2:.4f}\")\n",
    "\n",
    "# Ablate CNN branch\n",
    "cnn_test_ablated = np.zeros_like(extract_patch_for_generator(\n",
    "    coords_test, raster_paths, int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0])\n",
    "))\n",
    "y_pred_cnn_ablated = model.predict((cnn_test_ablated, mlp_test, gnn_test)).flatten()\n",
    "r2_cnn_ablated = r2_score(y_test, y_pred_cnn_ablated)\n",
    "importance_cnn = baseline_r2 - r2_cnn_ablated\n",
    "\n",
    "# Ablate MLP branch\n",
    "mlp_test_ablated = np.zeros_like(mlp_test)\n",
    "y_pred_mlp_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0])\n",
    "), mlp_test_ablated, gnn_test)).flatten()\n",
    "r2_mlp_ablated = r2_score(y_test, y_pred_mlp_ablated)\n",
    "importance_mlp = baseline_r2 - r2_mlp_ablated\n",
    "\n",
    "# Ablate GNN branch\n",
    "gnn_test_ablated = np.zeros_like(gnn_test)\n",
    "y_pred_gnn_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0])\n",
    "), mlp_test, gnn_test_ablated)).flatten()\n",
    "r2_gnn_ablated = r2_score(y_test, y_pred_gnn_ablated)\n",
    "importance_gnn = baseline_r2 - r2_gnn_ablated\n",
    "\n",
    "print(\"\\n--- Combined Feature Importance (by Model Branch) ---\")\n",
    "print(f\"CNN Branch Importance (R² drop): {importance_cnn:.4f}\")\n",
    "print(f\"MLP Branch Importance (R² drop): {importance_mlp:.4f}\")\n",
    "print(f\"GNN Branch Importance (R² drop): {importance_gnn:.4f}\")\n",
    "\n",
    "\n",
    "# --- 9.2 MLP Feature Importance (Permutation-based) ---\n",
    "# This method measures the importance of each individual MLP feature\n",
    "# by shuffling its values and measuring the drop in model performance.\n",
    "\n",
    "mlp_feature_importance = {}\n",
    "for i, feature_name in enumerate(numeric_cols):\n",
    "    # Create a copy of the test data to avoid modifying the original\n",
    "    mlp_test_shuffled = np.copy(mlp_test)\n",
    "    # Shuffle the values of the current feature (column i)\n",
    "    np.random.shuffle(mlp_test_shuffled[:, i])\n",
    "    \n",
    "    # Make predictions with the shuffled feature\n",
    "    y_pred_shuffled = model.predict((extract_patch_for_generator(\n",
    "        coords_test, raster_paths, int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0])\n",
    "    ), mlp_test_shuffled, gnn_test)).flatten()\n",
    "    r2_shuffled = r2_score(y_test, y_pred_shuffled)\n",
    "    \n",
    "    # Calculate the drop in performance\n",
    "    importance = baseline_r2 - r2_shuffled\n",
    "    mlp_feature_importance[feature_name] = importance\n",
    "\n",
    "# Sort and print the results\n",
    "print(\"\\n--- MLP Feature Importance (Permutation-based) ---\")\n",
    "sorted_importance = sorted(mlp_feature_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, importance in sorted_importance:\n",
    "    print(f\"{feature:<20}: {importance:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2970f-f191-47f1-bb49-fdf99e0e3677",
   "metadata": {},
   "source": [
    "# CNN GAT MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b76e408-afd1-40a8-9291-4663d01e19d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_39\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_39\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ cnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,520</span> │ cnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_46    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_46… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_47    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_25          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33856</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_47… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_153 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ mlp_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_154 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,104</span> │ gnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,333,696</span> │ flatten_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense_153[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense_154[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_21      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cnn_out[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ mlp_out[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ gnn_out[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_155 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ concatenate_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_83          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_155[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_156 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_83[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_156[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ cnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m26\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_49 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m98\u001b[0m,    │      \u001b[38;5;34m7,520\u001b[0m │ cnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_46    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m49\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_50 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m47\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_46… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_47    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m110\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_25          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33856\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_47… │\n",
       "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_153 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m1,024\u001b[0m │ mlp_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_154 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m7,104\u001b[0m │ gnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_out (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m4,333,696\u001b[0m │ flatten_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_out (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dense_153[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_out (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dense_154[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_21      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ cnn_out[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ mlp_out[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ gnn_out[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_155 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m24,704\u001b[0m │ concatenate_21[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_83          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_155[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_156 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_83[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_156[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,405,025</span> (16.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,405,025\u001b[0m (16.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,405,025</span> (16.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,405,025\u001b[0m (16.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "12513"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import pickle\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "# Remove 'Source' column if it exists in river_100 dataframe\n",
    "if 'Source' in river_100.columns:\n",
    "    river_100.drop(columns=\"Source\", inplace=True)\n",
    "\n",
    "drop_cols = ['Stations', 'River', 'Lat', 'Long', 'geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# --- IMPUTATION FIX: Fill NaN values with 0 before further processing ---\n",
    "orig.fillna(0, inplace=True)\n",
    "river_100.fillna(0, inplace=True)\n",
    "\n",
    "# Train-test split\n",
    "np.random.seed(42)\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(f\"Using {len(raster_paths)} raster layers for CNN input.\")\n",
    "for r in raster_paths:\n",
    "    print(\"  -\", os.path.basename(r))\n",
    "\n",
    "# ==================== 3. Create a Custom Data Generator ==================== #\n",
    "def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):\n",
    "    \"\"\"\n",
    "    Extracts a batch of patches from rasters for a given set of coordinates.\n",
    "    This function is optimized to be called by the data generator for each batch.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    # Loop through each coordinate pair in the batch\n",
    "    for lon, lat in coords:\n",
    "        channels = []\n",
    "        # Loop through each raster file to get a single patch for each raster\n",
    "        for rfile in raster_files:\n",
    "            with rasterio.open(rfile) as src:\n",
    "                try:\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)\n",
    "                    arr = src.read(1, window=win, boundless=True, fill_value=0)\n",
    "                    arr = arr.astype(np.float32)\n",
    "\n",
    "                    # --- NORMALIZATION FIX: Add a small epsilon to avoid division by zero ---\n",
    "                    max_val = np.nanmax(arr)\n",
    "                    if max_val != 0:\n",
    "                        arr /= max_val + 1e-8 # Add epsilon for stability\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {rfile} for coordinates ({lon}, {lat}): {e}\")\n",
    "                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)\n",
    "            channels.append(arr)\n",
    "        patches.append(np.stack(channels, axis=-1))\n",
    "    \n",
    "    return np.array(patches)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.coords = coords\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.raster_paths = raster_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.buffer_meters = buffer_meters\n",
    "\n",
    "        # Pre-calculate patch size from the first raster\n",
    "        with rasterio.open(raster_paths[0]) as src:\n",
    "            res_x, res_y = src.res\n",
    "            self.buffer_pixels_x = int(self.buffer_meters / res_x)\n",
    "            self.buffer_pixels_y = int(self.buffer_meters / res_y)\n",
    "            self.patch_width = 2 * self.buffer_pixels_x\n",
    "            self.patch_height = 2 * self.buffer_pixels_y\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        \n",
    "        # Slice the GNN adjacency matrix for the current batch\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "\n",
    "        batch_y = self.y[batch_indices]\n",
    "\n",
    "        # Extract CNN patches for the current batch\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            self.raster_paths,\n",
    "            self.buffer_pixels_x,\n",
    "            self.buffer_pixels_y,\n",
    "            self.patch_width,\n",
    "            self.patch_height\n",
    "        )\n",
    "\n",
    "        # Return a tuple of inputs and the target, which Keras expects\n",
    "        return (batch_cnn, batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #\n",
    "coords_train = train_combined[['Long', 'Lat']].values\n",
    "coords_test = test_orig[['Long', 'Lat']].values\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_test_train = distance_matrix(coords_test, coords_train)\n",
    "gnn_test = np.exp(-dist_mat_test_train/10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# --- IMPUTATION FIX: Fill NaN in raw MLP data before scaling ---\n",
    "train_combined.fillna(0, inplace=True)\n",
    "test_orig.fillna(0, inplace=True)\n",
    "mlp_train = scaler.fit_transform(train_combined[numeric_cols])\n",
    "mlp_test = scaler.transform(test_orig[numeric_cols])\n",
    "y_train = train_combined['RI'].values\n",
    "y_test = test_orig['RI'].values\n",
    "\n",
    "# ==================== 5. Define Enhanced CNN–GNN–MLP Model ==================== #\n",
    "def build_fusion_model(patch_shape, gnn_dim, mlp_dim):\n",
    "    # CNN branch (for raster data)\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    x = Conv2D(32, (3,3), activation=\"relu\")(cnn_input)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(64, (3,3), activation=\"relu\")(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Flatten()(x)\n",
    "    cnn_out = Dense(128, activation=\"relu\", name=\"cnn_out\")(x)\n",
    "\n",
    "    # MLP branch (for numerical site features)\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    m = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_out = Dense(32, activation=\"relu\", name=\"mlp_out\")(m)\n",
    "\n",
    "    # GNN branch (for spatial connectivity)\n",
    "    # The GNN input dimension is now the number of training samples\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    g = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    gnn_out = Dense(32, activation=\"relu\", name=\"gnn_out\")(g)\n",
    "\n",
    "    # Fusion Layer\n",
    "    combined = Concatenate()([cnn_out, mlp_out, gnn_out])\n",
    "    f = Dense(128, activation=\"relu\")(combined)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# We need to determine the final GNN input dimension for the model\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "\n",
    "# Calculate CNN patch shape based on the current buffer size\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    cnn_patch_shape = (patch_width, patch_width, len(raster_paths))\n",
    "\n",
    "model = build_fusion_model(cnn_patch_shape, gnn_input_dim, mlp_train.shape[1])\n",
    "model.summary()\n",
    "\n",
    "# ==================== 6. Create Data Generators ==================== #\n",
    "train_generator = DataGenerator(\n",
    "    coords=coords_train,\n",
    "    mlp_data=mlp_train,\n",
    "    gnn_data=gnn_train,\n",
    "    y=y_train,\n",
    "    raster_paths=raster_paths,\n",
    "    buffer_meters=BUFFER_METERS,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "def evaluate_model(model, coords_test, mlp_test, gnn_test_matrix, y_test, raster_paths, buffer_meters, batch_size=4, return_preds=False):\n",
    "    num_samples = len(y_test)\n",
    "    y_pred_list = []\n",
    "    \n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, res_y = src.res\n",
    "        buffer_pixels_x = int(buffer_meters / res_x)\n",
    "        buffer_pixels_y = int(buffer_meters / res_y)\n",
    "        patch_width = 2 * buffer_pixels_x\n",
    "        patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_coords = coords_test[i:i+batch_size]\n",
    "        batch_mlp = mlp_test[i:i+batch_size]\n",
    "        \n",
    "        batch_gnn = gnn_test_matrix[i:i+batch_size, :]\n",
    "        batch_y = y_test[i:i+batch_size]\n",
    "\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            raster_paths,\n",
    "            buffer_pixels_x,\n",
    "            buffer_pixels_y,\n",
    "            patch_width,\n",
    "            patch_height\n",
    "        )\n",
    "        \n",
    "        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn), verbose=0).flatten())\n",
    "        \n",
    "    y_pred = np.concatenate(y_pred_list)\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        # --- NaN FIX: Ensure y_pred has no NaNs before calculating metrics ---\n",
    "        y_pred[np.isnan(y_pred)] = 0\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return r2, rmse\n",
    "\n",
    "# ==================== 7. Train Model ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing with CNN–GAT–MLP Model ({BUFFER_METERS}m)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=train_generator\n",
    ")\n",
    "\n",
    "# ==================== 8. Evaluate ==================== #\n",
    "train_eval_generator = DataGenerator(\n",
    "    coords=coords_train,\n",
    "    mlp_data=mlp_train,\n",
    "    gnn_data=gnn_train,\n",
    "    y=y_train,\n",
    "    raster_paths=raster_paths,\n",
    "    buffer_meters=BUFFER_METERS,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "y_pred_train = model.predict(train_eval_generator, verbose=0).flatten()\n",
    "# --- NaN FIX: Ensure y_pred has no NaNs before calculating metrics ---\n",
    "y_pred_train[np.isnan(y_pred_train)] = 0\n",
    "r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))\n",
    "\n",
    "r2_test, rmse_test = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size)\n",
    "\n",
    "print(f\"\\n✅ CNN–GAT–MLP Model Performance ({BUFFER_METERS}m):\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# ==================== 9. Feature Importance Analysis ==================== #\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(f\"Feature Importance Analysis for {BUFFER_METERS}m\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# --- 9.1 Combined Feature Importance (by Model Branch) ---\n",
    "y_pred_baseline = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, return_preds=True)\n",
    "y_pred_baseline[np.isnan(y_pred_baseline)] = 0\n",
    "baseline_r2 = r2_score(y_test, y_pred_baseline)\n",
    "print(f\"\\nBaseline Performance on Test Set: R² = {baseline_r2:.4f}\")\n",
    "\n",
    "# Ablate CNN branch\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    buffer_pixels_y = int(BUFFER_METERS / res_y)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "cnn_test_ablated = np.zeros_like(extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "))\n",
    "y_pred_cnn_ablated = model.predict((cnn_test_ablated, mlp_test, gnn_test), verbose=0).flatten()\n",
    "y_pred_cnn_ablated[np.isnan(y_pred_cnn_ablated)] = 0\n",
    "r2_cnn_ablated = r2_score(y_test, y_pred_cnn_ablated)\n",
    "importance_cnn = baseline_r2 - r2_cnn_ablated\n",
    "\n",
    "# Ablate MLP branch\n",
    "mlp_test_ablated = np.zeros_like(mlp_test)\n",
    "y_pred_mlp_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test_ablated, gnn_test), verbose=0).flatten()\n",
    "y_pred_mlp_ablated[np.isnan(y_pred_mlp_ablated)] = 0\n",
    "r2_mlp_ablated = r2_score(y_test, y_pred_mlp_ablated)\n",
    "importance_mlp = baseline_r2 - r2_mlp_ablated\n",
    "\n",
    "# Ablate GNN branch\n",
    "gnn_test_ablated = np.zeros_like(gnn_test)\n",
    "y_pred_gnn_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test, gnn_test_ablated), verbose=0).flatten()\n",
    "y_pred_gnn_ablated[np.isnan(y_pred_gnn_ablated)] = 0\n",
    "r2_gnn_ablated = r2_score(y_test, y_pred_gnn_ablated)\n",
    "importance_gnn = baseline_r2 - r2_gnn_ablated\n",
    "\n",
    "print(\"\\n--- Combined Feature Importance (by Model Branch) ---\")\n",
    "print(f\"CNN Branch Importance (R² drop): {importance_cnn:.4f}\")\n",
    "print(f\"MLP Branch Importance (R² drop): {importance_mlp:.4f}\")\n",
    "print(f\"GNN Branch Importance (R² drop): {importance_gnn:.4f}\")\n",
    "\n",
    "# --- 9.2 MLP Feature Importance (Permutation-based) ---\n",
    "mlp_feature_importance = {}\n",
    "mlp_data_test_raw = test_orig[numeric_cols]\n",
    "for i, feature_name in enumerate(mlp_data_test_raw.columns):\n",
    "    mlp_test_shuffled = np.copy(mlp_test)\n",
    "    np.random.shuffle(mlp_test_shuffled[:, i])\n",
    "    \n",
    "    y_pred_shuffled = model.predict((extract_patch_for_generator(\n",
    "        coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "    ), mlp_test_shuffled, gnn_test), verbose=0).flatten()\n",
    "    y_pred_shuffled[np.isnan(y_pred_shuffled)] = 0\n",
    "    r2_shuffled = r2_score(y_test, y_pred_shuffled)\n",
    "    \n",
    "    importance = baseline_r2 - r2_shuffled\n",
    "    mlp_feature_importance[feature_name] = importance\n",
    "\n",
    "print(\"\\n--- MLP Feature Importance (Permutation-based) ---\")\n",
    "sorted_importance = sorted(mlp_feature_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, importance in sorted_importance:\n",
    "    print(f\"{feature:<20}: {importance:.4f}\")\n",
    "    \n",
    "# ==================== 10. Save Model and Data for Reproducibility ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Saving Model, Data, and Feature Importance Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create the single output directory\n",
    "output_dir = \"cnn_gat_mlp\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the trained model in the Keras native format\n",
    "model_filename = os.path.join(output_dir, f\"fusion_model_{BUFFER_METERS}m.keras\")\n",
    "model.save(model_filename)\n",
    "print(f\"✅ Model saved to '{model_filename}'\")\n",
    "\n",
    "# Save the training history using pickle\n",
    "history_filename = os.path.join(output_dir, \"training_history.pkl\")\n",
    "with open(history_filename, 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(f\"✅ Training history saved to '{history_filename}'\")\n",
    "\n",
    "# --- New: Save Feature Importance Results ---\n",
    "feature_importance_results = {\n",
    "    \"mlp_feature_names\": test_orig[numeric_cols].columns.tolist(),\n",
    "    \"mlp_permutation_importance\": mlp_feature_importance,\n",
    "    \"cnn_ablation_importance\": importance_cnn,\n",
    "    \"mlp_ablation_importance\": importance_mlp,\n",
    "    \"gnn_ablation_importance\": importance_gnn\n",
    "}\n",
    "importance_filename = os.path.join(output_dir, \"feature_importance.pkl\")\n",
    "with open(importance_filename, 'wb') as f:\n",
    "    pickle.dump(feature_importance_results, f)\n",
    "print(f\"✅ Feature importance results saved to '{importance_filename}'\")\n",
    "\n",
    "# Save processed NumPy arrays for later use\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, \"processed_train_data.npz\"),\n",
    "    coords=coords_train,\n",
    "    mlp=mlp_train,\n",
    "    y=y_train\n",
    ")\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, \"processed_test_data.npz\"),\n",
    "    coords=coords_test,\n",
    "    mlp=mlp_test,\n",
    "    y=y_test\n",
    ")\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, \"gnn_data.npz\"),\n",
    "    gnn_train=gnn_train,\n",
    "    gnn_test=gnn_test\n",
    ")\n",
    "print(f\"✅ Processed data arrays saved to '{output_dir}'\")\n",
    "\n",
    "# Save the raw dataframes to CSV for easy inspection\n",
    "train_combined.to_csv(os.path.join(output_dir, \"train_combined.csv\"), index=False)\n",
    "test_orig.to_csv(os.path.join(output_dir, \"test_orig.csv\"), index=False)\n",
    "print(f\"✅ Raw dataframes saved to '{output_dir}'\")\n",
    "\n",
    "# Garbage collect to free up memory\n",
    "del model, history, train_generator\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e643b0d-5fcb-46ab-9263-7a1dd1ab577f",
   "metadata": {},
   "source": [
    "# Mixture of Experts (MoE) Ensemble\n",
    "\n",
    "```\n",
    "[Expert 1: CNN] ┐\n",
    "[Expert 2: GNN] ├── Gating Network (softmax weights) → Weighted Sum → Output\n",
    "[Expert 3: MLP] ┘\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "422d6287-816a-4789-b832-9655bebbdccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_40\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_40\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ cnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,520</span> │ cnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_48    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_48… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_49    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">88</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_26          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40000</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_49… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_158 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ mlp_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_160 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,696</span> │ gnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_157 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,128</span> │ flatten_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_159 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense_158[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_161 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense_160[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_22      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_157[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_159[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ dense_161[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_162 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │ concatenate_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_expert_out      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dense_157[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_expert_out      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ dense_159[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_expert_out      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ dense_161[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_163 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense_162[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ experts_stack       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cnn_expert_out[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ mlp_expert_out[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ gnn_expert_out[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gate_weights        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> │ dense_163[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ experts_stack[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │ gate_weights[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ cnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m26\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_51 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │      \u001b[38;5;34m7,520\u001b[0m │ cnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_48    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_52 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_48… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_49    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m88\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_26          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40000\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_49… │\n",
       "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_158 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m1,024\u001b[0m │ mlp_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_160 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m5,696\u001b[0m │ gnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_157 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m5,120,128\u001b[0m │ flatten_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_159 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dense_158[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_161 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dense_160[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_22      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_157[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_159[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ dense_161[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_162 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m12,352\u001b[0m │ concatenate_22[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_expert_out      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dense_157[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_expert_out      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m33\u001b[0m │ dense_159[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_expert_out      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m33\u001b[0m │ dense_161[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_163 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dense_162[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ experts_stack       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ cnn_expert_out[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ mlp_expert_out[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ gnn_expert_out[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gate_weights        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │         \u001b[38;5;34m99\u001b[0m │ dense_163[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ experts_stack[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │ gate_weights[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,171,750</span> (19.73 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,171,750\u001b[0m (19.73 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,171,750</span> (19.73 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,171,750\u001b[0m (19.73 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Concatenate,\n",
    "    Dropout,\n",
    "    Layer,\n",
    "    LayerNormalization,\n",
    "    Lambda\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import pickle # For saving and loading the scaler and feature importance results\n",
    "import json # For saving the feature importance results\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "# NOTE: The data loading logic remains the same as it provides the inputs\n",
    "# required for the new model architecture.\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "\n",
    "# Define the columns to drop and the numeric columns to use for MLP\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# Ensure there are no NaNs in the numeric columns before proceeding\n",
    "orig[numeric_cols] = orig[numeric_cols].fillna(0)\n",
    "river_100[numeric_cols] = river_100[numeric_cols].fillna(0)\n",
    "\n",
    "# Train-test split\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(f\"Using {len(raster_paths)} raster layers for CNN input.\")\n",
    "for r in raster_paths:\n",
    "    print(\"   -\", os.path.basename(r))\n",
    "\n",
    "# ==================== 3. Create a Custom Data Generator ==================== #\n",
    "def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):\n",
    "    \"\"\"\n",
    "    Extracts a batch of patches from rasters for a given set of coordinates.\n",
    "    This function is optimized to be called by the data generator for each batch.\n",
    "    \n",
    "    This version includes robust NaN handling to prevent model training errors.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    # Loop through each coordinate pair in the batch\n",
    "    for lon, lat in coords:\n",
    "        channels = []\n",
    "        # Loop through each raster file to get a single patch for each raster\n",
    "        for rfile in raster_files:\n",
    "            with rasterio.open(rfile) as src:\n",
    "                try:\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)\n",
    "                    arr = src.read(1, window=win, boundless=True, fill_value=0)\n",
    "                    \n",
    "                    # Corrected logic: Convert any NaNs to a numerical value, e.g., 0,\n",
    "                    # to prevent them from propagating through the model.\n",
    "                    arr = np.nan_to_num(arr, nan=0.0)\n",
    "                    \n",
    "                    arr = arr.astype(np.float32)\n",
    "\n",
    "                    # Get the maximum value, but check if it's a valid number and > 0.\n",
    "                    # This prevents division by zero if the patch is all zeros.\n",
    "                    max_val = np.nanmax(arr)\n",
    "                    if np.isfinite(max_val) and max_val > 0:\n",
    "                        arr /= max_val\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {rfile} for coordinates ({lon}, {lat}): {e}\")\n",
    "                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)\n",
    "            channels.append(arr)\n",
    "        patches.append(np.stack(channels, axis=-1))\n",
    "    \n",
    "    return np.array(patches)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.coords = coords\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.raster_paths = raster_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.buffer_meters = buffer_meters\n",
    "\n",
    "        # Pre-calculate patch size from the first raster\n",
    "        with rasterio.open(raster_paths[0]) as src:\n",
    "            res_x, res_y = src.res\n",
    "            self.buffer_pixels_x = int(self.buffer_meters / res_x)\n",
    "            self.buffer_pixels_y = int(self.buffer_meters / res_y)\n",
    "            self.patch_width = 2 * self.buffer_pixels_x\n",
    "            self.patch_height = 2 * self.buffer_pixels_y\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        \n",
    "        # Slice the GNN adjacency matrix for the current batch\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "\n",
    "        batch_y = self.y[batch_indices]\n",
    "\n",
    "        # Extract CNN patches for the current batch\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            self.raster_paths,\n",
    "            self.buffer_pixels_x,\n",
    "            self.buffer_pixels_y,\n",
    "            self.patch_width,\n",
    "            self.patch_height\n",
    "        )\n",
    "\n",
    "        # Return a tuple of inputs and the target, which Keras expects\n",
    "        return (batch_cnn, batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "\n",
    "# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #\n",
    "coords_train = train_combined[['Long','Lat']].values\n",
    "coords_test = test_orig[['Long','Lat']].values\n",
    "\n",
    "# We now split the training data into a training and validation set\n",
    "train_split, val_split = train_test_split(train_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "coords_train_split = train_split[['Long','Lat']].values\n",
    "coords_val_split = val_split[['Long','Lat']].values\n",
    "\n",
    "dist_mat_train_split = distance_matrix(coords_train_split, coords_train_split)\n",
    "gnn_train_split = np.exp(-dist_mat_train_split/10)\n",
    "dist_mat_test_train = distance_matrix(coords_test, coords_train_split)\n",
    "gnn_test = np.exp(-dist_mat_test_train/10)\n",
    "dist_mat_val_train = distance_matrix(coords_val_split, coords_train_split)\n",
    "gnn_val_split = np.exp(-dist_mat_val_train/10)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mlp_train_split = scaler.fit_transform(train_split[numeric_cols])\n",
    "mlp_val_split = scaler.transform(val_split[numeric_cols])\n",
    "mlp_test = scaler.transform(test_orig[numeric_cols])\n",
    "y_train_split = train_split['RI'].values\n",
    "y_val_split = val_split['RI'].values\n",
    "y_test = test_orig['RI'].values\n",
    "\n",
    "# ==================== 5. Define the Mixture of Experts Model ==================== #\n",
    "def build_moe_model(patch_shape, gnn_dim, mlp_dim):\n",
    "    # Inputs for all branches\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    \n",
    "    # --- Expert 1: CNN Branch ---\n",
    "    cnn_branch = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_branch = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(cnn_branch)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_branch_flattened = Flatten()(cnn_branch)\n",
    "    cnn_branch_dense = Dense(128, activation=\"relu\")(cnn_branch_flattened)\n",
    "    # The CNN expert's final prediction\n",
    "    cnn_expert_out = Dense(1, activation=\"linear\", name=\"cnn_expert_out\")(cnn_branch_dense)\n",
    "\n",
    "    # --- Expert 2: MLP Branch ---\n",
    "    mlp_branch = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_branch = Dense(32, activation=\"relu\")(mlp_branch)\n",
    "    # The MLP expert's final prediction\n",
    "    mlp_expert_out = Dense(1, activation=\"linear\", name=\"mlp_expert_out\")(mlp_branch)\n",
    "\n",
    "    # --- Expert 3: GNN Branch ---\n",
    "    gnn_branch = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    gnn_branch = Dense(32, activation=\"relu\")(gnn_branch)\n",
    "    # The GNN expert's final prediction\n",
    "    gnn_expert_out = Dense(1, activation=\"linear\", name=\"gnn_expert_out\")(gnn_branch)\n",
    "\n",
    "    # --- Gating Network ---\n",
    "    # The gating network needs features from all inputs to make its decision.\n",
    "    # We use the outputs of the dense layers before the final predictions as features.\n",
    "    gate_input = Concatenate()([cnn_branch_dense, mlp_branch, gnn_branch])\n",
    "    gate_network = Dense(64, activation=\"relu\")(gate_input)\n",
    "    gate_network = Dense(32, activation=\"relu\")(gate_network)\n",
    "    # The output is a set of weights for each expert (summing to 1 via softmax)\n",
    "    gate_weights = Dense(3, activation=\"softmax\", name=\"gate_weights\")(gate_network)\n",
    "\n",
    "    # --- Combine Experts and Gating Network ---\n",
    "    # Stack the predictions from each expert.\n",
    "    # The shape will be (batch_size, 3)\n",
    "    experts_stack = Concatenate(axis=1, name=\"experts_stack\")([cnn_expert_out, mlp_expert_out, gnn_expert_out])\n",
    "    \n",
    "    # Perform the weighted sum.\n",
    "    # This is done using a Lambda layer which takes the experts' outputs and\n",
    "    # the gating network's weights, and computes the dot product for each sample.\n",
    "    final_output = Lambda(lambda x: tf.reduce_sum(x[0] * x[1], axis=1, keepdims=True), name=\"final_output\")([experts_stack, gate_weights])\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, coords_test, mlp_test, gnn_test_matrix, y_test, raster_paths, buffer_meters, batch_size=4, return_preds=False):\n",
    "    num_samples = len(y_test)\n",
    "    y_pred_list = []\n",
    "    \n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, res_y = src.res\n",
    "        buffer_pixels_x = int(buffer_meters / res_x)\n",
    "        buffer_pixels_y = int(buffer_meters / res_y)\n",
    "        patch_width = 2 * buffer_pixels_x\n",
    "        patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_coords = coords_test[i:i+batch_size]\n",
    "        batch_mlp = mlp_test[i:i+batch_size]\n",
    "        \n",
    "        batch_gnn = gnn_test_matrix[i:i+batch_size, :]\n",
    "        batch_y = y_test[i:i+batch_size]\n",
    "\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            raster_paths,\n",
    "            buffer_pixels_x,\n",
    "            buffer_pixels_y,\n",
    "            patch_width,\n",
    "            patch_height\n",
    "        )\n",
    "        \n",
    "        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn)).flatten())\n",
    "        \n",
    "    y_pred = np.concatenate(y_pred_list)\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return r2, rmse\n",
    "\n",
    "\n",
    "# ==================== Run the Analysis ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing for BUFFER_METERS = {BUFFER_METERS}m\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(train_split)\n",
    "\n",
    "# Calculate CNN patch shape based on the current buffer size\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    cnn_patch_shape = (patch_width, patch_width, len(raster_paths))\n",
    "\n",
    "model = build_moe_model(cnn_patch_shape, gnn_input_dim, mlp_train_split.shape[1])\n",
    "model.summary()\n",
    "\n",
    "# ==================== 6. Create Data Generators ==================== #\n",
    "train_generator = DataGenerator(\n",
    "    coords=coords_train_split,\n",
    "    mlp_data=mlp_train_split,\n",
    "    gnn_data=gnn_train_split,\n",
    "    y=y_train_split,\n",
    "    raster_paths=raster_paths,\n",
    "    buffer_meters=BUFFER_METERS,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = DataGenerator(\n",
    "    coords=coords_val_split,\n",
    "    mlp_data=mlp_val_split,\n",
    "    gnn_data=gnn_val_split,\n",
    "    y=y_val_split,\n",
    "    raster_paths=raster_paths,\n",
    "    buffer_meters=BUFFER_METERS,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False # No need to shuffle validation data\n",
    ")\n",
    "\n",
    "\n",
    "# ==================== 7. Train Model ==================== #\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "\n",
    "# ==================== 8. Evaluate ==================== #\n",
    "y_pred_train = model.predict(train_generator).flatten()\n",
    "r2_train = r2_score(y_train_split[:len(y_pred_train)], y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train_split[:len(y_pred_train)], y_pred_train))\n",
    "\n",
    "r2_test, rmse_test = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size)\n",
    "\n",
    "print(f\"\\n Mixture of Experts Model Performance ({BUFFER_METERS}m):\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# ==================== 9. Feature Importance Analysis ==================== #\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(f\"Feature Importance Analysis for {BUFFER_METERS}m\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# --- 9.1 Combined Feature Importance (by Model Branch) ---\n",
    "y_pred_baseline = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, return_preds=True)\n",
    "baseline_r2 = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(f\"\\nBaseline Performance on Test Set: R² = {baseline_r2:.4f}\")\n",
    "\n",
    "# Ablate CNN branch\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    buffer_pixels_y = int(BUFFER_METERS / res_y)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "cnn_test_ablated = np.zeros_like(extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "))\n",
    "y_pred_cnn_ablated = model.predict((cnn_test_ablated, mlp_test, gnn_test)).flatten()\n",
    "r2_cnn_ablated = r2_score(y_test, y_pred_cnn_ablated)\n",
    "importance_cnn = baseline_r2 - r2_cnn_ablated\n",
    "\n",
    "# Ablate MLP branch\n",
    "mlp_test_ablated = np.zeros_like(mlp_test)\n",
    "y_pred_mlp_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test_ablated, gnn_test)).flatten()\n",
    "r2_mlp_ablated = r2_score(y_test, y_pred_mlp_ablated)\n",
    "importance_mlp = baseline_r2 - r2_mlp_ablated\n",
    "\n",
    "# Ablate GNN branch\n",
    "gnn_test_ablated = np.zeros_like(gnn_test)\n",
    "y_pred_gnn_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test, gnn_test_ablated)).flatten()\n",
    "r2_gnn_ablated = r2_score(y_test, y_pred_gnn_ablated)\n",
    "importance_gnn = baseline_r2 - r2_gnn_ablated\n",
    "\n",
    "print(\"\\n--- Combined Feature Importance (by Model Branch) ---\")\n",
    "print(f\"CNN Branch Importance (R² drop): {importance_cnn:.4f}\")\n",
    "print(f\"MLP Branch Importance (R² drop): {importance_mlp:.4f}\")\n",
    "print(f\"GNN Branch Importance (R² drop): {importance_gnn:.4f}\")\n",
    "\n",
    "# --- 9.2 MLP Feature Importance (Permutation-based) ---\n",
    "mlp_feature_importance = {}\n",
    "for i, feature_name in enumerate(numeric_cols):\n",
    "    mlp_test_shuffled = np.copy(mlp_test)\n",
    "    np.random.shuffle(mlp_test_shuffled[:, i])\n",
    "    \n",
    "    y_pred_shuffled = model.predict((extract_patch_for_generator(\n",
    "        coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "    ), mlp_test_shuffled, gnn_test)).flatten()\n",
    "    r2_shuffled = r2_score(y_test, y_pred_shuffled)\n",
    "    \n",
    "    importance = baseline_r2 - r2_shuffled\n",
    "    mlp_feature_importance[feature_name] = importance\n",
    "\n",
    "print(\"\\n--- MLP Feature Importance (Permutation-based) ---\")\n",
    "sorted_importance = sorted(mlp_feature_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, importance in sorted_importance:\n",
    "    print(f\"{feature:<20}: {importance:.4f}\")\n",
    "    \n",
    "# Garbage collect to free up memory\n",
    "del model, history, train_generator, validation_generator\n",
    "gc.collect()\n",
    "\n",
    "# ==================== 10. Save Model and Results ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Saving Model and Analysis Results...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Rebuild and re-train the model to ensure it's in a savable state.\n",
    "# This is a good practice to avoid issues with saving during a live session.\n",
    "# First, let's get the necessary dimensions again.\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(train_split)\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    cnn_patch_shape = (patch_width, patch_width, len(raster_paths))\n",
    "\n",
    "final_model = build_moe_model(cnn_patch_shape, gnn_input_dim, mlp_train_split.shape[1])\n",
    "# Re-fit the model on the full training and validation data\n",
    "final_model.fit(\n",
    "    DataGenerator(coords=coords_train_split, mlp_data=mlp_train_split, gnn_data=gnn_train_split, y=y_train_split, raster_paths=raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size),\n",
    "    epochs=2,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "    validation_data=DataGenerator(coords=coords_val_split, mlp_data=mlp_val_split, gnn_data=gnn_val_split, y=y_val_split, raster_paths=raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, shuffle=False)\n",
    ")\n",
    "\n",
    "output_dir = \"mixture_of_experts\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 10.1 Save the trained Keras model\n",
    "model_name = \"mixture_of_experts_model\"\n",
    "final_model.save(f'{output_dir}/{model_name}.keras')\n",
    "print(f\"Saved trained model to {output_dir}/{model_name}.keras\")\n",
    "\n",
    "# 10.2 Save the StandardScaler object\n",
    "# Using pickle.dump for serialization\n",
    "with open(f'{output_dir}/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"Saved StandardScaler object to {output_dir}/scaler.pkl\")\n",
    "\n",
    "# 10.3 Combine and save all feature importance results into a single file\n",
    "feature_importance_results = {\n",
    "    \"combined_branch_importance\": {\n",
    "        \"CNN_Importance_R2_drop\": importance_cnn,\n",
    "        \"MLP_Importance_R2_drop\": importance_mlp,\n",
    "        \"GNN_Importance_R2_drop\": importance_gnn\n",
    "    },\n",
    "    \"mlp_permutation_importance\": mlp_feature_importance\n",
    "}\n",
    "\n",
    "# Using pickle.dump for serialization\n",
    "with open(f'{output_dir}/feature_importance.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_importance_results, f)\n",
    "print(f\"Saved all feature importance results to {output_dir}/feature_importance.pkl\")\n",
    "\n",
    "# 10.4 Save the preprocessed data arrays for future use\n",
    "np.save(f'{output_dir}/coords_train.npy', coords_train)\n",
    "np.save(f'{output_dir}/mlp_train.npy', mlp_train_split)\n",
    "np.save(f'{output_dir}/gnn_train.npy', gnn_train_split)\n",
    "np.save(f'{output_dir}/y_train.npy', y_train_split)\n",
    "np.save(f'{output_dir}/coords_test.npy', coords_test)\n",
    "np.save(f'{output_dir}/mlp_test.npy', mlp_test)\n",
    "np.save(f'{output_dir}/gnn_test.npy', gnn_test)\n",
    "np.save(f'{output_dir}/y_test.npy', y_test)\n",
    "print(\"Saved preprocessed data arrays to .npy files\")\n",
    "\n",
    "print(\"\\nAll requested artifacts have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d3d2a-3b3e-4da5-ad4d-f1e4230786dd",
   "metadata": {},
   "source": [
    "# Dual Attention Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7958c587-867c-4d28-9697-5f55ae584734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Concatenate,\n",
    "    Dropout,\n",
    "    Layer,\n",
    "    Lambda,\n",
    "    GlobalAveragePooling2D,\n",
    "    Reshape,\n",
    "    Multiply\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import sys\n",
    "from io import StringIO # To capture print output\n",
    "import pickle # For saving dictionaries and other objects\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "# NOTE: The data loading logic remains the same.\n",
    "# Replace with your actual data paths if needed\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# Train-test split\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(f\"Using {len(raster_paths)} raster layers for CNN input.\")\n",
    "for r in raster_paths:\n",
    "    print(\"  -\", os.path.basename(r))\n",
    "\n",
    "# ==================== 3. Create a Custom Data Generator ==================== #\n",
    "def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):\n",
    "    \"\"\"\n",
    "    Extracts a batch of patches from rasters for a given set of coordinates.\n",
    "    This function is optimized to be called by the data generator for each batch.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    # Loop through each coordinate pair in the batch\n",
    "    for lon, lat in coords:\n",
    "        channels = []\n",
    "        # Loop through each raster file to get a single patch for each raster\n",
    "        for rfile in raster_files:\n",
    "            with rasterio.open(rfile) as src:\n",
    "                try:\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)\n",
    "                    arr = src.read(1, window=win, boundless=True, fill_value=0)\n",
    "                    arr = arr.astype(np.float32)\n",
    "\n",
    "                    if np.nanmax(arr) != 0:\n",
    "                        arr /= np.nanmax(arr)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {rfile} for coordinates ({lon}, {lat}): {e}\")\n",
    "                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)\n",
    "            channels.append(arr)\n",
    "        patches.append(np.stack(channels, axis=-1))\n",
    "    \n",
    "    return np.array(patches)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.coords = coords\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.raster_paths = raster_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.buffer_meters = buffer_meters\n",
    "\n",
    "        # Pre-calculate patch size from the first raster\n",
    "        with rasterio.open(raster_paths[0]) as src:\n",
    "            res_x, res_y = src.res\n",
    "            self.buffer_pixels_x = int(self.buffer_meters / res_x)\n",
    "            self.buffer_pixels_y = int(self.buffer_meters / res_y)\n",
    "            self.patch_width = 2 * self.buffer_pixels_x\n",
    "            self.patch_height = 2 * self.buffer_pixels_y\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        \n",
    "        # Slice the GNN adjacency matrix for the current batch\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "\n",
    "        batch_y = self.y[batch_indices]\n",
    "\n",
    "        # Extract CNN patches for the current batch\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            self.raster_paths,\n",
    "            self.buffer_pixels_x,\n",
    "            self.buffer_pixels_y,\n",
    "            self.patch_width,\n",
    "            self.patch_height\n",
    "        )\n",
    "\n",
    "        # Return a tuple of inputs and the target, which Keras expects\n",
    "        return (batch_cnn, batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "\n",
    "# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #\n",
    "coords_train = train_combined[['Long','Lat']].values\n",
    "coords_test = test_orig[['Long','Lat']].values\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_test_train = distance_matrix(coords_test, coords_train)\n",
    "gnn_test = np.exp(-dist_mat_test_train/10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mlp_train = scaler.fit_transform(train_combined[numeric_cols])\n",
    "mlp_test = scaler.transform(test_orig[numeric_cols])\n",
    "y_train = train_combined['RI'].values\n",
    "y_test = test_orig['RI'].values\n",
    "\n",
    "# ==================== 5. Define Custom Attention Layers ==================== #\n",
    "\n",
    "class SpatialAttention(Layer):\n",
    "    \"\"\"\n",
    "    A custom layer to apply spatial attention to a feature map.\n",
    "    It generates a spatial attention map and multiplies it with the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SpatialAttention, self).__init__(**kwargs)\n",
    "        self.conv1 = Conv2D(1, (1, 1), activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Squeeze the channels and generate a 2D attention map\n",
    "        attention_map = self.conv1(inputs)\n",
    "        # Multiply the input feature map by the attention map\n",
    "        return Multiply()([inputs, attention_map])\n",
    "\n",
    "class FeatureAttention(Layer):\n",
    "    \"\"\"\n",
    "    A custom layer to apply feature-wise attention.\n",
    "    It learns a weight for each feature channel and multiplies it with the input.\n",
    "    Inspired by Squeeze-and-Excitation networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction_ratio=16, **kwargs):\n",
    "        super(FeatureAttention, self).__init__(**kwargs)\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(FeatureAttention, self).build(input_shape)\n",
    "        if len(input_shape) == 4: # CNN output\n",
    "            self.avg_pool = GlobalAveragePooling2D()\n",
    "            self.dense1 = Dense(units=input_shape[-1] // self.reduction_ratio, activation='relu')\n",
    "            self.dense2 = Dense(units=input_shape[-1], activation='sigmoid')\n",
    "            self.reshape_output = Reshape((1, 1, input_shape[-1]))\n",
    "        else: # MLP or GNN output\n",
    "            self.dense1 = Dense(units=input_shape[-1] // self.reduction_ratio, activation='relu')\n",
    "            self.dense2 = Dense(units=input_shape[-1], activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if len(inputs.shape) == 4: # CNN branch\n",
    "            x = self.avg_pool(inputs)\n",
    "            x = self.dense1(x)\n",
    "            x = self.dense2(x)\n",
    "            x = self.reshape_output(x)\n",
    "        else: # MLP or GNN branch\n",
    "            x = self.dense1(inputs)\n",
    "            x = self.dense2(x)\n",
    "            \n",
    "        return Multiply()([inputs, x])\n",
    "\n",
    "# ==================== 6. Define the Dual Attention Model ==================== #\n",
    "def build_dual_attention_model(patch_shape, gnn_dim, mlp_dim):\n",
    "    # Inputs for all branches\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    \n",
    "    # --- CNN Branch with Spatial and Feature Attention ---\n",
    "    cnn_branch = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_branch = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(cnn_branch)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    \n",
    "    # Spatial Attention\n",
    "    cnn_spatial_attn = SpatialAttention()(cnn_branch)\n",
    "    \n",
    "    # Feature Attention\n",
    "    cnn_feature_attn = FeatureAttention()(cnn_spatial_attn)\n",
    "    \n",
    "    # Flatten and get embedding\n",
    "    cnn_embedding = Flatten()(cnn_feature_attn)\n",
    "    cnn_embedding = Dense(128, activation=\"relu\", name=\"cnn_embedding\")(cnn_embedding)\n",
    "\n",
    "    # --- MLP Branch with Embedding ---\n",
    "    mlp_embedding = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(32, activation=\"relu\", name=\"mlp_embedding\")(mlp_embedding)\n",
    "\n",
    "    # --- GNN Branch with Feature Attention and Embedding ---\n",
    "    gnn_branch = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    \n",
    "    # Feature Attention\n",
    "    gnn_feature_attn = FeatureAttention()(gnn_branch)\n",
    "    gnn_embedding = Dense(32, activation=\"relu\", name=\"gnn_embedding\")(gnn_feature_attn)\n",
    "\n",
    "    # --- Attention Fusion ---\n",
    "    # Concatenate all embeddings\n",
    "    combined_embedding = Concatenate(name=\"combined_embedding\")([cnn_embedding, mlp_embedding, gnn_embedding])\n",
    "    \n",
    "    # Final dense layers for prediction\n",
    "    f = Dense(128, activation=\"relu\")(combined_embedding)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, coords_test, mlp_test, gnn_test_matrix, y_test, raster_paths, buffer_meters, batch_size=4, return_preds=False):\n",
    "    num_samples = len(y_test)\n",
    "    y_pred_list = []\n",
    "    \n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, res_y = src.res\n",
    "        buffer_pixels_x = int(buffer_meters / res_x)\n",
    "        buffer_pixels_y = int(buffer_meters / res_y)\n",
    "        patch_width = 2 * buffer_pixels_x\n",
    "        patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_coords = coords_test[i:i+batch_size]\n",
    "        batch_mlp = mlp_test[i:i+batch_size]\n",
    "        \n",
    "        batch_gnn = gnn_test_matrix[i:i+batch_size, :]\n",
    "        batch_y = y_test[i:i+batch_size]\n",
    "\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            raster_paths,\n",
    "            buffer_pixels_x,\n",
    "            buffer_pixels_y,\n",
    "            patch_width,\n",
    "            patch_height\n",
    "        )\n",
    "        \n",
    "        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn)).flatten())\n",
    "        \n",
    "    y_pred = np.concatenate(y_pred_list)\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return r2, rmse\n",
    "\n",
    "# ==================== Run the Analysis ==================== #\n",
    "# Capture all print statements to a string\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = captured_output = StringIO()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing for BUFFER_METERS = {BUFFER_METERS}m\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "\n",
    "# Calculate CNN patch shape based on the current buffer size\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    cnn_patch_shape = (patch_width, patch_width, len(raster_paths))\n",
    "\n",
    "model = build_dual_attention_model(cnn_patch_shape, gnn_input_dim, mlp_train.shape[1])\n",
    "model.summary(print_fn=lambda x: captured_output.write(x + '\\n')) # Capture model summary\n",
    "\n",
    "# ==================== 7. Create Data Generators ==================== #\n",
    "train_generator = DataGenerator(\n",
    "    coords=coords_train,\n",
    "    mlp_data=mlp_train,\n",
    "    gnn_data=gnn_train,\n",
    "    y=y_train,\n",
    "    raster_paths=raster_paths,\n",
    "    buffer_meters=BUFFER_METERS,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# ==================== 8. Train Model ==================== #\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=train_generator\n",
    ")\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# ==================== 9. Evaluate ==================== #\n",
    "y_pred_train = model.predict(train_generator).flatten()\n",
    "r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))\n",
    "\n",
    "# Get test predictions for saving as .npy\n",
    "y_pred_test = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, return_preds=True)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"\\n Dual Attention Model Performance ({BUFFER_METERS}m):\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# ==================== 10. Feature Importance Analysis ==================== #\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(f\"Feature Importance Analysis for {BUFFER_METERS}m\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# --- 10.1 Combined Feature Importance (by Model Branch) ---\n",
    "y_pred_baseline = y_pred_test\n",
    "baseline_r2 = r2_test\n",
    "print(f\"\\nBaseline Performance on Test Set: R² = {baseline_r2:.4f}\")\n",
    "\n",
    "# Ablate CNN branch\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    buffer_pixels_y = int(BUFFER_METERS / res_y)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "cnn_test_ablated = np.zeros_like(extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "))\n",
    "y_pred_cnn_ablated = model.predict((cnn_test_ablated, mlp_test, gnn_test)).flatten()\n",
    "r2_cnn_ablated = r2_score(y_test, y_pred_cnn_ablated)\n",
    "importance_cnn = baseline_r2 - r2_cnn_ablated\n",
    "\n",
    "# Ablate MLP branch\n",
    "mlp_test_ablated = np.zeros_like(mlp_test)\n",
    "y_pred_mlp_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test_ablated, gnn_test)).flatten()\n",
    "r2_mlp_ablated = r2_score(y_test, y_pred_mlp_ablated)\n",
    "importance_mlp = baseline_r2 - r2_mlp_ablated\n",
    "\n",
    "# Ablate GNN branch\n",
    "gnn_test_ablated = np.zeros_like(gnn_test)\n",
    "y_pred_gnn_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test, gnn_test_ablated)).flatten()\n",
    "r2_gnn_ablated = r2_score(y_test, y_pred_gnn_ablated)\n",
    "importance_gnn = baseline_r2 - r2_gnn_ablated\n",
    "\n",
    "print(\"\\n--- Combined Feature Importance (by Model Branch) ---\")\n",
    "print(f\"CNN Branch Importance (R² drop): {importance_cnn:.4f}\")\n",
    "print(f\"MLP Branch Importance (R² drop): {importance_mlp:.4f}\")\n",
    "print(f\"GNN Branch Importance (R² drop): {importance_gnn:.4f}\")\n",
    "\n",
    "# --- 10.2 MLP Feature Importance (Permutation-based) ---\n",
    "mlp_feature_importance = {}\n",
    "for i, feature_name in enumerate(numeric_cols):\n",
    "    mlp_test_shuffled = np.copy(mlp_test)\n",
    "    np.random.shuffle(mlp_test_shuffled[:, i])\n",
    "    \n",
    "    y_pred_shuffled = model.predict((extract_patch_for_generator(\n",
    "        coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "    ), mlp_test_shuffled, gnn_test)).flatten()\n",
    "    r2_shuffled = r2_score(y_test, y_pred_shuffled)\n",
    "    \n",
    "    importance = baseline_r2 - r2_shuffled\n",
    "    mlp_feature_importance[feature_name] = importance\n",
    "\n",
    "print(\"\\n--- MLP Feature Importance (Permutation-based) ---\")\n",
    "sorted_importance = sorted(mlp_feature_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, importance in sorted_importance:\n",
    "    print(f\"{feature:<20}: {importance:.4f}\")\n",
    "    \n",
    "# Garbage collect to free up memory\n",
    "del history, train_generator\n",
    "gc.collect()\n",
    "\n",
    "# ==================== 11. Save all info to a folder ==================== #\n",
    "# Restore standard output\n",
    "sys.stdout = old_stdout\n",
    "printed_output = captured_output.getvalue()\n",
    "\n",
    "output_folder = \"dual_attention_analysis\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(f\"\\nCreating folder: '{output_folder}' and saving results...\")\n",
    "\n",
    "# Save the trained model in the .keras format\n",
    "model_path = os.path.join(output_folder, \"dual_attention.keras\")\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save the MLP feature importance to a .pkl file\n",
    "mlp_importance_path = os.path.join(output_folder, \"mlp_feature_importance.pkl\")\n",
    "with open(mlp_importance_path, 'wb') as f:\n",
    "    pickle.dump(mlp_feature_importance, f)\n",
    "print(f\"MLP feature importance saved to: {mlp_importance_path}\")\n",
    "\n",
    "# Save all relevant data to .npy files\n",
    "np.save(os.path.join(output_folder, \"coords_train.npy\"), coords_train)\n",
    "np.save(os.path.join(output_folder, \"coords_test.npy\"), coords_test)\n",
    "np.save(os.path.join(output_folder, \"mlp_train.npy\"), mlp_train)\n",
    "np.save(os.path.join(output_folder, \"mlp_test.npy\"), mlp_test)\n",
    "np.save(os.path.join(output_folder, \"gnn_train.npy\"), gnn_train)\n",
    "np.save(os.path.join(output_folder, \"gnn_test.npy\"), gnn_test)\n",
    "np.save(os.path.join(output_folder, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(output_folder, \"y_test.npy\"), y_test)\n",
    "np.save(os.path.join(output_folder, \"y_pred_train.npy\"), y_pred_train)\n",
    "np.save(os.path.join(output_folder, \"y_pred_test.npy\"), y_pred_test)\n",
    "print(f\"Coordinates, scaled data, GNN matrices, and labels/predictions saved as .npy files.\")\n",
    "\n",
    "# Save the printed output to a text file\n",
    "output_path = os.path.join(output_folder, \"analysis_output.txt\")\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(printed_output)\n",
    "print(f\"Analysis results saved to: {output_path}\")\n",
    "\n",
    "print(\"All information successfully saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5291cf5-d34f-40a8-b496-18b8ba415bea",
   "metadata": {},
   "source": [
    "# Stacked Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a8ef4bd-be79-4a10-a6bf-ba26d1b7484e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20619"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Concatenate,\n",
    "    Dropout,\n",
    "    Layer,\n",
    "    Lambda,\n",
    "    GlobalAveragePooling2D,\n",
    "    Reshape,\n",
    "    Multiply\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import sys\n",
    "from io import StringIO\n",
    "import pickle\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "# NOTE: The data loading logic remains the same.\n",
    "# Replace with your actual data paths if needed\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# Train-test split\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(f\"Using {len(raster_paths)} raster layers for CNN input.\")\n",
    "for r in raster_paths:\n",
    "    print(\"  -\", os.path.basename(r))\n",
    "\n",
    "# ==================== 3. Create a Custom Data Generator ==================== #\n",
    "def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):\n",
    "    \"\"\"\n",
    "    Extracts a batch of patches from rasters for a given set of coordinates.\n",
    "    This function is optimized to be called by the data generator for each batch.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    # Loop through each coordinate pair in the batch\n",
    "    for lon, lat in coords:\n",
    "        channels = []\n",
    "        # Loop through each raster file to get a single patch for each raster\n",
    "        for rfile in raster_files:\n",
    "            with rasterio.open(rfile) as src:\n",
    "                try:\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)\n",
    "                    arr = src.read(1, window=win, boundless=True, fill_value=0)\n",
    "                    arr = arr.astype(np.float32)\n",
    "\n",
    "                    if np.nanmax(arr) != 0:\n",
    "                        arr /= np.nanmax(arr)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {rfile} for coordinates ({lon}, {lat}): {e}\")\n",
    "                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)\n",
    "            channels.append(arr)\n",
    "        patches.append(np.stack(channels, axis=-1))\n",
    "    \n",
    "    return np.array(patches)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.coords = coords\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.raster_paths = raster_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.buffer_meters = buffer_meters\n",
    "\n",
    "        # Pre-calculate patch size from the first raster\n",
    "        with rasterio.open(raster_paths[0]) as src:\n",
    "            res_x, res_y = src.res\n",
    "            self.buffer_pixels_x = int(self.buffer_meters / res_x)\n",
    "            self.buffer_pixels_y = int(self.buffer_meters / res_y)\n",
    "            self.patch_width = 2 * self.buffer_pixels_x\n",
    "            self.patch_height = 2 * self.buffer_pixels_y\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "        batch_y = self.y[batch_indices]\n",
    "\n",
    "        # Extract CNN patches for the current batch\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            self.raster_paths,\n",
    "            self.buffer_pixels_x,\n",
    "            self.buffer_pixels_y,\n",
    "            self.patch_width,\n",
    "            self.patch_height\n",
    "        )\n",
    "\n",
    "        return (batch_cnn, batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #\n",
    "coords_train = train_combined[['Long','Lat']].values\n",
    "coords_test = test_orig[['Long','Lat']].values\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_test_train = distance_matrix(coords_test, coords_train)\n",
    "gnn_test = np.exp(-dist_mat_test_train/10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mlp_train = scaler.fit_transform(train_combined[numeric_cols])\n",
    "mlp_test = scaler.transform(test_orig[numeric_cols])\n",
    "y_train = train_combined['RI'].values\n",
    "y_test = test_orig['RI'].values\n",
    "\n",
    "# ==================== 5. Define Base Models ==================== #\n",
    "def build_cnn_mlp_model(patch_shape, mlp_dim):\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "\n",
    "    # CNN branch\n",
    "    cnn_branch = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_branch = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(cnn_branch)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_embedding = Flatten()(cnn_branch)\n",
    "\n",
    "    # MLP branch\n",
    "    mlp_embedding = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(32, activation=\"relu\")(mlp_embedding)\n",
    "\n",
    "    # Combine\n",
    "    combined = Concatenate()([cnn_embedding, mlp_embedding])\n",
    "    f = Dense(128, activation=\"relu\")(combined)\n",
    "    output = Dense(1, activation=\"linear\", name=\"cnn_mlp_output\")(f)\n",
    "    \n",
    "    model = Model(inputs=[cnn_input, mlp_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def build_gnn_mlp_model(gnn_dim, mlp_dim):\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "\n",
    "    # GNN branch\n",
    "    gnn_embedding = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    gnn_embedding = Dense(32, activation=\"relu\")(gnn_embedding)\n",
    "\n",
    "    # MLP branch\n",
    "    mlp_embedding = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(32, activation=\"relu\")(mlp_embedding)\n",
    "\n",
    "    # Combine\n",
    "    combined = Concatenate()([gnn_embedding, mlp_embedding])\n",
    "    f = Dense(64, activation=\"relu\")(combined)\n",
    "    output = Dense(1, activation=\"linear\", name=\"gnn_mlp_output\")(f)\n",
    "    \n",
    "    model = Model(inputs=[gnn_input, mlp_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def build_cnn_gnn_model(patch_shape, gnn_dim):\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "\n",
    "    # CNN branch\n",
    "    cnn_branch = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_branch = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(cnn_branch)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_embedding = Flatten()(cnn_branch)\n",
    "    \n",
    "    # GNN branch\n",
    "    gnn_embedding = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    gnn_embedding = Dense(32, activation=\"relu\")(gnn_embedding)\n",
    "\n",
    "    # Combine\n",
    "    combined = Concatenate()([cnn_embedding, gnn_embedding])\n",
    "    f = Dense(128, activation=\"relu\")(combined)\n",
    "    output = Dense(1, activation=\"linear\", name=\"cnn_gnn_output\")(f)\n",
    "    \n",
    "    model = Model(inputs=[cnn_input, gnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def build_meta_learner_model():\n",
    "    # Takes predictions from the 3 base models as input\n",
    "    pred1_input = Input(shape=(1,), name=\"pred1_input\")\n",
    "    pred2_input = Input(shape=(1,), name=\"pred2_input\")\n",
    "    pred3_input = Input(shape=(1,), name=\"pred3_input\")\n",
    "\n",
    "    # Concatenate the predictions\n",
    "    combined = Concatenate()([pred1_input, pred2_input, pred3_input])\n",
    "    \n",
    "    # Simple MLP as the meta-learner\n",
    "    f = Dense(32, activation=\"relu\")(combined)\n",
    "    f = Dense(16, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "    \n",
    "    model = Model(inputs=[pred1_input, pred2_input, pred3_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# ==================== 6. Create Data Generators for Base Models ==================== #\n",
    "# NOTE: We create generators that provide only the necessary inputs for each base model.\n",
    "class CNNDropoutGenerator(DataGenerator):\n",
    "    def __getitem__(self, index):\n",
    "        (batch_cnn, batch_mlp, batch_gnn), batch_y = super().__getitem__(index)\n",
    "        return (batch_cnn, batch_mlp), batch_y\n",
    "\n",
    "class GNNDropoutGenerator(DataGenerator):\n",
    "    def __getitem__(self, index):\n",
    "        (batch_cnn, batch_mlp, batch_gnn), batch_y = super().__getitem__(index)\n",
    "        return (batch_gnn, batch_mlp), batch_y\n",
    "\n",
    "class MLPDropoutGenerator(DataGenerator):\n",
    "    def __getitem__(self, index):\n",
    "        (batch_cnn, batch_mlp, batch_gnn), batch_y = super().__getitem__(index)\n",
    "        return (batch_cnn, batch_gnn), batch_y\n",
    "\n",
    "def get_base_model_predictions(model, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size):\n",
    "    num_samples = len(y)\n",
    "    y_pred_list = []\n",
    "    \n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, res_y = src.res\n",
    "        buffer_pixels_x = int(buffer_meters / res_x)\n",
    "        buffer_pixels_y = int(buffer_meters / res_y)\n",
    "        patch_width = 2 * buffer_pixels_x\n",
    "        patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_coords = coords[i:i+batch_size]\n",
    "        batch_mlp = mlp_data[i:i+batch_size]\n",
    "        batch_gnn = gnn_data[i:i+batch_size, :]\n",
    "        \n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "        )\n",
    "        \n",
    "        # Check which inputs the model expects and provide them\n",
    "        input_names = [inp.name for inp in model.inputs]\n",
    "        input_dict = {}\n",
    "        if 'cnn_input' in input_names:\n",
    "            input_dict['cnn_input'] = batch_cnn\n",
    "        if 'mlp_input' in input_names:\n",
    "            input_dict['mlp_input'] = batch_mlp\n",
    "        if 'gnn_input' in input_names:\n",
    "            input_dict['gnn_input'] = batch_gnn\n",
    "            \n",
    "        y_pred_list.append(model.predict(input_dict).flatten())\n",
    "            \n",
    "    return np.concatenate(y_pred_list)\n",
    "\n",
    "\n",
    "# ==================== Run the Analysis ==================== #\n",
    "\n",
    "# Redirect all print statements to a string (this is for logging to a file).\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = captured_output = StringIO()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing Stacked Deep Ensemble for BUFFER_METERS = {BUFFER_METERS}m\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "\n",
    "# Calculate CNN patch shape based on the current buffer size\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    cnn_patch_shape = (patch_width, patch_width, len(raster_paths))\n",
    "\n",
    "mlp_input_dim = mlp_train.shape[1]\n",
    "\n",
    "# --- Train Base Models ---\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- Training CNN-MLP Base Model ---\")\n",
    "cnn_mlp_model = build_cnn_mlp_model(cnn_patch_shape, mlp_input_dim)\n",
    "cnn_mlp_train_gen = CNNDropoutGenerator(\n",
    "    coords=coords_train, mlp_data=mlp_train, gnn_data=gnn_train, y=y_train,\n",
    "    raster_paths=raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "cnn_mlp_model.fit(cnn_mlp_train_gen, epochs=100, verbose=1, callbacks=[early_stopping], validation_data=cnn_mlp_train_gen)\n",
    "\n",
    "print(\"\\n--- Training GNN-MLP Base Model ---\")\n",
    "gnn_mlp_model = build_gnn_mlp_model(gnn_input_dim, mlp_input_dim)\n",
    "gnn_mlp_train_gen = GNNDropoutGenerator(\n",
    "    coords=coords_train, mlp_data=mlp_train, gnn_data=gnn_train, y=y_train,\n",
    "    raster_paths=raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "gnn_mlp_model.fit(gnn_mlp_train_gen, epochs=100, verbose=1, callbacks=[early_stopping], validation_data=gnn_mlp_train_gen)\n",
    "\n",
    "print(\"\\n--- Training CNN-GNN Base Model ---\")\n",
    "cnn_gnn_model = build_cnn_gnn_model(cnn_patch_shape, gnn_input_dim)\n",
    "cnn_gnn_train_gen = MLPDropoutGenerator(\n",
    "    coords=coords_train, mlp_data=mlp_train, gnn_data=gnn_train, y=y_train,\n",
    "    raster_paths=raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "cnn_gnn_model.fit(cnn_gnn_train_gen, epochs=100, verbose=1, callbacks=[early_stopping], validation_data=cnn_gnn_train_gen)\n",
    "\n",
    "# --- Generate predictions for meta-learner ---\n",
    "# Get predictions from base models on training data\n",
    "preds1_train = get_base_model_predictions(cnn_mlp_model, coords_train, mlp_train, gnn_train, y_train, raster_paths, BUFFER_METERS, batch_size)\n",
    "preds2_train = get_base_model_predictions(gnn_mlp_model, coords_train, mlp_train, gnn_train, y_train, raster_paths, BUFFER_METERS, batch_size)\n",
    "preds3_train = get_base_model_predictions(cnn_gnn_model, coords_train, mlp_train, gnn_train, y_train, raster_paths, BUFFER_METERS, batch_size)\n",
    "\n",
    "meta_train_inputs = (preds1_train.reshape(-1, 1), preds2_train.reshape(-1, 1), preds3_train.reshape(-1, 1))\n",
    "\n",
    "# --- Train Meta-Learner ---\n",
    "print(\"\\n--- Training Meta-Learner Model ---\")\n",
    "meta_model = build_meta_learner_model()\n",
    "meta_model.fit(meta_train_inputs, y_train, epochs=100, verbose=1, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "# --- Get predictions from base models on test data ---\n",
    "preds1_test = get_base_model_predictions(cnn_mlp_model, coords_test, mlp_test, gnn_test, y_test, raster_paths, BUFFER_METERS, batch_size)\n",
    "preds2_test = get_base_model_predictions(gnn_mlp_model, coords_test, mlp_test, gnn_test, y_test, raster_paths, BUFFER_METERS, batch_size)\n",
    "preds3_test = get_base_model_predictions(cnn_gnn_model, coords_test, mlp_test, gnn_test, y_test, raster_paths, BUFFER_METERS, batch_size)\n",
    "\n",
    "meta_test_inputs = (preds1_test.reshape(-1, 1), preds2_test.reshape(-1, 1), preds3_test.reshape(-1, 1))\n",
    "\n",
    "# --- Evaluate with Meta-Learner ---\n",
    "y_pred = meta_model.predict(meta_test_inputs).flatten()\n",
    "r2_test = r2_score(y_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"\\n Stacked Deep Ensemble Model Performance ({BUFFER_METERS}m):\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# --- NEW: Feature Importance for Meta-Learner ---\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(f\"Meta-Learner Feature Importance (Permutation-based)\")\n",
    "print(\"-\"*50)\n",
    "baseline_r2 = r2_test\n",
    "\n",
    "# Importance for CNN-MLP predictions\n",
    "preds1_test_shuffled = np.copy(preds1_test)\n",
    "np.random.shuffle(preds1_test_shuffled)\n",
    "shuffled_test_inputs = (preds1_test_shuffled.reshape(-1, 1), preds2_test.reshape(-1, 1), preds3_test.reshape(-1, 1))\n",
    "y_pred_shuffled = meta_model.predict(shuffled_test_inputs).flatten()\n",
    "r2_shuffled = r2_score(y_test, y_pred_shuffled)\n",
    "importance_cnn_mlp = baseline_r2 - r2_shuffled\n",
    "print(f\"Importance of CNN-MLP predictions (R² drop): {importance_cnn_mlp:.4f}\")\n",
    "\n",
    "# Importance for GNN-MLP predictions\n",
    "preds2_test_shuffled = np.copy(preds2_test)\n",
    "np.random.shuffle(preds2_test_shuffled)\n",
    "shuffled_test_inputs = (preds1_test.reshape(-1, 1), preds2_test_shuffled.reshape(-1, 1), preds3_test.reshape(-1, 1))\n",
    "y_pred_shuffled = meta_model.predict(shuffled_test_inputs).flatten()\n",
    "r2_shuffled = r2_score(y_test, y_pred_shuffled)\n",
    "importance_gnn_mlp = baseline_r2 - r2_shuffled\n",
    "print(f\"Importance of GNN-MLP predictions (R² drop): {importance_gnn_mlp:.4f}\")\n",
    "\n",
    "# Importance for CNN-GNN predictions\n",
    "preds3_test_shuffled = np.copy(preds3_test)\n",
    "np.random.shuffle(preds3_test_shuffled)\n",
    "shuffled_test_inputs = (preds1_test.reshape(-1, 1), preds2_test.reshape(-1, 1), preds3_test_shuffled.reshape(-1, 1))\n",
    "y_pred_shuffled = meta_model.predict(shuffled_test_inputs).flatten()\n",
    "r2_shuffled = r2_score(y_test, y_pred_shuffled)\n",
    "importance_cnn_gnn = baseline_r2 - r2_shuffled\n",
    "print(f\"Importance of CNN-GNN predictions (R² drop): {importance_cnn_gnn:.4f}\")\n",
    "\n",
    "# ==================== NEW: Save all info to a folder ==================== #\n",
    "# Restore standard output\n",
    "sys.stdout = old_stdout\n",
    "printed_output = captured_output.getvalue()\n",
    "\n",
    "output_folder = \"stacked_ensemble\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(f\"\\nCreating folder: '{output_folder}' and saving results...\")\n",
    "\n",
    "# Save all four models\n",
    "cnn_mlp_model_path = os.path.join(output_folder, \"cnn_mlp_model.keras\")\n",
    "cnn_mlp_model.save(cnn_mlp_model_path)\n",
    "print(f\"CNN-MLP model saved to: {cnn_mlp_model_path}\")\n",
    "\n",
    "gnn_mlp_model_path = os.path.join(output_folder, \"gnn_mlp_model.keras\")\n",
    "gnn_mlp_model.save(gnn_mlp_model_path)\n",
    "print(f\"GNN-MLP model saved to: {gnn_mlp_model_path}\")\n",
    "\n",
    "cnn_gnn_model_path = os.path.join(output_folder, \"cnn_gnn_model.keras\")\n",
    "cnn_gnn_model.save(cnn_gnn_model_path)\n",
    "print(f\"CNN-GNN model saved to: {cnn_gnn_model_path}\")\n",
    "\n",
    "meta_model_path = os.path.join(output_folder, \"meta_learner.keras\")\n",
    "meta_model.save(meta_model_path)\n",
    "print(f\"Meta-learner model saved to: {meta_model_path}\")\n",
    "\n",
    "\n",
    "# Save the base model predictions and true labels\n",
    "np.save(os.path.join(output_folder, \"preds1_train.npy\"), preds1_train)\n",
    "np.save(os.path.join(output_folder, \"preds2_train.npy\"), preds2_train)\n",
    "np.save(os.path.join(output_folder, \"preds3_train.npy\"), preds3_train)\n",
    "np.save(os.path.join(output_folder, \"preds1_test.npy\"), preds1_test)\n",
    "np.save(os.path.join(output_folder, \"preds2_test.npy\"), preds2_test)\n",
    "np.save(os.path.join(output_folder, \"preds3_test.npy\"), preds3_test)\n",
    "np.save(os.path.join(output_folder, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(output_folder, \"y_test.npy\"), y_test)\n",
    "np.save(os.path.join(output_folder, \"y_pred.npy\"), y_pred)\n",
    "print(f\"Predictions and true labels saved as .npy files.\")\n",
    "\n",
    "# Save the feature importance results\n",
    "feature_importance = {\n",
    "    \"CNN-MLP_importance\": importance_cnn_mlp,\n",
    "    \"GNN-MLP_importance\": importance_gnn_mlp,\n",
    "    \"CNN-GNN_importance\": importance_cnn_gnn\n",
    "}\n",
    "importance_path = os.path.join(output_folder, \"meta_learner_importance.pkl\")\n",
    "with open(importance_path, 'wb') as f:\n",
    "    pickle.dump(feature_importance, f)\n",
    "print(f\"Meta-learner feature importance saved to: {importance_path}\")\n",
    "\n",
    "# Save the printed output to a text file\n",
    "output_path = os.path.join(output_folder, \"analysis_output.txt\")\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(printed_output)\n",
    "print(f\"Analysis results saved to: {output_path}\")\n",
    "\n",
    "print(\"All information successfully saved.\")\n",
    "\n",
    "# Garbage collect to free up memory now that everything is saved\n",
    "del cnn_mlp_model, gnn_mlp_model, cnn_gnn_model, meta_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c98e1-a046-4312-a85a-4192a7b47f9f",
   "metadata": {},
   "source": [
    "# CNN + LSTM (Spatio-Temporal)\n",
    "\n",
    "- *If* LULC rasters are time-series (2017–2022), stack them and process with **ConvLSTM2D**.\n",
    "\n",
    "```\n",
    "Time-Series Rasters → ConvLSTM2D → Flatten → Dense → Fusion → Output\n",
    "\n",
    "```\n",
    "\n",
    "- **Fusion:** Combine ConvLSTM output with MLP (hydrology) and GNN (spatial network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd1d9c62-deb9-4efd-a61c-8086beed2d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_47\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_47\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ cnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv_lstm2d_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">207,616</span> │ cnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_31          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640000</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_lstm2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_192 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ mlp_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_193 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,104</span> │ gnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">81,920,128</span> │ flatten_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense_192[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense_193[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ combined_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cnn_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ mlp_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ gnn_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_194 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ combined_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_85          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_194[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_195 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_85[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_195[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ cnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m100\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m26\u001b[0m)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv_lstm2d_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │    \u001b[38;5;34m207,616\u001b[0m │ cnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mConvLSTM2D\u001b[0m)        │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m110\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_31          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640000\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv_lstm2d_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_192 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m1,024\u001b[0m │ mlp_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_193 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m7,104\u001b[0m │ gnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │ \u001b[38;5;34m81,920,128\u001b[0m │ flatten_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dense_192[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dense_193[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ combined_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ cnn_embedding[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ mlp_embedding[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ gnn_embedding[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_194 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m24,704\u001b[0m │ combined_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_85          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_194[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_195 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_85[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_195[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">82,173,057</span> (313.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m82,173,057\u001b[0m (313.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">82,173,057</span> (313.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m82,173,057\u001b[0m (313.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "16209"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, ConvLSTM2D, Flatten, Dense, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import sys\n",
    "from io import StringIO\n",
    "import pickle\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "# Define number of time steps for mock data\n",
    "TIME_STEPS = 5\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "# NOTE: The data loading logic remains the same as in the original script.\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# Train-test split\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "# NOTE: This code assumes the rasters are not time-series.\n",
    "# The `generate_mock_time_series` function below will create a time-series\n",
    "# for demonstration purposes. In a real-world scenario, you would load\n",
    "# different raster data for each time step.\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(f\"Using {len(raster_paths)} raster layers for CNN input.\")\n",
    "for r in raster_paths:\n",
    "    print(\"  -\", os.path.basename(r))\n",
    "\n",
    "# ==================== 3. Create a Custom Data Generator ==================== #\n",
    "def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):\n",
    "    \"\"\"\n",
    "    Extracts a batch of patches from rasters for a given set of coordinates.\n",
    "    This function is optimized to be called by the data generator for each batch.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    # Loop through each coordinate pair in the batch\n",
    "    for lon, lat in coords:\n",
    "        channels = []\n",
    "        # Loop through each raster file to get a single patch for each raster\n",
    "        for rfile in raster_files:\n",
    "            with rasterio.open(rfile) as src:\n",
    "                try:\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)\n",
    "                    arr = src.read(1, window=win, boundless=True, fill_value=0)\n",
    "                    arr = arr.astype(np.float32)\n",
    "\n",
    "                    if np.nanmax(arr) != 0:\n",
    "                        arr /= np.nanmax(arr)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {rfile} for coordinates ({lon}, {lat}): {e}\")\n",
    "                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)\n",
    "            channels.append(arr)\n",
    "        patches.append(np.stack(channels, axis=-1))\n",
    "    \n",
    "    return np.array(patches)\n",
    "\n",
    "def generate_mock_time_series(patches, time_steps):\n",
    "    \"\"\"\n",
    "    Generates mock time-series data by stacking the same patch for 'time_steps'\n",
    "    time steps. In a real-world scenario, you would have different rasters\n",
    "    for each time step, and this function would not be needed.\n",
    "    \n",
    "    Input shape: (batch_size, height, width, channels)\n",
    "    Output shape: (batch_size, time_steps, height, width, channels)\n",
    "    \"\"\"\n",
    "    return np.stack([patches] * time_steps, axis=1)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, time_steps=TIME_STEPS, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.coords = coords\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.raster_paths = raster_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.buffer_meters = buffer_meters\n",
    "        self.time_steps = time_steps\n",
    "\n",
    "        # Pre-calculate patch size from the first raster\n",
    "        with rasterio.open(raster_paths[0]) as src:\n",
    "            res_x, res_y = src.res\n",
    "            self.buffer_pixels_x = int(self.buffer_meters / res_x)\n",
    "            self.buffer_pixels_y = int(self.buffer_meters / res_y)\n",
    "            self.patch_width = 2 * self.buffer_pixels_x\n",
    "            self.patch_height = 2 * self.buffer_pixels_y\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "        batch_y = self.y[batch_indices]\n",
    "\n",
    "        # Extract CNN patches for the current batch\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            self.raster_paths,\n",
    "            self.buffer_pixels_x,\n",
    "            self.buffer_pixels_y,\n",
    "            self.patch_width,\n",
    "            self.patch_height\n",
    "        )\n",
    "        \n",
    "        # Generate mock time-series data\n",
    "        batch_cnn_time_series = generate_mock_time_series(batch_cnn, self.time_steps)\n",
    "\n",
    "        return (batch_cnn_time_series, batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #\n",
    "coords_train = train_combined[['Long','Lat']].values\n",
    "coords_test = test_orig[['Long','Lat']].values\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_test_train = distance_matrix(coords_test, coords_train)\n",
    "gnn_test = np.exp(-dist_mat_test_train/10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mlp_train = scaler.fit_transform(train_combined[numeric_cols])\n",
    "mlp_test = scaler.transform(test_orig[numeric_cols])\n",
    "y_train = train_combined['RI'].values\n",
    "y_test = test_orig['RI'].values\n",
    "\n",
    "# ==================== 5. Define Spatio-Temporal Model ==================== #\n",
    "def build_spatio_temporal_model(time_series_shape, gnn_dim, mlp_dim):\n",
    "    # Inputs for all branches\n",
    "    cnn_input = Input(shape=time_series_shape, name=\"cnn_input\")\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    \n",
    "    # --- ConvLSTM2D Branch for Spatio-Temporal Data ---\n",
    "    # `return_sequences=False` means we get the final output of the sequence\n",
    "    conv_lstm_branch = ConvLSTM2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        padding='same',\n",
    "        return_sequences=False,\n",
    "        activation='relu'\n",
    "    )(cnn_input)\n",
    "    \n",
    "    # Flatten and get embedding\n",
    "    cnn_embedding = Flatten()(conv_lstm_branch)\n",
    "    cnn_embedding = Dense(128, activation=\"relu\", name=\"cnn_embedding\")(cnn_embedding)\n",
    "\n",
    "    # --- MLP Branch with Embedding ---\n",
    "    mlp_embedding = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(32, activation=\"relu\", name=\"mlp_embedding\")(mlp_embedding)\n",
    "\n",
    "    # --- GNN Branch with Embedding ---\n",
    "    gnn_embedding = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    gnn_embedding = Dense(32, activation=\"relu\", name=\"gnn_embedding\")(gnn_embedding)\n",
    "\n",
    "    # --- Fusion ---\n",
    "    combined_embedding = Concatenate(name=\"combined_embedding\")([cnn_embedding, mlp_embedding, gnn_embedding])\n",
    "    \n",
    "    # Final dense layers for prediction\n",
    "    f = Dense(128, activation=\"relu\")(combined_embedding)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters, time_steps, batch_size=4):\n",
    "    \"\"\"\n",
    "    Evaluates the model on test data and returns R², RMSE, and predictions.\n",
    "    \"\"\"\n",
    "    num_samples = len(y_test)\n",
    "    y_pred_list = []\n",
    "    \n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, res_y = src.res\n",
    "        buffer_pixels_x = int(buffer_meters / res_x)\n",
    "        buffer_pixels_y = int(buffer_meters / res_y)\n",
    "        patch_width = 2 * buffer_pixels_x\n",
    "        patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_coords = coords_test[i:i+batch_size]\n",
    "        batch_mlp = mlp_test[i:i+batch_size]\n",
    "        batch_gnn = gnn_test[i:i+batch_size, :]\n",
    "        \n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "        )\n",
    "        batch_cnn_time_series = generate_mock_time_series(batch_cnn, time_steps)\n",
    "        \n",
    "        y_pred_list.append(model.predict((batch_cnn_time_series, batch_mlp, batch_gnn)).flatten())\n",
    "        \n",
    "    y_pred = np.concatenate(y_pred_list)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    return r2, rmse, y_pred\n",
    "\n",
    "def calculate_mlp_feature_importance(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters, time_steps, numeric_cols, batch_size=4):\n",
    "    \"\"\"\n",
    "    Calculates feature importance for MLP features using a permutation-based approach.\n",
    "    \"\"\"\n",
    "    # First, get baseline performance on the original test set\n",
    "    _, baseline_rmse, _ = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters, time_steps, batch_size)\n",
    "    \n",
    "    feature_importances = {}\n",
    "    \n",
    "    # Iterate through each MLP feature\n",
    "    for i, feature_name in enumerate(numeric_cols):\n",
    "        print(f\"Calculating importance for feature: {feature_name}\")\n",
    "        \n",
    "        # Create a copy of the MLP test data to shuffle one feature\n",
    "        shuffled_mlp_test = mlp_test.copy()\n",
    "        \n",
    "        # Shuffle the current feature's column\n",
    "        np.random.shuffle(shuffled_mlp_test[:, i])\n",
    "        \n",
    "        # Evaluate model with shuffled data\n",
    "        _, shuffled_rmse, _ = evaluate_model(model, coords_test, shuffled_mlp_test, gnn_test, y_test, raster_paths, buffer_meters, time_steps, batch_size)\n",
    "        \n",
    "        # The importance is the increase in RMSE\n",
    "        importance = shuffled_rmse - baseline_rmse\n",
    "        feature_importances[feature_name] = importance\n",
    "        \n",
    "    return feature_importances\n",
    "\n",
    "# ==================== Run the Analysis ==================== #\n",
    "# Redirect output to a string for later saving\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = captured_output = StringIO()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing CNN + LSTM Model with {TIME_STEPS} mock time steps\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "\n",
    "# Calculate CNN patch shape based on the current buffer size\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    time_series_shape = (TIME_STEPS, patch_width, patch_width, len(raster_paths))\n",
    "\n",
    "mlp_input_dim = mlp_train.shape[1]\n",
    "\n",
    "model = build_spatio_temporal_model(time_series_shape, gnn_input_dim, mlp_input_dim)\n",
    "model.summary()\n",
    "\n",
    "# ==================== 6. Create Data Generators ==================== #\n",
    "train_generator = DataGenerator(\n",
    "    coords=coords_train, mlp_data=mlp_train, gnn_data=gnn_train, y=y_train,\n",
    "    raster_paths=raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "# ==================== 7. Train Model ==================== #\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=train_generator\n",
    ")\n",
    "\n",
    "# ==================== 8. Evaluate ==================== #\n",
    "# Evaluate on training data\n",
    "y_pred_train = model.predict(train_generator).flatten()\n",
    "r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))\n",
    "\n",
    "# Evaluate on test data\n",
    "r2_test, rmse_test, y_pred_test = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, BUFFER_METERS, TIME_STEPS, batch_size=batch_size)\n",
    "\n",
    "print(f\"\\n Spatio-Temporal Model Performance ({BUFFER_METERS}m):\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# ==================== 9. Save all info to a folder ==================== #\n",
    "# Restore standard output\n",
    "sys.stdout = old_stdout\n",
    "printed_output = captured_output.getvalue()\n",
    "\n",
    "output_folder = \"cnn_lstm\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(f\"\\nCreating folder: '{output_folder}' and saving results...\")\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(output_folder, \"spatio_temporal_model.keras\")\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save the predictions and true labels\n",
    "np.save(os.path.join(output_folder, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(output_folder, \"y_test.npy\"), y_test)\n",
    "np.save(os.path.join(output_folder, \"y_pred_train.npy\"), y_pred_train)\n",
    "np.save(os.path.join(output_folder, \"y_pred_test.npy\"), y_pred_test)\n",
    "print(f\"Predictions and true labels saved as .npy files.\")\n",
    "\n",
    "# Save the printed output to a text file\n",
    "output_path = os.path.join(output_folder, \"analysis_output.txt\")\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(printed_output)\n",
    "print(f\"Analysis results saved to: {output_path}\")\n",
    "\n",
    "# ==================== 10. Calculate and save feature importance ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Calculating MLP Feature Importance...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "mlp_importance = calculate_mlp_feature_importance(\n",
    "    model, coords_test, mlp_test, gnn_test, y_test, raster_paths, BUFFER_METERS, TIME_STEPS, numeric_cols, batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Save feature importance to a pickle file\n",
    "importance_path = os.path.join(output_folder, \"mlp_feature_importance.pkl\")\n",
    "with open(importance_path, 'wb') as f:\n",
    "    pickle.dump(mlp_importance, f)\n",
    "\n",
    "print(f\"MLP Feature importance saved to: {importance_path}\")\n",
    "print(\"Feature Importances (change in RMSE):\")\n",
    "for feature, importance in mlp_importance.items():\n",
    "    print(f\"  - {feature}: {importance:.4f}\")\n",
    "\n",
    "print(\"\\nAll information successfully saved.\")\n",
    "\n",
    "# Garbage collect to free up memory now that everything is saved\n",
    "del model, history, train_generator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4488e9e5-3019-4173-aed7-0ad3d2936bfa",
   "metadata": {},
   "source": [
    "# Transformer-based Fusion (CNN + GNN + MLP)\n",
    "\n",
    "- **Idea:** Use a **Transformer Encoder** to fuse embeddings from CNN, GNN, and MLP branches.\n",
    "- **Architecture:**\n",
    "\n",
    "```\n",
    "CNN Embedding ┐\n",
    "GNN Embedding ├── Transformer Encoder → Dense → Output\n",
    "MLP Embedding ┘\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f975b6d-9ba9-41f0-aca2-7bdd8da49f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_48\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_48\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ cnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,520</span> │ cnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_58    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_58… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_59    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_196 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ mlp_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_197 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">14,208</span> │ gnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_embedding_flat… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40000</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_59… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dense_196[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dense_197[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_198 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,064</span> │ cnn_embedding_fl… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_199 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ mlp_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_200 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ gnn_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_198[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_199[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_200[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_28      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ reshape_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ reshape_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ reshape_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">66,368</span> │ concatenate_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ concatenate_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_87          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ dropout_87[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_32          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_201 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ flatten_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_88          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_201[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_202 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_88[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_202[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ cnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m26\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_62 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │      \u001b[38;5;34m7,520\u001b[0m │ cnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_58    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_62[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_63 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_58… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m110\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_59    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_63[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_196 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ mlp_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_197 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m14,208\u001b[0m │ gnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_embedding_flat… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40000\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_59… │\n",
       "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dense_196[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dense_197[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_198 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │  \u001b[38;5;34m2,560,064\u001b[0m │ cnn_embedding_fl… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_199 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m4,160\u001b[0m │ mlp_embedding[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_200 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m4,160\u001b[0m │ gnn_embedding[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_7 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_198[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_8 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_199[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_9 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_200[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_28      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ reshape_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ reshape_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ reshape_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │     \u001b[38;5;34m66,368\u001b[0m │ concatenate_28[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ concatenate_28[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_87          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ concatenate_28[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ dropout_87[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m128\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_32          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_201 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m24,704\u001b[0m │ flatten_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_88          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_201[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_202 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_88[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_202[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,726,689</span> (10.40 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,726,689\u001b[0m (10.40 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,726,689</span> (10.40 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,726,689\u001b[0m (10.40 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "14991"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout, Layer, MultiHeadAttention, LayerNormalization, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import sys\n",
    "from io import StringIO\n",
    "import pickle # Import the pickle library for saving objects\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# Train-test split\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(f\"Using {len(raster_paths)} raster layers for CNN input.\")\n",
    "for r in raster_paths:\n",
    "    print(\"  -\", os.path.basename(r))\n",
    "\n",
    "# ==================== 3. Create a Custom Data Generator ==================== #\n",
    "def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):\n",
    "    \"\"\"\n",
    "    Extracts a batch of patches from rasters for a given set of coordinates.\n",
    "    This function is optimized to be called by the data generator for each batch.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    # Loop through each coordinate pair in the batch\n",
    "    for lon, lat in coords:\n",
    "        channels = []\n",
    "        # Loop through each raster file to get a single patch for each raster\n",
    "        for rfile in raster_files:\n",
    "            with rasterio.open(rfile) as src:\n",
    "                try:\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)\n",
    "                    arr = src.read(1, window=win, boundless=True, fill_value=0)\n",
    "                    arr = arr.astype(np.float32)\n",
    "\n",
    "                    if np.nanmax(arr) != 0:\n",
    "                        arr /= np.nanmax(arr)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {rfile} for coordinates ({lon}, {lat}): {e}\")\n",
    "                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)\n",
    "            channels.append(arr)\n",
    "        patches.append(np.stack(channels, axis=-1))\n",
    "    \n",
    "    return np.array(patches)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.coords = coords\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.raster_paths = raster_paths\n",
    "        self.buffer_meters = buffer_meters\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "\n",
    "        # Pre-calculate patch size from the first raster\n",
    "        with rasterio.open(raster_paths[0]) as src:\n",
    "            res_x, res_y = src.res\n",
    "            self.buffer_pixels_x = int(self.buffer_meters / res_x)\n",
    "            self.buffer_pixels_y = int(self.buffer_meters / res_y)\n",
    "            self.patch_width = 2 * self.buffer_pixels_x\n",
    "            self.patch_height = 2 * self.buffer_pixels_y\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "        batch_y = self.y[batch_indices]\n",
    "\n",
    "        # Extract CNN patches for the current batch\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            self.raster_paths,\n",
    "            self.buffer_pixels_x,\n",
    "            self.buffer_pixels_y,\n",
    "            self.patch_width,\n",
    "            self.patch_height\n",
    "        )\n",
    "        \n",
    "        return (batch_cnn, batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #\n",
    "coords_train = train_combined[['Long','Lat']].values\n",
    "coords_test = test_orig[['Long','Lat']].values\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_test_train = distance_matrix(coords_test, coords_train)\n",
    "gnn_test = np.exp(-dist_mat_test_train/10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mlp_train = scaler.fit_transform(train_combined[numeric_cols])\n",
    "mlp_test = scaler.transform(test_orig[numeric_cols])\n",
    "y_train = train_combined['RI'].values\n",
    "y_test = test_orig['RI'].values\n",
    "\n",
    "# ==================== 5. Define Transformer-based Fusion Model ==================== #\n",
    "def build_transformer_fusion_model(patch_shape, gnn_dim, mlp_dim):\n",
    "    # Inputs for all branches\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    \n",
    "    # --- CNN Branch ---\n",
    "    cnn_branch = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_branch = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(cnn_branch)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_embedding = Flatten(name=\"cnn_embedding_flatten\")(cnn_branch)\n",
    "    \n",
    "    # --- MLP Branch ---\n",
    "    mlp_embedding = Dense(128, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(64, activation=\"relu\", name=\"mlp_embedding\")(mlp_embedding)\n",
    "\n",
    "    # --- GNN Branch ---\n",
    "    gnn_embedding = Dense(128, activation=\"relu\")(gnn_input)\n",
    "    gnn_embedding = Dense(64, activation=\"relu\", name=\"gnn_embedding\")(gnn_embedding)\n",
    "\n",
    "    # --- Transformer Fusion ---\n",
    "    # To feed into the transformer, we need to make all embeddings have the same dimension.\n",
    "    # Let's use a dense layer to project them to a common size.\n",
    "    projection_dim = 64\n",
    "    cnn_proj = Dense(projection_dim)(cnn_embedding)\n",
    "    mlp_proj = Dense(projection_dim)(mlp_embedding)\n",
    "    gnn_proj = Dense(projection_dim)(gnn_embedding)\n",
    "\n",
    "    # Stack the embeddings to create a sequence for the transformer\n",
    "    # Shape becomes (None, 3, projection_dim)\n",
    "    # Corrected code to use Keras-compatible operations\n",
    "    cnn_expanded = Reshape((1, projection_dim))(cnn_proj)\n",
    "    mlp_expanded = Reshape((1, projection_dim))(mlp_proj)\n",
    "    gnn_expanded = Reshape((1, projection_dim))(gnn_proj)\n",
    "    embeddings = Concatenate(axis=1)([cnn_expanded, mlp_expanded, gnn_expanded])\n",
    "\n",
    "    # Transformer Encoder block\n",
    "    transformer_output = MultiHeadAttention(\n",
    "        num_heads=4,\n",
    "        key_dim=projection_dim\n",
    "    )(embeddings, embeddings)\n",
    "    transformer_output = Dropout(0.2)(transformer_output)\n",
    "    transformer_output = LayerNormalization(epsilon=1e-6)(embeddings + transformer_output)\n",
    "    \n",
    "    # The output from the transformer is a sequence of 3 vectors.\n",
    "    # We flatten this for the final prediction layer.\n",
    "    transformer_output_flattened = Flatten()(transformer_output)\n",
    "    \n",
    "    # Final dense layers for prediction\n",
    "    f = Dense(128, activation=\"relu\")(transformer_output_flattened)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=4, return_preds=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on given data and returns R², RMSE, and predictions.\n",
    "    \"\"\"\n",
    "    num_samples = len(y_true)\n",
    "    y_pred_list = []\n",
    "    \n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, res_y = src.res\n",
    "        buffer_pixels_x = int(buffer_meters / res_x)\n",
    "        buffer_pixels_y = int(buffer_meters / res_y)\n",
    "        patch_width = 2 * buffer_pixels_x\n",
    "        patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_coords = coords[i:i+batch_size]\n",
    "        batch_mlp = mlp_data[i:i+batch_size]\n",
    "        batch_gnn = gnn_data[i:i+batch_size, :]\n",
    "        \n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "        )\n",
    "        \n",
    "        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn)).flatten())\n",
    "        \n",
    "    y_pred = np.concatenate(y_pred_list)\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        return r2, rmse\n",
    "\n",
    "def calculate_permutation_importance(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=4):\n",
    "    \"\"\"\n",
    "    Calculates permutation feature importance for the three model branches.\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting Permutation Feature Importance Analysis...\")\n",
    "    # Get baseline R² on the unshuffled data\n",
    "    baseline_r2, _ = evaluate_model(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)\n",
    "    print(f\"Baseline R² on test set: {baseline_r2:.4f}\")\n",
    "\n",
    "    importance = {}\n",
    "    \n",
    "    # Permute CNN input\n",
    "    shuffled_indices = np.random.permutation(len(y_true))\n",
    "    coords_shuffled = coords[shuffled_indices]\n",
    "    shuffled_r2, _ = evaluate_model(model, coords_shuffled, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)\n",
    "    importance['CNN'] = baseline_r2 - shuffled_r2\n",
    "    \n",
    "    # Permute MLP input\n",
    "    shuffled_mlp_data = mlp_data.copy()\n",
    "    np.random.shuffle(shuffled_mlp_data)\n",
    "    shuffled_r2, _ = evaluate_model(model, coords, shuffled_mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)\n",
    "    importance['MLP'] = baseline_r2 - shuffled_r2\n",
    "\n",
    "    # Permute GNN input\n",
    "    shuffled_gnn_data = gnn_data.copy()\n",
    "    np.random.shuffle(shuffled_gnn_data)\n",
    "    shuffled_r2, _ = evaluate_model(model, coords, mlp_data, shuffled_gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)\n",
    "    importance['GNN'] = baseline_r2 - shuffled_r2\n",
    "\n",
    "    return importance\n",
    "\n",
    "# ==================== Run the Analysis ==================== #\n",
    "# Redirect output to a string for later saving\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = captured_output = StringIO()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing Transformer-based Fusion Model for BUFFER_METERS = {BUFFER_METERS}m\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "\n",
    "# Calculate CNN patch shape based on the current buffer size\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    cnn_patch_shape = (patch_width, patch_width, len(raster_paths))\n",
    "\n",
    "mlp_input_dim = mlp_train.shape[1]\n",
    "\n",
    "model = build_transformer_fusion_model(cnn_patch_shape, gnn_input_dim, mlp_input_dim)\n",
    "model.summary()\n",
    "\n",
    "# ==================== 6. Create Data Generators ==================== #\n",
    "train_generator = DataGenerator(\n",
    "    coords=coords_train, mlp_data=mlp_train, gnn_data=gnn_train, y=y_train,\n",
    "    raster_paths=raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "# ==================== 7. Train Model ==================== #\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=train_generator\n",
    ")\n",
    "\n",
    "# ==================== 8. Evaluate & Perform Feature Importance ==================== #\n",
    "y_pred_train = model.predict(train_generator).flatten()\n",
    "r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))\n",
    "\n",
    "r2_test, rmse_test = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, BUFFER_METERS, batch_size=batch_size)\n",
    "\n",
    "print(f\"\\n Transformer-based Fusion Model Performance ({BUFFER_METERS}m):\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# Calculate and print feature importance\n",
    "feature_importance = calculate_permutation_importance(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, BUFFER_METERS, batch_size=batch_size)\n",
    "print(\"\\n--- Feature Importance (Permutation) ---\")\n",
    "sorted_importance = sorted(feature_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, score in sorted_importance:\n",
    "    print(f\"{feature}: {score:.4f}\")\n",
    "\n",
    "\n",
    "# ==================== 9. Save all info to a folder ==================== #\n",
    "# Restore standard output\n",
    "sys.stdout = old_stdout\n",
    "printed_output = captured_output.getvalue()\n",
    "\n",
    "output_folder = \"transformer_fusion\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(f\"\\nCreating folder: '{output_folder}' and saving results...\")\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(output_folder, \"transformer_fusion_model.keras\")\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save the predictions and true labels\n",
    "np.save(os.path.join(output_folder, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(output_folder, \"y_test.npy\"), y_test)\n",
    "np.save(os.path.join(output_folder, \"y_pred_train.npy\"), y_pred_train)\n",
    "np.save(os.path.join(output_folder, \"y_pred_test.npy\"), y_pred_test)\n",
    "print(f\"Predictions and true labels saved as .npy files.\")\n",
    "\n",
    "# Save the printed output to a text file\n",
    "output_path = os.path.join(output_folder, \"analysis_output.txt\")\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(printed_output)\n",
    "print(f\"Analysis results saved to: {output_path}\")\n",
    "\n",
    "# Save the feature importance dictionary as a .pkl file\n",
    "importance_path = os.path.join(output_folder, \"feature_importance.pkl\")\n",
    "with open(importance_path, 'wb') as f:\n",
    "    pickle.dump(feature_importance, f)\n",
    "print(f\"Feature importance results saved to: {importance_path}\")\n",
    "\n",
    "print(\"\\nAll information successfully saved.\")\n",
    "\n",
    "# Garbage collect to free up memory now that everything is saved\n",
    "del model, history, train_generator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e625ba2c-9730-4c23-a33a-972f97b36a88",
   "metadata": {},
   "source": [
    "# GNN-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f617f76c-a9b9-4a83-b0d9-1777e51a3fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_49\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_49\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ mlp_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_203 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ mlp_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_204 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">14,208</span> │ gnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dense_203[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dense_204[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_29      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mlp_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ gnn_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_205 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ concatenate_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_89          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_205[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_206 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_89[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_206[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ mlp_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m110\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_203 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ mlp_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_204 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m14,208\u001b[0m │ gnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dense_203[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dense_204[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_29      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ mlp_embedding[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ gnn_embedding[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_205 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ concatenate_29[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_89          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_205[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_206 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_89[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_206[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">57,601</span> (225.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m57,601\u001b[0m (225.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">57,601</span> (225.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m57,601\u001b[0m (225.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "14845"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout, Layer, MultiHeadAttention, LayerNormalization, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import sys\n",
    "from io import StringIO\n",
    "import pickle # Import the pickle library for saving objects\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# Train-test split\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "# We are not using rasters in this GNN-MLP model, but the paths are still\n",
    "# defined for consistency with previous versions.\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(\"Note: Raster data is not used in this GNN-MLP model.\")\n",
    "\n",
    "# ==================== 3. Create a Custom Data Generator ==================== #\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, mlp_data, gnn_data, y, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "        batch_y = self.y[batch_indices]\n",
    "        \n",
    "        return (batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #\n",
    "coords_train = train_combined[['Long','Lat']].values\n",
    "coords_test = test_orig[['Long','Lat']].values\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_test_train = distance_matrix(coords_test, coords_train)\n",
    "gnn_test = np.exp(-dist_mat_test_train/10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mlp_train = scaler.fit_transform(train_combined[numeric_cols])\n",
    "mlp_test = scaler.transform(test_orig[numeric_cols])\n",
    "y_train = train_combined['RI'].values\n",
    "y_test = test_orig['RI'].values\n",
    "\n",
    "# ==================== 5. Define GNN-MLP Fusion Model ==================== #\n",
    "def build_gnn_mlp_model(mlp_dim, gnn_dim):\n",
    "    # Inputs for all branches\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    \n",
    "    # --- MLP Branch ---\n",
    "    mlp_embedding = Dense(128, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(64, activation=\"relu\", name=\"mlp_embedding\")(mlp_embedding)\n",
    "\n",
    "    # --- GNN Branch ---\n",
    "    gnn_embedding = Dense(128, activation=\"relu\")(gnn_input)\n",
    "    gnn_embedding = Dense(64, activation=\"relu\", name=\"gnn_embedding\")(gnn_embedding)\n",
    "\n",
    "    # --- Concatenate Embeddings ---\n",
    "    combined = Concatenate()([mlp_embedding, gnn_embedding])\n",
    "    \n",
    "    # Final dense layers for prediction\n",
    "    f = Dense(128, activation=\"relu\")(combined)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[mlp_input, gnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, mlp_test, gnn_test_matrix, y_test, return_preds=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on given data and returns R², RMSE, and predictions.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict((mlp_test, gnn_test_matrix)).flatten()\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return r2, rmse\n",
    "\n",
    "def calculate_permutation_importance(model, mlp_data, gnn_data, y_true):\n",
    "    \"\"\"\n",
    "    Calculates permutation feature importance for the MLP and GNN branches.\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting Permutation Feature Importance Analysis...\")\n",
    "    # Get baseline R² on the unshuffled data\n",
    "    baseline_r2, _ = evaluate_model(model, mlp_data, gnn_data, y_true)\n",
    "    print(f\"Baseline R² on test set: {baseline_r2:.4f}\")\n",
    "\n",
    "    importance = {}\n",
    "    \n",
    "    # Permute MLP input\n",
    "    shuffled_mlp_data = mlp_data.copy()\n",
    "    np.random.shuffle(shuffled_mlp_data)\n",
    "    shuffled_r2, _ = evaluate_model(model, shuffled_mlp_data, gnn_data, y_true)\n",
    "    importance['MLP'] = baseline_r2 - shuffled_r2\n",
    "\n",
    "    # Permute GNN input\n",
    "    shuffled_gnn_data = gnn_data.copy()\n",
    "    np.random.shuffle(shuffled_gnn_data)\n",
    "    shuffled_r2, _ = evaluate_model(model, mlp_data, shuffled_gnn_data, y_true)\n",
    "    importance['GNN'] = baseline_r2 - shuffled_r2\n",
    "\n",
    "    return importance\n",
    "        \n",
    "# ==================== Run the Analysis ==================== #\n",
    "# Redirect output to a string for later saving\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = captured_output = StringIO()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing GNN-MLP Fusion Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "mlp_input_dim = mlp_train.shape[1]\n",
    "\n",
    "model = build_gnn_mlp_model(mlp_input_dim, gnn_input_dim)\n",
    "model.summary()\n",
    "\n",
    "# ==================== 6. Create Data Generators ==================== #\n",
    "train_generator = DataGenerator(\n",
    "    mlp_data=mlp_train, gnn_data=gnn_train, y=y_train,\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "# ==================== 7. Train Model ==================== #\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=train_generator\n",
    ")\n",
    "\n",
    "# ==================== 8. Evaluate & Perform Feature Importance ==================== #\n",
    "# Predict on the training data using the generator\n",
    "y_pred_train = model.predict(train_generator).flatten()\n",
    "r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))\n",
    "\n",
    "# Evaluate on the test data using the updated function\n",
    "r2_test, rmse_test = evaluate_model(model, mlp_test, gnn_test, y_test)\n",
    "y_pred_test = evaluate_model(model, mlp_test, gnn_test, y_test, return_preds=True)\n",
    "\n",
    "print(f\"\\n GNN-MLP Fusion Model Performance:\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# Calculate and print feature importance\n",
    "feature_importance = calculate_permutation_importance(model, mlp_test, gnn_test, y_test)\n",
    "print(\"\\n--- Feature Importance (Permutation) ---\")\n",
    "sorted_importance = sorted(feature_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, score in sorted_importance:\n",
    "    print(f\"{feature}: {score:.4f}\")\n",
    "\n",
    "# ==================== 9. Save all info to a folder ==================== #\n",
    "# Restore standard output\n",
    "sys.stdout = old_stdout\n",
    "printed_output = captured_output.getvalue()\n",
    "\n",
    "output_folder = \"gnn_mlp\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(f\"\\nCreating folder: '{output_folder}' and saving results...\")\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(output_folder, \"gnn_mlp_model.keras\")\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save the predictions and true labels\n",
    "np.save(os.path.join(output_folder, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(output_folder, \"y_test.npy\"), y_test)\n",
    "np.save(os.path.join(output_folder, \"y_pred_train.npy\"), y_pred_train)\n",
    "np.save(os.path.join(output_folder, \"y_pred_test.npy\"), y_pred_test)\n",
    "print(f\"Predictions and true labels saved as .npy files.\")\n",
    "\n",
    "# Save the printed output to a text file\n",
    "output_path = os.path.join(output_folder, \"analysis_output.txt\")\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(printed_output)\n",
    "print(f\"Analysis results saved to: {output_path}\")\n",
    "\n",
    "# Save the feature importance dictionary as a .pkl file\n",
    "importance_path = os.path.join(output_folder, \"feature_importance.pkl\")\n",
    "with open(importance_path, 'wb') as f:\n",
    "    pickle.dump(feature_importance, f)\n",
    "print(f\"Feature importance results saved to: {importance_path}\")\n",
    "\n",
    "print(\"\\nAll information successfully saved.\")\n",
    "\n",
    "# Garbage collect to free up memory now that everything is saved\n",
    "del model, history, train_generator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273ed05-6660-4dbe-a0f1-dc8d8c719cb4",
   "metadata": {},
   "source": [
    "# GNN-MLP Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1463f2dc-b4ad-45c8-a72e-b6c169b100d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_50\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_50\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ mlp_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_207 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ mlp_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_208 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">14,208</span> │ gnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_encoder_output  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dense_207[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_encoder_output  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dense_208[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ latent_space        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mlp_encoder_outp… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ gnn_encoder_outp… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_209 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ latent_space[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_90          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_209[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_210 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_90[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_210[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ mlp_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m110\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_207 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ mlp_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_208 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m14,208\u001b[0m │ gnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_encoder_output  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dense_207[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_encoder_output  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dense_208[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ latent_space        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ mlp_encoder_outp… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ gnn_encoder_outp… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_209 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ latent_space[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_90          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_209[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_210 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_90[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_210[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">57,601</span> (225.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m57,601\u001b[0m (225.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">57,601</span> (225.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m57,601\u001b[0m (225.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8731"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout, Layer, MultiHeadAttention, LayerNormalization, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import sys\n",
    "from io import StringIO\n",
    "import pickle # Import the pickle library for saving objects\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# Train-test split\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "# We are not using rasters in this GNN-MLP model, but the paths are still\n",
    "# defined for consistency with previous versions.\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(\"Note: Raster data is not used in this GNN-MLP model.\")\n",
    "\n",
    "# ==================== 3. Create a Custom Data Generator ==================== #\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, mlp_data, gnn_data, y, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "        batch_y = self.y[batch_indices]\n",
    "        \n",
    "        return (batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #\n",
    "coords_train = train_combined[['Long','Lat']].values\n",
    "coords_test = test_orig[['Long','Lat']].values\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_test_train = distance_matrix(coords_test, coords_train)\n",
    "gnn_test = np.exp(-dist_mat_test_train/10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mlp_train = scaler.fit_transform(train_combined[numeric_cols])\n",
    "mlp_test = scaler.transform(test_orig[numeric_cols])\n",
    "y_train = train_combined['RI'].values\n",
    "y_test = test_orig['RI'].values\n",
    "\n",
    "# ==================== 5. Define GNN-MLP Fusion Autoencoder Model ==================== #\n",
    "def build_gnn_mlp_autoencoder_model(mlp_dim, gnn_dim):\n",
    "    # Inputs for all branches\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    \n",
    "    # --- Encoder Branch (MLP) ---\n",
    "    mlp_encoded = Dense(128, activation=\"relu\")(mlp_input)\n",
    "    mlp_encoded = Dense(64, activation=\"relu\", name=\"mlp_encoder_output\")(mlp_encoded)\n",
    "\n",
    "    # --- Encoder Branch (GNN) ---\n",
    "    gnn_encoded = Dense(128, activation=\"relu\")(gnn_input)\n",
    "    gnn_encoded = Dense(64, activation=\"relu\", name=\"gnn_encoder_output\")(gnn_encoded)\n",
    "\n",
    "    # --- Bottleneck/Latent Space ---\n",
    "    # Concatenate the encoded representations\n",
    "    latent_space = Concatenate(name=\"latent_space\")([mlp_encoded, gnn_encoded])\n",
    "    \n",
    "    # --- Decoder Branch for Prediction ---\n",
    "    # The decoder takes the latent space and performs the final prediction\n",
    "    f = Dense(128, activation=\"relu\")(latent_space)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[mlp_input, gnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, mlp_test, gnn_test_matrix, y_test, return_preds=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model on given data and returns R², RMSE, and predictions.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict((mlp_test, gnn_test_matrix)).flatten()\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return r2, rmse\n",
    "\n",
    "def calculate_permutation_importance(model, mlp_data, gnn_data, y_true):\n",
    "    \"\"\"\n",
    "    Calculates permutation feature importance for the MLP and GNN branches.\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting Permutation Feature Importance Analysis...\")\n",
    "    # Get baseline R² on the unshuffled data\n",
    "    baseline_r2, _ = evaluate_model(model, mlp_data, gnn_data, y_true)\n",
    "    print(f\"Baseline R² on test set: {baseline_r2:.4f}\")\n",
    "\n",
    "    importance = {}\n",
    "    \n",
    "    # Permute MLP input\n",
    "    shuffled_mlp_data = mlp_data.copy()\n",
    "    np.random.shuffle(shuffled_mlp_data)\n",
    "    shuffled_r2, _ = evaluate_model(model, shuffled_mlp_data, gnn_data, y_true)\n",
    "    importance['MLP'] = baseline_r2 - shuffled_r2\n",
    "\n",
    "    # Permute GNN input\n",
    "    shuffled_gnn_data = gnn_data.copy()\n",
    "    np.random.shuffle(shuffled_gnn_data)\n",
    "    shuffled_r2, _ = evaluate_model(model, mlp_data, shuffled_gnn_data, y_true)\n",
    "    importance['GNN'] = baseline_r2 - shuffled_r2\n",
    "\n",
    "    return importance\n",
    "        \n",
    "# ==================== Run the Analysis ==================== #\n",
    "# Redirect output to a string for later saving\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = captured_output = StringIO()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing GNN-MLP Autoencoder Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "mlp_input_dim = mlp_train.shape[1]\n",
    "\n",
    "model = build_gnn_mlp_autoencoder_model(mlp_input_dim, gnn_input_dim)\n",
    "model.summary()\n",
    "\n",
    "# ==================== 6. Create Data Generators ==================== #\n",
    "train_generator = DataGenerator(\n",
    "    mlp_data=mlp_train, gnn_data=gnn_train, y=y_train,\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "# ==================== 7. Train Model ==================== #\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=train_generator\n",
    ")\n",
    "\n",
    "# ==================== 8. Evaluate & Perform Feature Importance ==================== #\n",
    "# Predict on the training data using the generator\n",
    "y_pred_train = model.predict(train_generator).flatten()\n",
    "r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))\n",
    "\n",
    "# Evaluate on the test data using the updated function\n",
    "r2_test, rmse_test = evaluate_model(model, mlp_test, gnn_test, y_test)\n",
    "y_pred_test = evaluate_model(model, mlp_test, gnn_test, y_test, return_preds=True)\n",
    "\n",
    "print(f\"\\n GNN-MLP Autoencoder Model Performance:\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# Calculate and print feature importance\n",
    "feature_importance = calculate_permutation_importance(model, mlp_test, gnn_test, y_test)\n",
    "print(\"\\n--- Feature Importance (Permutation) ---\")\n",
    "sorted_importance = sorted(feature_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, score in sorted_importance:\n",
    "    print(f\"{feature}: {score:.4f}\")\n",
    "\n",
    "# ==================== 9. Save all info to a folder ==================== #\n",
    "# Restore standard output\n",
    "sys.stdout = old_stdout\n",
    "printed_output = captured_output.getvalue()\n",
    "\n",
    "output_folder = \"gnn_mlp_ae\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(f\"\\nCreating folder: '{output_folder}' and saving results...\")\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(output_folder, \"gnn_mlp_ae_model.keras\")\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save the predictions and true labels\n",
    "np.save(os.path.join(output_folder, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(output_folder, \"y_test.npy\"), y_test)\n",
    "np.save(os.path.join(output_folder, \"y_pred_train.npy\"), y_pred_train)\n",
    "np.save(os.path.join(output_folder, \"y_pred_test.npy\"), y_pred_test)\n",
    "print(f\"Predictions and true labels saved as .npy files.\")\n",
    "\n",
    "# Save the printed output to a text file\n",
    "output_path = os.path.join(output_folder, \"analysis_output.txt\")\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(printed_output)\n",
    "print(f\"Analysis results saved to: {output_path}\")\n",
    "\n",
    "# Save the feature importance dictionary as a .pkl file\n",
    "importance_path = os.path.join(output_folder, \"feature_importance.pkl\")\n",
    "with open(importance_path, 'wb') as f:\n",
    "    pickle.dump(feature_importance, f)\n",
    "print(f\"Feature importance results saved to: {importance_path}\")\n",
    "\n",
    "print(\"\\nAll information successfully saved.\")\n",
    "\n",
    "# Garbage collect to free up memory now that everything is saved\n",
    "del model, history, train_generator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ff9bd1-9951-47ba-9d92-a8c8f5370346",
   "metadata": {},
   "source": [
    "# GCN GAT"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a06dba0-12a6-4dbb-b0fd-339ddb88c27f",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import pickle # Import for saving feature importance results\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "# NOTE: The file paths are relative to the notebook's execution directory.\n",
    "# Please ensure they are correct for your environment.\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# Train-test split\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "# We are not using rasters in this GNN model, but the paths are still\n",
    "# defined for consistency with previous versions.\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(\"Note: Raster data is not used in this Stacking GNN ensemble model.\")\n",
    "\n",
    "# ==================== 3. Prepare GNN & MLP Input (only once) ==================== #\n",
    "# Split the combined training data into a training and a validation set\n",
    "mlp_train_val, mlp_test = train_test_split(train_combined, test_size=len(test_orig), random_state=42)\n",
    "y_train_val, y_test = train_test_split(train_combined['RI'], test_size=len(test_orig), random_state=42)\n",
    "mlp_train, mlp_val, y_train, y_val = train_test_split(mlp_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now, re-do the distance matrices and scaling with the new splits\n",
    "coords_train = mlp_train[['Long', 'Lat']].values\n",
    "coords_val = mlp_val[['Long', 'Lat']].values\n",
    "coords_test = test_orig[['Long', 'Lat']].values\n",
    "\n",
    "# Create distance matrices, which serve as the adjacency matrix for the GNN\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train_data = np.exp(-dist_mat_train/10) # Using a radial basis function kernel\n",
    "dist_mat_val = distance_matrix(coords_val, coords_val)\n",
    "gnn_val_data = np.exp(-dist_mat_val/10)\n",
    "dist_mat_test = distance_matrix(coords_test, coords_test)\n",
    "gnn_test_data = np.exp(-dist_mat_test/10)\n",
    "\n",
    "# Scale the MLP features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "mlp_train_scaled = scaler.fit_transform(mlp_train[numeric_cols])\n",
    "mlp_val_scaled = scaler.transform(mlp_val[numeric_cols])\n",
    "mlp_test_scaled = scaler.transform(test_orig[numeric_cols])\n",
    "\n",
    "# Convert target data to numpy arrays\n",
    "y_train_arr = y_train.values\n",
    "y_val_arr = y_val.values\n",
    "y_test_arr = y_test.values\n",
    "\n",
    "# Add a batch dimension to the data since we're using full-graph training\n",
    "mlp_train_data = np.expand_dims(mlp_train_scaled, axis=0)\n",
    "gnn_train_data = np.expand_dims(gnn_train_data, axis=0)\n",
    "mlp_val_data = np.expand_dims(mlp_val_scaled, axis=0)\n",
    "gnn_val_data = np.expand_dims(gnn_val_data, axis=0)\n",
    "mlp_test_data = np.expand_dims(mlp_test_scaled, axis=0)\n",
    "gnn_test_data = np.expand_dims(gnn_test_data, axis=0)\n",
    "\n",
    "\n",
    "# ==================== 4. Define Stacking GNN Ensemble Model ==================== #\n",
    "\n",
    "class GCNLayer(Layer):\n",
    "    \"\"\"\n",
    "    Custom GCN Layer. Given the pre-computed similarity matrix, this layer\n",
    "    aggregates information from neighboring nodes and transforms it.\n",
    "    \"\"\"\n",
    "    def __init__(self, units, activation=\"relu\", **kwargs):\n",
    "        super(GCNLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape is a list of two shapes: [(batch, nodes, features), (batch, nodes, nodes)]\n",
    "        mlp_shape, gnn_shape = input_shape\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(mlp_shape[-1], self.units),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        super(GCNLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mlp_input, gnn_input = inputs\n",
    "        # Perform batched matrix multiplication: (B, N, N) x (B, N, F) -> (B, N, F)\n",
    "        aggregated_features = tf.matmul(gnn_input, mlp_input)\n",
    "        # Apply the linear transformation: (B, N, F) x (F, U) -> (B, N, U)\n",
    "        output = tf.matmul(aggregated_features, self.kernel)\n",
    "        # Apply activation\n",
    "        return self.activation(output)\n",
    "\n",
    "class GATLayer(Layer):\n",
    "    \"\"\"\n",
    "    Custom GAT Layer. This layer computes attention scores for neighboring\n",
    "    nodes and aggregates features based on these scores.\n",
    "    \"\"\"\n",
    "    def __init__(self, units, num_heads=4, activation=\"relu\", **kwargs):\n",
    "        super(GATLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.num_heads = num_heads\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        mlp_shape, gnn_shape = input_shape\n",
    "        # The feature transformation kernel\n",
    "        self.kernel_f = self.add_weight(\n",
    "            shape=(mlp_shape[-1], self.units * self.num_heads),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        # The attention score kernels\n",
    "        # Kernel 1 for the source node, Kernel 2 for the target node\n",
    "        self.kernel_a_1 = self.add_weight(\n",
    "            shape=(self.units, 1),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.kernel_a_2 = self.add_weight(\n",
    "            shape=(self.units, 1),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        super(GATLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        mlp_input, gnn_input = inputs\n",
    "        \n",
    "        # Linear transformation\n",
    "        features = tf.matmul(mlp_input, self.kernel_f)\n",
    "        \n",
    "        # Split features into attention heads and transpose\n",
    "        # Shape: (batch_size, num_nodes, num_heads, units)\n",
    "        features_heads = tf.reshape(features, (-1, tf.shape(mlp_input)[1], self.num_heads, self.units))\n",
    "        # Transpose to (batch_size, num_heads, num_nodes, units) for easier batched operations\n",
    "        features_heads_t = tf.transpose(features_heads, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # Calculate attention scores for each head\n",
    "        # This will be of shape (batch_size, num_heads, num_nodes, 1)\n",
    "        e_input_1 = tf.matmul(features_heads_t, self.kernel_a_1)\n",
    "        # This will be of shape (batch_size, num_heads, num_nodes, 1)\n",
    "        e_input_2_pre_t = tf.matmul(features_heads_t, self.kernel_a_2)\n",
    "        # Transpose the last two dimensions to get shape (batch_size, num_heads, 1, num_nodes)\n",
    "        e_input_2 = tf.transpose(e_input_2_pre_t, perm=[0, 1, 3, 2])\n",
    "        \n",
    "        # Combine the scores using broadcasting to create the attention matrix for each head\n",
    "        # Shape will be (batch_size, num_heads, num_nodes, num_nodes)\n",
    "        e = e_input_1 + e_input_2\n",
    "        e = tf.nn.leaky_relu(e, alpha=0.2)\n",
    "\n",
    "        # Mask attention scores for non-existent edges\n",
    "        # The gnn_input is (batch, nodes, nodes), expand it to (batch, 1, nodes, nodes)\n",
    "        # so it can be broadcast to the attention_scores shape\n",
    "        mask = -1e9 * (1.0 - tf.expand_dims(gnn_input, axis=1))\n",
    "        attention_scores = e + mask\n",
    "        \n",
    "        # Softmax normalization across nodes (the last axis)\n",
    "        attention = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        \n",
    "        # Aggregate features\n",
    "        # Perform batched matrix multiplication: attention (B,H,N,N) * features_heads_t (B,H,N,U) -> (B, H, N, U)\n",
    "        aggregated_features = tf.matmul(attention, features_heads_t)\n",
    "        \n",
    "        # Concatenate heads and apply final activation\n",
    "        # Reshape to (batch_size, num_nodes, num_heads * units)\n",
    "        output = tf.reshape(aggregated_features, (-1, tf.shape(mlp_input)[1], self.units * self.num_heads))\n",
    "        return self.activation(output)\n",
    "\n",
    "def build_stacking_ensemble_model(mlp_dim):\n",
    "    \"\"\"\n",
    "    Builds a stacking ensemble model with GCN and GAT base learners\n",
    "    and an MLP meta-learner.\n",
    "    \n",
    "    NOTE: The model architecture has been updated to produce a prediction\n",
    "    for each node in the graph, rather than a single prediction for the\n",
    "    entire graph, which was the cause of the previous ValueError.\n",
    "    \"\"\"\n",
    "    # Define inputs for all branches\n",
    "    # The `None` allows for a variable number of nodes per graph\n",
    "    mlp_input = Input(shape=(None, mlp_dim), name=\"mlp_input\")\n",
    "    gnn_input = Input(shape=(None, None), name=\"gnn_input\")\n",
    "    \n",
    "    # --- GCN Base Learner Branch ---\n",
    "    # This branch now outputs node-level features, not a single pooled vector\n",
    "    gcn_branch = GCNLayer(128, name=\"gcn_layer_1\")([mlp_input, gnn_input])\n",
    "    gcn_branch = Dropout(0.2)(gcn_branch)\n",
    "    gcn_output_features = GCNLayer(64, name=\"gcn_layer_2\")([gcn_branch, gnn_input])\n",
    "    \n",
    "    # --- GAT Base Learner Branch ---\n",
    "    # This branch also outputs node-level features\n",
    "    gat_branch = GATLayer(64, num_heads=4, name=\"gat_layer_1\")([mlp_input, gnn_input])\n",
    "    gat_branch = Dropout(0.2)(gat_branch)\n",
    "    gat_output_features = GATLayer(32, num_heads=4, name=\"gat_layer_2\")([gat_branch, gnn_input])\n",
    "    \n",
    "    # --- MLP Meta-Learner (now a prediction head) ---\n",
    "    # Concatenate the node-level feature outputs from the GNN branches\n",
    "    meta_learner_input = Concatenate(name=\"meta_learner_input\")([gcn_output_features, gat_output_features])\n",
    "    \n",
    "    # The final prediction layers operate on the node features to produce a single\n",
    "    # value for each node.\n",
    "    meta_learner_output = Dense(16, activation=\"relu\", name=\"meta_dense_1\")(meta_learner_input)\n",
    "    meta_learner_output = Dense(8, activation=\"relu\", name=\"meta_dense_2\")(meta_learner_output)\n",
    "    \n",
    "    # Final prediction layer: one output per node\n",
    "    final_output = Dense(1, activation=\"linear\", name=\"final_output\")(meta_learner_output)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[mlp_input, gnn_input], outputs=final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# Function to calculate permutation feature importance\n",
    "def calculate_permutation_importance(model, mlp_data, gnn_data, y_true, feature_names):\n",
    "    \"\"\"\n",
    "    Calculates permutation feature importance for each feature.\n",
    "    \n",
    "    NOTE: This function now expects un-batched `mlp_data` and `gnn_data` and\n",
    "    handles the batching internally for predictions.\n",
    "    \"\"\"\n",
    "    print(\"\\nCalculating permutation feature importance...\")\n",
    "    \n",
    "    # 1. Calculate a baseline score on the un-permuted data\n",
    "    # Add batch dimension to data for prediction\n",
    "    y_pred_baseline = model.predict([np.expand_dims(mlp_data, axis=0), np.expand_dims(gnn_data, axis=0)]).flatten()\n",
    "    baseline_score = mean_squared_error(y_true, y_pred_baseline)\n",
    "    \n",
    "    importance_scores = {}\n",
    "    \n",
    "    # 2. Iterate through each feature\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        # Create a copy of the data to avoid modifying the original\n",
    "        X_mlp_permuted = mlp_data.copy()\n",
    "        \n",
    "        # Shuffle the values of the current feature\n",
    "        X_mlp_permuted[:, i] = np.random.permutation(X_mlp_permuted[:, i])\n",
    "        \n",
    "        # 3. Make predictions with the permuted data\n",
    "        # Add batch dimension to permuted data for prediction\n",
    "        y_pred_permuted = model.predict([np.expand_dims(X_mlp_permuted, axis=0), np.expand_dims(gnn_data, axis=0)]).flatten()\n",
    "        \n",
    "        # 4. Calculate the new score and the importance\n",
    "        permuted_score = mean_squared_error(y_true, y_pred_permuted)\n",
    "        importance = permuted_score - baseline_score\n",
    "        \n",
    "        importance_scores[feature] = importance\n",
    "        print(f\"  Feature '{feature}': Importance = {importance:.4f}\")\n",
    "        \n",
    "    return importance_scores\n",
    "\n",
    "\n",
    "# ==================== Run the Analysis ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing Stacking GNN Ensemble Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "mlp_input_dim = mlp_train_scaled.shape[1]\n",
    "\n",
    "# Build the stacking ensemble model\n",
    "model = build_stacking_ensemble_model(mlp_input_dim)\n",
    "model.summary()\n",
    "\n",
    "# ==================== 5. Train Model ==================== #\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# NOTE: Training on the full graph, not a generator.\n",
    "# The y data now also needs a batch dimension to match the model's output\n",
    "history = model.fit(\n",
    "    x=[mlp_train_data, gnn_train_data],\n",
    "    y=np.expand_dims(y_train_arr, axis=0),\n",
    "    epochs=2, # Increased epochs for better training\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=([mlp_val_data, gnn_val_data], np.expand_dims(y_val_arr, axis=0))\n",
    ")\n",
    "\n",
    "# ==================== 6. Evaluate ==================== #\n",
    "# Predict on the training data\n",
    "# The model now outputs predictions for each node, so flattening works correctly.\n",
    "y_pred_train = model.predict([mlp_train_data, gnn_train_data]).flatten()\n",
    "r2_train = r2_score(y_train_arr, y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train_arr, y_pred_train))\n",
    "\n",
    "# Evaluate on the validation data\n",
    "y_pred_val = model.predict([mlp_val_data, gnn_val_data]).flatten()\n",
    "r2_val = r2_score(y_val_arr, y_pred_val)\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val_arr, y_pred_val))\n",
    "\n",
    "# Evaluate on the test data\n",
    "y_pred_test = model.predict([mlp_test_data, gnn_test_data]).flatten()\n",
    "r2_test = r2_score(y_test_arr, y_pred_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test_arr, y_pred_test))\n",
    "\n",
    "\n",
    "print(f\"\\n Stacking GNN Ensemble Model Performance:\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Val: {r2_val:.4f} | RMSE Val: {rmse_val:.4f}\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "\n",
    "# ==================== 7. Permutation Importance and Saving Results ==================== #\n",
    "\n",
    "# First, calculate feature importance on the test data\n",
    "# Pass un-batched adjacency matrix for consistency\n",
    "importance_results = calculate_permutation_importance(\n",
    "    model=model,\n",
    "    mlp_data=mlp_test_scaled, # Pass the un-batched data here\n",
    "    gnn_data=dist_mat_test, # Pass the un-batched adjacency matrix\n",
    "    y_true=y_test_arr,\n",
    "    feature_names=numeric_cols\n",
    ")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "output_dir = 'gcn_gat'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the model in the .keras format\n",
    "model_path = os.path.join(output_dir, 'gcn_gat.keras')\n",
    "model.save(model_path)\n",
    "print(f\"\\nModel saved to {model_path}\")\n",
    "\n",
    "# Save the feature importance results as a pickled file\n",
    "importance_path = os.path.join(output_dir, 'feature_importance.pkl')\n",
    "with open(importance_path, 'wb') as f:\n",
    "    pickle.dump(importance_results, f)\n",
    "print(f\"Feature importance results saved to {importance_path}\")\n",
    "\n",
    "# Save the data splits for reproducibility in .npy format\n",
    "np.save(os.path.join(output_dir, 'mlp_train_data.npy'), mlp_train_scaled)\n",
    "np.save(os.path.join(output_dir, 'mlp_val_data.npy'), mlp_val_scaled)\n",
    "np.save(os.path.join(output_dir, 'mlp_test_data.npy'), mlp_test_scaled)\n",
    "np.save(os.path.join(output_dir, 'y_train_data.npy'), y_train_arr)\n",
    "np.save(os.path.join(output_dir, 'y_val_data.npy'), y_val_arr)\n",
    "np.save(os.path.join(output_dir, 'y_test_data.npy'), y_test_arr)\n",
    "print(\"Training, validation, and test data splits saved to the gnn_gat folder in .npy format.\")\n",
    "\n",
    "# Garbage collect to free up memory\n",
    "del model, history\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nAnalysis complete and files have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e6a19a2-7880-471f-b7d8-715fe98f446e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import pickle # Import for saving feature importance results\n",
    "import sys # Import sys for stdout redirection\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# Save the original stdout\n",
    "original_stdout = sys.stdout\n",
    "\n",
    "# Try-finally block to ensure stdout is restored even if errors occur\n",
    "try:\n",
    "    # Open the file and redirect stdout to it\n",
    "    with open('gcn_gat/analysis_output.txt', 'w') as f:\n",
    "        sys.stdout = f\n",
    "\n",
    "        # ==================== 1. Load Data ==================== #\n",
    "        # NOTE: The file paths are relative to the notebook's execution directory.\n",
    "        # Please ensure they are correct for your environment.\n",
    "        orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "        river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "\n",
    "        drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "        numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "        # Train-test split\n",
    "        train_orig = orig.sample(10, random_state=42)\n",
    "        test_orig = orig.drop(train_orig.index)\n",
    "        train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "        # ==================== 2. Collect ALL Rasters ==================== #\n",
    "        # We are not using rasters in this GNN model, but the paths are still\n",
    "        # defined for consistency with previous versions.\n",
    "        raster_paths = []\n",
    "        raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "        raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "        raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "        print(\"Note: Raster data is not used in this Stacking GNN ensemble model.\")\n",
    "\n",
    "        # ==================== 3. Prepare GNN & MLP Input (only once) ==================== #\n",
    "        # Split the combined training data into a training and a validation set\n",
    "        mlp_train_val, mlp_test = train_test_split(train_combined, test_size=len(test_orig), random_state=42)\n",
    "        y_train_val, y_test = train_test_split(train_combined['RI'], test_size=len(test_orig), random_state=42)\n",
    "        mlp_train, mlp_val, y_train, y_val = train_test_split(mlp_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Now, re-do the distance matrices and scaling with the new splits\n",
    "        coords_train = mlp_train[['Long', 'Lat']].values\n",
    "        coords_val = mlp_val[['Long', 'Lat']].values\n",
    "        coords_test = test_orig[['Long', 'Lat']].values\n",
    "\n",
    "        # Create distance matrices, which serve as the adjacency matrix for the GNN\n",
    "        dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "        gnn_train_data = np.exp(-dist_mat_train/10) # Using a radial basis function kernel\n",
    "        dist_mat_val = distance_matrix(coords_val, coords_val)\n",
    "        gnn_val_data = np.exp(-dist_mat_val/10)\n",
    "        dist_mat_test = distance_matrix(coords_test, coords_test)\n",
    "        gnn_test_data = np.exp(-dist_mat_test/10)\n",
    "\n",
    "        # Scale the MLP features using StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        mlp_train_scaled = scaler.fit_transform(mlp_train[numeric_cols])\n",
    "        mlp_val_scaled = scaler.transform(mlp_val[numeric_cols])\n",
    "        mlp_test_scaled = scaler.transform(test_orig[numeric_cols])\n",
    "\n",
    "        # Convert target data to numpy arrays\n",
    "        y_train_arr = y_train.values\n",
    "        y_val_arr = y_val.values\n",
    "        y_test_arr = y_test.values\n",
    "\n",
    "        # Add a batch dimension to the data since we're using full-graph training\n",
    "        mlp_train_data = np.expand_dims(mlp_train_scaled, axis=0)\n",
    "        gnn_train_data = np.expand_dims(gnn_train_data, axis=0)\n",
    "        mlp_val_data = np.expand_dims(mlp_val_scaled, axis=0)\n",
    "        gnn_val_data = np.expand_dims(gnn_val_data, axis=0)\n",
    "        mlp_test_data = np.expand_dims(mlp_test_scaled, axis=0)\n",
    "        gnn_test_data = np.expand_dims(gnn_test_data, axis=0)\n",
    "\n",
    "\n",
    "        # ==================== 4. Define Stacking GNN Ensemble Model ==================== #\n",
    "\n",
    "        class GCNLayer(Layer):\n",
    "            \"\"\"\n",
    "            Custom GCN Layer. Given the pre-computed similarity matrix, this layer\n",
    "            aggregates information from neighboring nodes and transforms it.\n",
    "            \"\"\"\n",
    "            def __init__(self, units, activation=\"relu\", **kwargs):\n",
    "                super(GCNLayer, self).__init__(**kwargs)\n",
    "                self.units = units\n",
    "                self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "            def build(self, input_shape):\n",
    "                # input_shape is a list of two shapes: [(batch, nodes, features), (batch, nodes, nodes)]\n",
    "                mlp_shape, gnn_shape = input_shape\n",
    "                self.kernel = self.add_weight(\n",
    "                    shape=(mlp_shape[-1], self.units),\n",
    "                    initializer=\"glorot_uniform\",\n",
    "                    trainable=True\n",
    "                )\n",
    "                super(GCNLayer, self).build(input_shape)\n",
    "\n",
    "            def call(self, inputs):\n",
    "                mlp_input, gnn_input = inputs\n",
    "                # Perform batched matrix multiplication: (B, N, N) x (B, N, F) -> (B, N, F)\n",
    "                aggregated_features = tf.matmul(gnn_input, mlp_input)\n",
    "                # Apply the linear transformation: (B, N, F) x (F, U) -> (B, N, U)\n",
    "                output = tf.matmul(aggregated_features, self.kernel)\n",
    "                # Apply activation\n",
    "                return self.activation(output)\n",
    "\n",
    "        class GATLayer(Layer):\n",
    "            \"\"\"\n",
    "            Custom GAT Layer. This layer computes attention scores for neighboring\n",
    "            nodes and aggregates features based on these scores.\n",
    "            \"\"\"\n",
    "            def __init__(self, units, num_heads=4, activation=\"relu\", **kwargs):\n",
    "                super(GATLayer, self).__init__(**kwargs)\n",
    "                self.units = units\n",
    "                self.num_heads = num_heads\n",
    "                self.activation = tf.keras.activations.get(activation)\n",
    "                \n",
    "            def build(self, input_shape):\n",
    "                mlp_shape, gnn_shape = input_shape\n",
    "                # The feature transformation kernel\n",
    "                self.kernel_f = self.add_weight(\n",
    "                    shape=(mlp_shape[-1], self.units * self.num_heads),\n",
    "                    initializer=\"glorot_uniform\",\n",
    "                    trainable=True\n",
    "                )\n",
    "                # The attention score kernels\n",
    "                # Kernel 1 for the source node, Kernel 2 for the target node\n",
    "                self.kernel_a_1 = self.add_weight(\n",
    "                    shape=(self.units, 1),\n",
    "                    initializer=\"glorot_uniform\",\n",
    "                    trainable=True\n",
    "                )\n",
    "                self.kernel_a_2 = self.add_weight(\n",
    "                    shape=(self.units, 1),\n",
    "                    initializer=\"glorot_uniform\",\n",
    "                    trainable=True\n",
    "                )\n",
    "                super(GATLayer, self).build(input_shape)\n",
    "            \n",
    "            def call(self, inputs):\n",
    "                mlp_input, gnn_input = inputs\n",
    "                \n",
    "                # Linear transformation\n",
    "                features = tf.matmul(mlp_input, self.kernel_f)\n",
    "                \n",
    "                # Split features into attention heads and transpose\n",
    "                # Shape: (batch_size, num_nodes, num_heads, units)\n",
    "                features_heads = tf.reshape(features, (-1, tf.shape(mlp_input)[1], self.num_heads, self.units))\n",
    "                # Transpose to (batch_size, num_heads, num_nodes, units) for easier batched operations\n",
    "                features_heads_t = tf.transpose(features_heads, perm=[0, 2, 1, 3])\n",
    "                \n",
    "                # Calculate attention scores for each head\n",
    "                # This will be of shape (batch_size, num_heads, num_nodes, 1)\n",
    "                e_input_1 = tf.matmul(features_heads_t, self.kernel_a_1)\n",
    "                # This will be of shape (batch_size, num_heads, num_nodes, 1)\n",
    "                e_input_2_pre_t = tf.matmul(features_heads_t, self.kernel_a_2)\n",
    "                # Transpose the last two dimensions to get shape (batch_size, num_heads, 1, num_nodes)\n",
    "                e_input_2 = tf.transpose(e_input_2_pre_t, perm=[0, 1, 3, 2])\n",
    "                \n",
    "                # Combine the scores using broadcasting to create the attention matrix for each head\n",
    "                # Shape will be (batch_size, num_heads, num_nodes, num_nodes)\n",
    "                e = e_input_1 + e_input_2\n",
    "                e = tf.nn.leaky_relu(e, alpha=0.2)\n",
    "\n",
    "                # Mask attention scores for non-existent edges\n",
    "                # The gnn_input is (batch, nodes, nodes), expand it to (batch, 1, nodes, nodes)\n",
    "                # so it can be broadcast to the attention_scores shape\n",
    "                mask = -1e9 * (1.0 - tf.expand_dims(gnn_input, axis=1))\n",
    "                attention_scores = e + mask\n",
    "                \n",
    "                # Softmax normalization across nodes (the last axis)\n",
    "                attention = tf.nn.softmax(attention_scores, axis=-1)\n",
    "                \n",
    "                # Aggregate features\n",
    "                # Perform batched matrix multiplication: attention (B,H,N,N) * features_heads_t (B,H,N,U) -> (B, H, N, U)\n",
    "                aggregated_features = tf.matmul(attention, features_heads_t)\n",
    "                \n",
    "                # Concatenate heads and apply final activation\n",
    "                # Reshape to (batch_size, num_nodes, num_heads * units)\n",
    "                output = tf.reshape(aggregated_features, (-1, tf.shape(mlp_input)[1], self.units * self.num_heads))\n",
    "                return self.activation(output)\n",
    "\n",
    "        def build_stacking_ensemble_model(mlp_dim):\n",
    "            \"\"\"\n",
    "            Builds a stacking ensemble model with GCN and GAT base learners\n",
    "            and an MLP meta-learner.\n",
    "            \n",
    "            NOTE: The model architecture has been updated to produce a prediction\n",
    "            for each node in the graph, rather than a single prediction for the\n",
    "            entire graph, which was the cause of the previous ValueError.\n",
    "            \"\"\"\n",
    "            # Define inputs for all branches\n",
    "            # The `None` allows for a variable number of nodes per graph\n",
    "            mlp_input = Input(shape=(None, mlp_dim), name=\"mlp_input\")\n",
    "            gnn_input = Input(shape=(None, None), name=\"gnn_input\")\n",
    "            \n",
    "            # --- GCN Base Learner Branch ---\n",
    "            # This branch now outputs node-level features, not a single pooled vector\n",
    "            gcn_branch = GCNLayer(128, name=\"gcn_layer_1\")([mlp_input, gnn_input])\n",
    "            gcn_branch = Dropout(0.2)(gcn_branch)\n",
    "            gcn_output_features = GCNLayer(64, name=\"gcn_layer_2\")([gcn_branch, gnn_input])\n",
    "            \n",
    "            # --- GAT Base Learner Branch ---\n",
    "            # This branch also outputs node-level features\n",
    "            gat_branch = GATLayer(64, num_heads=4, name=\"gat_layer_1\")([mlp_input, gnn_input])\n",
    "            gat_branch = Dropout(0.2)(gat_branch)\n",
    "            gat_output_features = GATLayer(32, num_heads=4, name=\"gat_layer_2\")([gat_branch, gnn_input])\n",
    "            \n",
    "            # --- MLP Meta-Learner (now a prediction head) ---\n",
    "            # Concatenate the node-level feature outputs from the GNN branches\n",
    "            meta_learner_input = Concatenate(name=\"meta_learner_input\")([gcn_output_features, gat_output_features])\n",
    "            \n",
    "            # The final prediction layers operate on the node features to produce a single\n",
    "            # value for each node.\n",
    "            meta_learner_output = Dense(16, activation=\"relu\", name=\"meta_dense_1\")(meta_learner_input)\n",
    "            meta_learner_output = Dense(8, activation=\"relu\", name=\"meta_dense_2\")(meta_learner_output)\n",
    "            \n",
    "            # Final prediction layer: one output per node\n",
    "            final_output = Dense(1, activation=\"linear\", name=\"final_output\")(meta_learner_output)\n",
    "\n",
    "            # Build and compile the model\n",
    "            model = Model(inputs=[mlp_input, gnn_input], outputs=final_output)\n",
    "            model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "            return model\n",
    "\n",
    "        # Function to calculate permutation feature importance\n",
    "        def calculate_permutation_importance(model, mlp_data, gnn_data, y_true, feature_names):\n",
    "            \"\"\"\n",
    "            Calculates permutation feature importance for each feature.\n",
    "            \n",
    "            NOTE: This function now expects un-batched `mlp_data` and `gnn_data` and\n",
    "            handles the batching internally for predictions.\n",
    "            \"\"\"\n",
    "            print(\"\\nCalculating permutation feature importance...\")\n",
    "            \n",
    "            # 1. Calculate a baseline score on the un-permuted data\n",
    "            # Add batch dimension to data for prediction\n",
    "            y_pred_baseline = model.predict([np.expand_dims(mlp_data, axis=0), np.expand_dims(gnn_data, axis=0)]).flatten()\n",
    "            baseline_score = mean_squared_error(y_true, y_pred_baseline)\n",
    "            \n",
    "            importance_scores = {}\n",
    "            \n",
    "            # 2. Iterate through each feature\n",
    "            for i, feature in enumerate(feature_names):\n",
    "                # Create a copy of the data to avoid modifying the original\n",
    "                X_mlp_permuted = mlp_data.copy()\n",
    "                \n",
    "                # Shuffle the values of the current feature\n",
    "                X_mlp_permuted[:, i] = np.random.permutation(X_mlp_permuted[:, i])\n",
    "                \n",
    "                # 3. Make predictions with the permuted data\n",
    "                # Add batch dimension to permuted data for prediction\n",
    "                y_pred_permuted = model.predict([np.expand_dims(X_mlp_permuted, axis=0), np.expand_dims(gnn_data, axis=0)]).flatten()\n",
    "                \n",
    "                # 4. Calculate the new score and the importance\n",
    "                permuted_score = mean_squared_error(y_true, y_pred_permuted)\n",
    "                importance = permuted_score - baseline_score\n",
    "                \n",
    "                importance_scores[feature] = importance\n",
    "                print(f\"  Feature '{feature}': Importance = {importance:.4f}\")\n",
    "                \n",
    "            return importance_scores\n",
    "\n",
    "\n",
    "        # ==================== Run the Analysis ==================== #\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Analyzing Stacking GNN Ensemble Model\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        mlp_input_dim = mlp_train_scaled.shape[1]\n",
    "\n",
    "        # Build the stacking ensemble model\n",
    "        model = build_stacking_ensemble_model(mlp_input_dim)\n",
    "        model.summary(print_fn=print) # Use print_fn to redirect summary output\n",
    "\n",
    "        # ==================== 5. Train Model ==================== #\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # NOTE: Training on the full graph, not a generator.\n",
    "        # The y data now also needs a batch dimension to match the model's output\n",
    "        history = model.fit(\n",
    "            x=[mlp_train_data, gnn_train_data],\n",
    "            y=np.expand_dims(y_train_arr, axis=0),\n",
    "            epochs=100, # Increased epochs for better training\n",
    "            verbose=1,\n",
    "            callbacks=[early_stopping],\n",
    "            validation_data=([mlp_val_data, gnn_val_data], np.expand_dims(y_val_arr, axis=0))\n",
    "        )\n",
    "\n",
    "        # ==================== 6. Evaluate ==================== #\n",
    "        # Predict on the training data\n",
    "        # The model now outputs predictions for each node, so flattening works correctly.\n",
    "        y_pred_train = model.predict([mlp_train_data, gnn_train_data]).flatten()\n",
    "        r2_train = r2_score(y_train_arr, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_arr, y_pred_train))\n",
    "\n",
    "        # Evaluate on the validation data\n",
    "        y_pred_val = model.predict([mlp_val_data, gnn_val_data]).flatten()\n",
    "        r2_val = r2_score(y_val_arr, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_arr, y_pred_val))\n",
    "\n",
    "        # Evaluate on the test data\n",
    "        y_pred_test = model.predict([mlp_test_data, gnn_test_data]).flatten()\n",
    "        r2_test = r2_score(y_test_arr, y_pred_test)\n",
    "        rmse_test = np.sqrt(mean_squared_error(y_test_arr, y_pred_test))\n",
    "\n",
    "\n",
    "        print(f\"\\n Stacking GNN Ensemble Model Performance:\")\n",
    "        print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "        print(f\"R² Test: {r2_val:.4f} | RMSE Test: {rmse_val:.4f}\")\n",
    "        \n",
    "\n",
    "        # ==================== 7. Permutation Importance and Saving Results ==================== #\n",
    "\n",
    "        # First, calculate feature importance on the test data\n",
    "        # Pass un-batched adjacency matrix for consistency\n",
    "        importance_results = calculate_permutation_importance(\n",
    "            model=model,\n",
    "            mlp_data=mlp_test_scaled, # Pass the un-batched data here\n",
    "            gnn_data=dist_mat_test, # Pass the un-batched adjacency matrix\n",
    "            y_true=y_test_arr,\n",
    "            feature_names=numeric_cols\n",
    "        )\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        output_dir = 'gcn_gat'\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # Save the model in the .keras format\n",
    "        model_path = os.path.join(output_dir, 'gcn_gat.keras')\n",
    "        model.save(model_path)\n",
    "        print(f\"\\nModel saved to {model_path}\")\n",
    "\n",
    "        # Save the feature importance results as a pickled file\n",
    "        importance_path = os.path.join(output_dir, 'feature_importance.pkl')\n",
    "        with open(importance_path, 'wb') as f:\n",
    "            pickle.dump(importance_results, f)\n",
    "        print(f\"Feature importance results saved to {importance_path}\")\n",
    "\n",
    "        # Save the data splits for reproducibility in .npy format\n",
    "        np.save(os.path.join(output_dir, 'mlp_train_data.npy'), mlp_train_scaled)\n",
    "        np.save(os.path.join(output_dir, 'mlp_val_data.npy'), mlp_val_scaled)\n",
    "        np.save(os.path.join(output_dir, 'mlp_test_data.npy'), mlp_test_scaled)\n",
    "        np.save(os.path.join(output_dir, 'y_train_data.npy'), y_train_arr)\n",
    "        np.save(os.path.join(output_dir, 'y_val_data.npy'), y_val_arr)\n",
    "        np.save(os.path.join(output_dir, 'y_test_data.npy'), y_test_arr)\n",
    "        print(\"Training, validation, and test data splits saved to the gnn_gat folder in .npy format.\")\n",
    "\n",
    "        # Garbage collect to free up memory\n",
    "        del model, history\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"\\nAnalysis complete and files have been saved.\")\n",
    "\n",
    "finally:\n",
    "    # Restore stdout to the original value\n",
    "    sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4ce44-cb44-4615-80a3-7ef7e3c46a71",
   "metadata": {},
   "source": [
    "# Graphsage GAT"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48bfba64-79d6-40fb-9c7b-893a6c8763cb",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# Train-test split\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "# We are not using rasters in this GCN-GIN model, but the paths are still\n",
    "# defined for consistency with previous versions.\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(\"Note: Raster data is not used in this Stacking GNN ensemble model.\")\n",
    "\n",
    "# ==================== 3. Create a Custom Data Generator ==================== #\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, mlp_data, gnn_data, y, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # We need to make sure we return an integer number of batches.\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        # FIX: Correctly create a sub-graph adjacency matrix for the current batch\n",
    "        batch_gnn = self.gnn_data[np.ix_(batch_indices, batch_indices)]\n",
    "        batch_y = self.y[batch_indices]\n",
    "        \n",
    "        return (batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #\n",
    "coords_train_val = train_combined[['Long','Lat']].values\n",
    "coords_test = test_orig[['Long','Lat']].values\n",
    "\n",
    "# FIX: Split the training combined data into a training and a validation set\n",
    "mlp_train_val, mlp_test = train_test_split(train_combined, test_size=len(test_orig), random_state=42)\n",
    "y_train_val, y_test = train_test_split(train_combined['RI'], test_size=len(test_orig), random_state=42)\n",
    "mlp_train, mlp_val, y_train, y_val = train_test_split(mlp_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now, re-do the distance matrices and scaling with the new splits\n",
    "coords_train = mlp_train[['Long', 'Lat']].values\n",
    "coords_val = mlp_val[['Long', 'Lat']].values\n",
    "coords_test = test_orig[['Long', 'Lat']].values\n",
    "\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_val = distance_matrix(coords_val, coords_val)\n",
    "gnn_val = np.exp(-dist_mat_val/10)\n",
    "dist_mat_test = distance_matrix(coords_test, coords_test)\n",
    "gnn_test_data = np.exp(-dist_mat_test/10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mlp_train_scaled = scaler.fit_transform(mlp_train[numeric_cols])\n",
    "mlp_val_scaled = scaler.transform(mlp_val[numeric_cols])\n",
    "mlp_test_scaled = scaler.transform(test_orig[numeric_cols])\n",
    "y_train_arr = y_train.values\n",
    "y_val_arr = y_val.values\n",
    "y_test_arr = y_test.values\n",
    "\n",
    "# ==================== 5. Define Stacking GNN Ensemble Model ==================== #\n",
    "\n",
    "class GCNLayer(Layer):\n",
    "    \"\"\"\n",
    "    Custom GCN Layer. Given the pre-computed similarity matrix, this layer\n",
    "    aggregates information from neighboring nodes and transforms it.\n",
    "    \"\"\"\n",
    "    def __init__(self, units, activation=\"relu\", **kwargs):\n",
    "        super(GCNLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        mlp_shape, gnn_shape = input_shape\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(mlp_shape[-1], self.units),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        super(GCNLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mlp_input, gnn_input = inputs\n",
    "        # The gnn_input is treated as the normalized adjacency matrix,\n",
    "        # which performs the aggregation.\n",
    "        aggregated_features = tf.matmul(gnn_input, mlp_input)\n",
    "        # Apply the linear transformation\n",
    "        output = tf.matmul(aggregated_features, self.kernel)\n",
    "        # Apply activation\n",
    "        return self.activation(output)\n",
    "\n",
    "class GATLayer(Layer):\n",
    "    \"\"\"\n",
    "    Custom GAT Layer. This layer computes attention scores for neighboring\n",
    "    nodes and aggregates features based on these scores.\n",
    "    \"\"\"\n",
    "    def __init__(self, units, num_heads=4, activation=\"relu\", **kwargs):\n",
    "        super(GATLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.num_heads = num_heads\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        mlp_shape, gnn_shape = input_shape\n",
    "        self.kernel_f = self.add_weight(\n",
    "            shape=(mlp_shape[-1], self.units * self.num_heads),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.kernel_a_1 = self.add_weight(\n",
    "            shape=(self.units, 1),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.kernel_a_2 = self.add_weight(\n",
    "            shape=(self.units, 1),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        super(GATLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        mlp_input, gnn_input = inputs\n",
    "        \n",
    "        # Linear transformation\n",
    "        features = tf.matmul(mlp_input, self.kernel_f)\n",
    "        \n",
    "        # Split features into attention heads\n",
    "        features_heads = tf.reshape(features, (-1, self.num_heads, self.units))\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        # We need to broadcast the weights for each head\n",
    "        a_1_heads = tf.tile(tf.expand_dims(self.kernel_a_1, axis=0), [self.num_heads, 1, 1])\n",
    "        a_2_heads = tf.tile(tf.expand_dims(self.kernel_a_2, axis=0), [self.num_heads, 1, 1])\n",
    "\n",
    "        # Calculate attention scores for each head\n",
    "        e_input_1 = tf.matmul(features_heads, a_1_heads)\n",
    "        e_input_2 = tf.transpose(tf.matmul(features_heads, a_2_heads), perm=[1, 2, 0])\n",
    "        \n",
    "        e = e_input_1 + e_input_2\n",
    "        e = tf.nn.leaky_relu(e, alpha=0.2)\n",
    "\n",
    "        # Mask attention scores for non-existent edges\n",
    "        mask = -10e9 * (1.0 - gnn_input)\n",
    "        attention_scores = e + mask\n",
    "        \n",
    "        # Softmax normalization\n",
    "        attention = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        \n",
    "        # Aggregate features\n",
    "        aggregated_features = tf.matmul(attention, features_heads)\n",
    "        \n",
    "        # Concatenate heads and apply final activation\n",
    "        output = tf.reshape(aggregated_features, (-1, self.units * self.num_heads))\n",
    "        return self.activation(output)\n",
    "\n",
    "def build_stacking_ensemble_model(mlp_dim):\n",
    "    \"\"\"\n",
    "    Builds a stacking ensemble model with GCN and GAT base learners\n",
    "    and an MLP meta-learner.\n",
    "    \"\"\"\n",
    "    # Define inputs for all branches\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    gnn_input = Input(shape=(None,), name=\"gnn_input\")\n",
    "    \n",
    "    # --- GCN Base Learner Branch ---\n",
    "    gcn_branch = GCNLayer(128, name=\"gcn_layer_1\")([mlp_input, gnn_input])\n",
    "    gcn_branch = Dropout(0.2)(gcn_branch)\n",
    "    gcn_branch = GCNLayer(64, name=\"gcn_layer_2\")([gcn_branch, gnn_input])\n",
    "    gcn_branch = Dropout(0.2)(gcn_branch)\n",
    "    # The output of this branch will be a feature vector for the meta-learner\n",
    "    gcn_output_features = Dense(32, activation=\"relu\", name=\"gcn_features\")(gcn_branch)\n",
    "\n",
    "    # --- GAT Base Learner Branch ---\n",
    "    gat_branch = GATLayer(64, num_heads=4, name=\"gat_layer_1\")([mlp_input, gnn_input])\n",
    "    gat_branch = Dropout(0.2)(gat_branch)\n",
    "    gat_branch = GATLayer(32, num_heads=4, name=\"gat_layer_2\")([gat_branch, gnn_input])\n",
    "    gat_branch = Dropout(0.2)(gat_branch)\n",
    "    # The output of this branch will be a feature vector for the meta-learner\n",
    "    gat_output_features = Dense(16, activation=\"relu\", name=\"gat_features\")(gat_branch)\n",
    "    \n",
    "    # --- MLP Meta-Learner ---\n",
    "    # Concatenate the feature outputs of the base learners\n",
    "    meta_learner_input = Concatenate(name=\"meta_learner_input\")([gcn_output_features, gat_output_features])\n",
    "    \n",
    "    # Define the meta-learner's layers\n",
    "    meta_learner_output = Dense(16, activation=\"relu\", name=\"meta_dense_1\")(meta_learner_input)\n",
    "    meta_learner_output = Dense(8, activation=\"relu\", name=\"meta_dense_2\")(meta_learner_output)\n",
    "    \n",
    "    # Final prediction layer\n",
    "    final_output = Dense(1, activation=\"linear\", name=\"final_output\")(meta_learner_output)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[mlp_input, gnn_input], outputs=final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# ==================== Run the Analysis ==================== #\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing Stacking GNN Ensemble Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "mlp_input_dim = mlp_train_scaled.shape[1]\n",
    "\n",
    "# Build the stacking ensemble model\n",
    "model = build_stacking_ensemble_model(mlp_input_dim)\n",
    "model.summary()\n",
    "\n",
    "# ==================== 6. Create Data Generators ==================== #\n",
    "train_generator = DataGenerator(\n",
    "    mlp_data=mlp_train_scaled, gnn_data=gnn_train, y=y_train_arr,\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = DataGenerator(\n",
    "    mlp_data=mlp_val_scaled, gnn_data=gnn_val, y=y_val_arr,\n",
    "    batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = DataGenerator(\n",
    "    mlp_data=mlp_test_scaled, gnn_data=gnn_test_data, y=y_test_arr,\n",
    "    batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# ==================== 7. Train Model ==================== #\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=val_generator\n",
    ")\n",
    "\n",
    "# ==================== 8. Evaluate ==================== #\n",
    "# Predict on the training data using the generator\n",
    "y_pred_train = model.predict(train_generator).flatten()\n",
    "r2_train = r2_score(y_train_arr[:len(y_pred_train)], y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train_arr[:len(y_pred_train)], y_pred_train))\n",
    "\n",
    "# Evaluate on the validation data\n",
    "y_pred_val = model.predict(val_generator).flatten()\n",
    "r2_val = r2_score(y_val_arr[:len(y_pred_val)], y_pred_val)\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val_arr[:len(y_pred_val)], y_pred_val))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n Stacking GNN Ensemble Model Performance:\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Test: {r2_val:.4f} | RMSE Test: {rmse_val:.4f}\")\n",
    "\n",
    "# Garbage collect to free up memory\n",
    "del model, history, train_generator, test_generator, val_generator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da99fc5d-59ab-4403-b360-5e9401ca1a43",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, Layer, Average, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import sys\n",
    "from io import StringIO\n",
    "import pickle # Import the pickle library for saving objects\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# Train-test split\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "# We are not using rasters in this GNN ensemble model, but the paths are still\n",
    "# defined for consistency with previous versions.\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(\"Note: Raster data is not used in this Stacking GNN ensemble model.\")\n",
    "\n",
    "# ==================== 3. Create a Custom Data Generator ==================== #\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, mlp_data, gnn_data, y, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # We need to make sure we return an integer number of batches.\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        # Correctly create a sub-graph adjacency matrix for the current batch\n",
    "        batch_gnn = self.gnn_data[np.ix_(batch_indices, batch_indices)]\n",
    "        batch_y = self.y[batch_indices]\n",
    "        \n",
    "        return (batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #\n",
    "# The data splitting logic was inconsistent. I've corrected it to ensure\n",
    "# that the test data is handled separately and consistently.\n",
    "mlp_train_val, mlp_test_df = train_test_split(train_combined, test_size=len(test_orig), random_state=42)\n",
    "y_train_val, y_test = train_test_split(train_combined['RI'], test_size=len(test_orig), random_state=42)\n",
    "mlp_train, mlp_val, y_train, y_val = train_test_split(mlp_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now, re-do the distance matrices and scaling with the new splits\n",
    "coords_train = mlp_train[['Long', 'Lat']].values\n",
    "coords_val = mlp_val[['Long', 'Lat']].values\n",
    "coords_test = mlp_test_df[['Long', 'Lat']].values\n",
    "\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_val = distance_matrix(coords_val, coords_val)\n",
    "gnn_val = np.exp(-dist_mat_val/10)\n",
    "dist_mat_test = distance_matrix(coords_test, coords_test)\n",
    "gnn_test_data = np.exp(-dist_mat_test/10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mlp_train_scaled = scaler.fit_transform(mlp_train[numeric_cols])\n",
    "mlp_val_scaled = scaler.transform(mlp_val[numeric_cols])\n",
    "mlp_test_scaled = scaler.transform(mlp_test_df[numeric_cols])\n",
    "y_train_arr = y_train.values\n",
    "y_val_arr = y_val.values\n",
    "y_test_arr = y_test.values\n",
    "\n",
    "# ==================== 5. Define Stacking GNN Ensemble Model ==================== #\n",
    "\n",
    "class GCNLayer(Layer):\n",
    "    \"\"\"\n",
    "    Custom GCN Layer. Given the pre-computed similarity matrix, this layer\n",
    "    aggregates information from neighboring nodes and transforms it.\n",
    "    \"\"\"\n",
    "    def __init__(self, units, activation=\"relu\", **kwargs):\n",
    "        super(GCNLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        mlp_shape, gnn_shape = input_shape\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(mlp_shape[-1], self.units),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        super(GCNLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mlp_input, gnn_input = inputs\n",
    "        # The gnn_input is treated as the normalized adjacency matrix,\n",
    "        # which performs the aggregation.\n",
    "        aggregated_features = tf.matmul(gnn_input, mlp_input)\n",
    "        # Apply the linear transformation\n",
    "        output = tf.matmul(aggregated_features, self.kernel)\n",
    "        # Apply activation\n",
    "        return self.activation(output)\n",
    "\n",
    "class GATLayer(Layer):\n",
    "    \"\"\"\n",
    "    FIXED GAT Layer. This version corrects the dimension issues in the call\n",
    "    method by using a more explicit and robust implementation of the\n",
    "    attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, units, num_heads=4, activation=\"relu\", **kwargs):\n",
    "        super(GATLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.num_heads = num_heads\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        mlp_shape, gnn_shape = input_shape\n",
    "        # Kernel for linear transformation of features\n",
    "        self.kernel_f = self.add_weight(\n",
    "            shape=(mlp_shape[-1], self.units * self.num_heads),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        # Attention kernels for pairwise scoring\n",
    "        self.att_kernel_1 = self.add_weight(\n",
    "            shape=(self.units, 1),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.att_kernel_2 = self.add_weight(\n",
    "            shape=(self.units, 1),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        super(GATLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        mlp_input, gnn_input = inputs\n",
    "        \n",
    "        # Linear transformation\n",
    "        features = tf.matmul(mlp_input, self.kernel_f)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        features_heads = tf.reshape(features, (-1, self.num_heads, self.units))\n",
    "        \n",
    "        # Calculate attention scores for each head\n",
    "        # This is a more direct and less error-prone way to get the pairwise scores\n",
    "        # We need to project each node's features with the attention kernels\n",
    "        att_scores_i = tf.einsum('bhu,uo->bho', features_heads, self.att_kernel_1)\n",
    "        att_scores_j = tf.einsum('bhu,uo->bho', features_heads, self.att_kernel_2)\n",
    "        \n",
    "        # Expand and broadcast to get a pairwise score matrix for each head\n",
    "        e = att_scores_i + tf.transpose(att_scores_j, perm=[0, 2, 1])\n",
    "        e = tf.nn.leaky_relu(e, alpha=0.2)\n",
    "\n",
    "        # Mask non-existent edges\n",
    "        mask = -1e9 * (1.0 - gnn_input)\n",
    "        mask = tf.expand_dims(mask, axis=1) # Shape: (batch, 1, batch, batch)\n",
    "        \n",
    "        attention_scores = e + mask\n",
    "        \n",
    "        # Softmax normalization\n",
    "        attention = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        \n",
    "        # Aggregate features\n",
    "        # attention shape: (batch, heads, batch, batch)\n",
    "        # features_heads shape: (batch, heads, units)\n",
    "        # We need to matmul the attention matrix with the features.\n",
    "        attention_T = tf.transpose(attention, perm=[0, 1, 3, 2])\n",
    "        x_heads_exp = tf.expand_dims(features_heads, axis=-1)\n",
    "        aggregated_heads = tf.matmul(attention_T, x_heads_exp)\n",
    "        \n",
    "        aggregated_features = tf.transpose(tf.squeeze(aggregated_heads, axis=-1), perm=[0, 2, 1])\n",
    "\n",
    "        # Concatenate heads and apply final activation\n",
    "        output = tf.reshape(aggregated_features, (-1, self.units * self.num_heads))\n",
    "        return self.activation(output)\n",
    "\n",
    "def build_stacking_ensemble_model(mlp_dim):\n",
    "    \"\"\"\n",
    "    Builds a stacking ensemble model with GCN and GAT base learners\n",
    "    and an MLP meta-learner.\n",
    "    \"\"\"\n",
    "    # Define inputs for all branches\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    # Corrected gnn_input shape for adjacency matrix\n",
    "    gnn_input = Input(shape=(None, None), name=\"gnn_input\")\n",
    "    \n",
    "    # --- GCN Base Learner Branch ---\n",
    "    gcn_branch = GCNLayer(128, name=\"gcn_layer_1\")([mlp_input, gnn_input])\n",
    "    gcn_branch = Dropout(0.2)(gcn_branch)\n",
    "    gcn_branch = GCNLayer(64, name=\"gcn_layer_2\")([gcn_branch, gnn_input])\n",
    "    gcn_branch = Dropout(0.2)(gcn_branch)\n",
    "    # The output of this branch will be a feature vector for the meta-learner\n",
    "    gcn_output_features = Dense(32, activation=\"relu\", name=\"gcn_features\")(gcn_branch)\n",
    "\n",
    "    # --- GAT Base Learner Branch ---\n",
    "    gat_branch = GATLayer(64, num_heads=4, name=\"gat_layer_1\")([mlp_input, gnn_input])\n",
    "    gat_branch = Dropout(0.2)(gat_branch)\n",
    "    gat_branch = GATLayer(32, num_heads=4, name=\"gat_layer_2\")([gat_branch, gnn_input])\n",
    "    gat_branch = Dropout(0.2)(gat_branch)\n",
    "    # The output of this branch will be a feature vector for the meta-learner\n",
    "    gat_output_features = Dense(16, activation=\"relu\", name=\"gat_features\")(gat_branch)\n",
    "    \n",
    "    # --- MLP Meta-Learner ---\n",
    "    # Concatenate the feature outputs of the base learners\n",
    "    meta_learner_input = Concatenate(name=\"meta_learner_input\")([gcn_output_features, gat_output_features])\n",
    "    \n",
    "    # Define the meta-learner's layers\n",
    "    meta_learner_output = Dense(16, activation=\"relu\", name=\"meta_dense_1\")(meta_learner_input)\n",
    "    meta_learner_output = Dense(8, activation=\"relu\", name=\"meta_dense_2\")(meta_learner_output)\n",
    "    \n",
    "    # Final prediction layer\n",
    "    final_output = Dense(1, activation=\"linear\", name=\"final_output\")(meta_learner_output)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[mlp_input, gnn_input], outputs=final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, generator):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a given data generator and returns R² and RMSE.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(generator).flatten()\n",
    "    y_true = generator.y[:len(y_pred)]\n",
    "    \n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    return r2, rmse, y_pred\n",
    "\n",
    "def calculate_permutation_importance(model, mlp_data, gnn_data, y_true):\n",
    "    \"\"\"\n",
    "    Calculates permutation feature importance for the MLP and GNN branches.\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting Permutation Feature Importance Analysis...\")\n",
    "    # Get baseline R² on the unshuffled data\n",
    "    y_pred = model.predict((mlp_data, gnn_data)).flatten()\n",
    "    baseline_r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"Baseline R² on test set: {baseline_r2:.4f}\")\n",
    "\n",
    "    importance = {}\n",
    "    \n",
    "    # Permute MLP input\n",
    "    shuffled_mlp_data = mlp_data.copy()\n",
    "    np.random.shuffle(shuffled_mlp_data)\n",
    "    y_pred_shuffled = model.predict((shuffled_mlp_data, gnn_data)).flatten()\n",
    "    shuffled_r2 = r2_score(y_true, y_pred_shuffled)\n",
    "    importance['MLP'] = baseline_r2 - shuffled_r2\n",
    "\n",
    "    # Permute GNN input\n",
    "    shuffled_gnn_data = gnn_data.copy()\n",
    "    np.random.shuffle(shuffled_gnn_data)\n",
    "    y_pred_shuffled = model.predict((mlp_data, shuffled_gnn_data)).flatten()\n",
    "    shuffled_r2 = r2_score(y_true, y_pred_shuffled)\n",
    "    importance['GNN'] = baseline_r2 - shuffled_r2\n",
    "\n",
    "    return importance\n",
    "\n",
    "# ==================== Run the Analysis ==================== #\n",
    "# Redirect output to a string for later saving\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = captured_output = StringIO()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing Stacking GNN Ensemble Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "mlp_input_dim = mlp_train_scaled.shape[1]\n",
    "\n",
    "# Build the stacking ensemble model\n",
    "model = build_stacking_ensemble_model(mlp_input_dim)\n",
    "model.summary()\n",
    "\n",
    "# ==================== 6. Create Data Generators ==================== #\n",
    "train_generator = DataGenerator(\n",
    "    mlp_data=mlp_train_scaled, gnn_data=gnn_train, y=y_train_arr,\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = DataGenerator(\n",
    "    mlp_data=mlp_val_scaled, gnn_data=gnn_val, y=y_val_arr,\n",
    "    batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = DataGenerator(\n",
    "    mlp_data=mlp_test_scaled, gnn_data=gnn_test_data, y=y_test_arr,\n",
    "    batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# ==================== 7. Train Model ==================== #\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=2,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=val_generator\n",
    ")\n",
    "\n",
    "# ==================== 8. Evaluate & Perform Feature Importance ==================== #\n",
    "# Predict and evaluate on the training data\n",
    "r2_train, rmse_train, y_pred_train = evaluate_model(model, train_generator)\n",
    "\n",
    "# Predict and evaluate on the validation data\n",
    "r2_val, rmse_val, y_pred_val = evaluate_model(model, val_generator)\n",
    "\n",
    "# Predict and evaluate on the test data\n",
    "r2_test, rmse_test, y_pred_test = evaluate_model(model, test_generator)\n",
    "\n",
    "\n",
    "print(f\"\\n Stacking GNN Ensemble Model Performance:\")\n",
    "print(f\"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R² Validation: {r2_val:.4f} | RMSE Validation: {rmse_val:.4f}\")\n",
    "print(f\"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# Calculate and print feature importance\n",
    "feature_importance = calculate_permutation_importance(model, mlp_test_scaled, gnn_test_data, y_test_arr)\n",
    "print(\"\\n--- Feature Importance (Permutation) ---\")\n",
    "sorted_importance = sorted(feature_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, score in sorted_importance:\n",
    "    print(f\"{feature}: {score:.4f}\")\n",
    "\n",
    "# ==================== 9. Save all info to a folder ==================== #\n",
    "# Restore standard output\n",
    "sys.stdout = old_stdout\n",
    "printed_output = captured_output.getvalue()\n",
    "\n",
    "output_folder = \"graphsage_gat\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(f\"\\nCreating folder: '{output_folder}' and saving results...\")\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(output_folder, \"stacking_gnn_model.keras\")\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save the predictions and true labels\n",
    "np.save(os.path.join(output_folder, \"y_train.npy\"), y_train_arr)\n",
    "np.save(os.path.join(output_folder, \"y_val.npy\"), y_val_arr)\n",
    "np.save(os.path.join(output_folder, \"y_test.npy\"), y_test_arr)\n",
    "np.save(os.path.join(output_folder, \"y_pred_train.npy\"), y_pred_train)\n",
    "np.save(os.path.join(output_folder, \"y_pred_val.npy\"), y_pred_val)\n",
    "np.save(os.path.join(output_folder, \"y_pred_test.npy\"), y_pred_test)\n",
    "print(f\"Predictions and true labels saved as .npy files.\")\n",
    "\n",
    "# Save the printed output to a text file\n",
    "output_path = os.path.join(output_folder, \"analysis_output.txt\")\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(printed_output)\n",
    "print(f\"Analysis results saved to: {output_path}\")\n",
    "\n",
    "# Save the feature importance dictionary as a .pkl file\n",
    "importance_path = os.path.join(output_folder, \"feature_importance.pkl\")\n",
    "with open(importance_path, 'wb') as f:\n",
    "    pickle.dump(feature_importance, f)\n",
    "print(f\"Feature importance results saved to: {importance_path}\")\n",
    "\n",
    "print(\"\\nAll information successfully saved.\")\n",
    "\n",
    "# Garbage collect to free up memory now that everything is saved\n",
    "del model, history, train_generator, test_generator, val_generator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c416c28-c10f-4191-bd0d-99f7c65de0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
