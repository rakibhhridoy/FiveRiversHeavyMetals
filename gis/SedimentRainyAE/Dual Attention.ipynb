{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44ce2c-d8a5-459b-a521-9a3d20d0cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Concatenate,\n",
    "    Dropout,\n",
    "    Layer,\n",
    "    Lambda,\n",
    "    GlobalAveragePooling2D,\n",
    "    Reshape,\n",
    "    Multiply\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import sys\n",
    "from io import StringIO # To capture print output\n",
    "import pickle # For saving dictionaries and other objects\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "# NOTE: The data loading logic remains the same.\n",
    "# Replace with your actual data paths if needed\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# Train-test split\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(f\"Using {len(raster_paths)} raster layers for CNN input.\")\n",
    "for r in raster_paths:\n",
    "    print(\"  -\", os.path.basename(r))\n",
    "\n",
    "# ==================== 3. Create a Custom Data Generator ==================== #\n",
    "def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):\n",
    "    \"\"\"\n",
    "    Extracts a batch of patches from rasters for a given set of coordinates.\n",
    "    This function is optimized to be called by the data generator for each batch.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    # Loop through each coordinate pair in the batch\n",
    "    for lon, lat in coords:\n",
    "        channels = []\n",
    "        # Loop through each raster file to get a single patch for each raster\n",
    "        for rfile in raster_files:\n",
    "            with rasterio.open(rfile) as src:\n",
    "                try:\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)\n",
    "                    arr = src.read(1, window=win, boundless=True, fill_value=0)\n",
    "                    arr = arr.astype(np.float32)\n",
    "\n",
    "                    if np.nanmax(arr) != 0:\n",
    "                        arr /= np.nanmax(arr)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {rfile} for coordinates ({lon}, {lat}): {e}\")\n",
    "                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)\n",
    "            channels.append(arr)\n",
    "        patches.append(np.stack(channels, axis=-1))\n",
    "    \n",
    "    return np.array(patches)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.coords = coords\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.raster_paths = raster_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.buffer_meters = buffer_meters\n",
    "\n",
    "        # Pre-calculate patch size from the first raster\n",
    "        with rasterio.open(raster_paths[0]) as src:\n",
    "            res_x, res_y = src.res\n",
    "            self.buffer_pixels_x = int(self.buffer_meters / res_x)\n",
    "            self.buffer_pixels_y = int(self.buffer_meters / res_y)\n",
    "            self.patch_width = 2 * self.buffer_pixels_x\n",
    "            self.patch_height = 2 * self.buffer_pixels_y\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        \n",
    "        # Slice the GNN adjacency matrix for the current batch\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "\n",
    "        batch_y = self.y[batch_indices]\n",
    "\n",
    "        # Extract CNN patches for the current batch\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            self.raster_paths,\n",
    "            self.buffer_pixels_x,\n",
    "            self.buffer_pixels_y,\n",
    "            self.patch_width,\n",
    "            self.patch_height\n",
    "        )\n",
    "\n",
    "        # Return a tuple of inputs and the target, which Keras expects\n",
    "        return (batch_cnn, batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "\n",
    "# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #\n",
    "coords_train = train_combined[['Long','Lat']].values\n",
    "coords_test = test_orig[['Long','Lat']].values\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_test_train = distance_matrix(coords_test, coords_train)\n",
    "gnn_test = np.exp(-dist_mat_test_train/10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mlp_train = scaler.fit_transform(train_combined[numeric_cols])\n",
    "mlp_test = scaler.transform(test_orig[numeric_cols])\n",
    "y_train = train_combined['RI'].values\n",
    "y_test = test_orig['RI'].values\n",
    "\n",
    "# ==================== 5. Define Custom Attention Layers ==================== #\n",
    "\n",
    "class SpatialAttention(Layer):\n",
    "    \"\"\"\n",
    "    A custom layer to apply spatial attention to a feature map.\n",
    "    It generates a spatial attention map and multiplies it with the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SpatialAttention, self).__init__(**kwargs)\n",
    "        self.conv1 = Conv2D(1, (1, 1), activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Squeeze the channels and generate a 2D attention map\n",
    "        attention_map = self.conv1(inputs)\n",
    "        # Multiply the input feature map by the attention map\n",
    "        return Multiply()([inputs, attention_map])\n",
    "\n",
    "class FeatureAttention(Layer):\n",
    "    \"\"\"\n",
    "    A custom layer to apply feature-wise attention.\n",
    "    It learns a weight for each feature channel and multiplies it with the input.\n",
    "    Inspired by Squeeze-and-Excitation networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction_ratio=16, **kwargs):\n",
    "        super(FeatureAttention, self).__init__(**kwargs)\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(FeatureAttention, self).build(input_shape)\n",
    "        if len(input_shape) == 4: # CNN output\n",
    "            self.avg_pool = GlobalAveragePooling2D()\n",
    "            self.dense1 = Dense(units=input_shape[-1] // self.reduction_ratio, activation='relu')\n",
    "            self.dense2 = Dense(units=input_shape[-1], activation='sigmoid')\n",
    "            self.reshape_output = Reshape((1, 1, input_shape[-1]))\n",
    "        else: # MLP or GNN output\n",
    "            self.dense1 = Dense(units=input_shape[-1] // self.reduction_ratio, activation='relu')\n",
    "            self.dense2 = Dense(units=input_shape[-1], activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if len(inputs.shape) == 4: # CNN branch\n",
    "            x = self.avg_pool(inputs)\n",
    "            x = self.dense1(x)\n",
    "            x = self.dense2(x)\n",
    "            x = self.reshape_output(x)\n",
    "        else: # MLP or GNN branch\n",
    "            x = self.dense1(inputs)\n",
    "            x = self.dense2(x)\n",
    "            \n",
    "        return Multiply()([inputs, x])\n",
    "\n",
    "# ==================== 6. Define the Dual Attention Model ==================== #\n",
    "def build_dual_attention_model(patch_shape, gnn_dim, mlp_dim):\n",
    "    # Inputs for all branches\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    \n",
    "    # --- CNN Branch with Spatial and Feature Attention ---\n",
    "    cnn_branch = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_branch = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(cnn_branch)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    \n",
    "    # Spatial Attention\n",
    "    cnn_spatial_attn = SpatialAttention()(cnn_branch)\n",
    "    \n",
    "    # Feature Attention\n",
    "    cnn_feature_attn = FeatureAttention()(cnn_spatial_attn)\n",
    "    \n",
    "    # Flatten and get embedding\n",
    "    cnn_embedding = Flatten()(cnn_feature_attn)\n",
    "    cnn_embedding = Dense(128, activation=\"relu\", name=\"cnn_embedding\")(cnn_embedding)\n",
    "\n",
    "    # --- MLP Branch with Embedding ---\n",
    "    mlp_embedding = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(32, activation=\"relu\", name=\"mlp_embedding\")(mlp_embedding)\n",
    "\n",
    "    # --- GNN Branch with Feature Attention and Embedding ---\n",
    "    gnn_branch = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    \n",
    "    # Feature Attention\n",
    "    gnn_feature_attn = FeatureAttention()(gnn_branch)\n",
    "    gnn_embedding = Dense(32, activation=\"relu\", name=\"gnn_embedding\")(gnn_feature_attn)\n",
    "\n",
    "    # --- Attention Fusion ---\n",
    "    # Concatenate all embeddings\n",
    "    combined_embedding = Concatenate(name=\"combined_embedding\")([cnn_embedding, mlp_embedding, gnn_embedding])\n",
    "    \n",
    "    # Final dense layers for prediction\n",
    "    f = Dense(128, activation=\"relu\")(combined_embedding)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, coords_test, mlp_test, gnn_test_matrix, y_test, raster_paths, buffer_meters, batch_size=4, return_preds=False):\n",
    "    num_samples = len(y_test)\n",
    "    y_pred_list = []\n",
    "    \n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, res_y = src.res\n",
    "        buffer_pixels_x = int(buffer_meters / res_x)\n",
    "        buffer_pixels_y = int(buffer_meters / res_y)\n",
    "        patch_width = 2 * buffer_pixels_x\n",
    "        patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_coords = coords_test[i:i+batch_size]\n",
    "        batch_mlp = mlp_test[i:i+batch_size]\n",
    "        \n",
    "        batch_gnn = gnn_test_matrix[i:i+batch_size, :]\n",
    "        batch_y = y_test[i:i+batch_size]\n",
    "\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            raster_paths,\n",
    "            buffer_pixels_x,\n",
    "            buffer_pixels_y,\n",
    "            patch_width,\n",
    "            patch_height\n",
    "        )\n",
    "        \n",
    "        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn)).flatten())\n",
    "        \n",
    "    y_pred = np.concatenate(y_pred_list)\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return r2, rmse\n",
    "\n",
    "# ==================== Run the Analysis ==================== #\n",
    "# Capture all print statements to a string\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = captured_output = StringIO()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing for BUFFER_METERS = {BUFFER_METERS}m\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "\n",
    "# Calculate CNN patch shape based on the current buffer size\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    cnn_patch_shape = (patch_width, patch_width, len(raster_paths))\n",
    "\n",
    "model = build_dual_attention_model(cnn_patch_shape, gnn_input_dim, mlp_train.shape[1])\n",
    "model.summary(print_fn=lambda x: captured_output.write(x + '\\n')) # Capture model summary\n",
    "\n",
    "# ==================== 7. Create Data Generators ==================== #\n",
    "train_generator = DataGenerator(\n",
    "    coords=coords_train,\n",
    "    mlp_data=mlp_train,\n",
    "    gnn_data=gnn_train,\n",
    "    y=y_train,\n",
    "    raster_paths=raster_paths,\n",
    "    buffer_meters=BUFFER_METERS,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# ==================== 8. Train Model ==================== #\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=train_generator\n",
    ")\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# ==================== 9. Evaluate ==================== #\n",
    "y_pred_train = model.predict(train_generator).flatten()\n",
    "r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))\n",
    "\n",
    "# Get test predictions for saving as .npy\n",
    "y_pred_test = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, return_preds=True)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"\\n Dual Attention Model Performance ({BUFFER_METERS}m):\")\n",
    "print(f\"R\u00b2 Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}\")\n",
    "print(f\"R\u00b2 Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}\")\n",
    "\n",
    "# ==================== 10. Feature Importance Analysis ==================== #\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(f\"Feature Importance Analysis for {BUFFER_METERS}m\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# --- 10.1 Combined Feature Importance (by Model Branch) ---\n",
    "y_pred_baseline = y_pred_test\n",
    "baseline_r2 = r2_test\n",
    "print(f\"\\nBaseline Performance on Test Set: R\u00b2 = {baseline_r2:.4f}\")\n",
    "\n",
    "# Ablate CNN branch\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    buffer_pixels_y = int(BUFFER_METERS / res_y)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "cnn_test_ablated = np.zeros_like(extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "))\n",
    "y_pred_cnn_ablated = model.predict((cnn_test_ablated, mlp_test, gnn_test)).flatten()\n",
    "r2_cnn_ablated = r2_score(y_test, y_pred_cnn_ablated)\n",
    "importance_cnn = baseline_r2 - r2_cnn_ablated\n",
    "\n",
    "# Ablate MLP branch\n",
    "mlp_test_ablated = np.zeros_like(mlp_test)\n",
    "y_pred_mlp_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test_ablated, gnn_test)).flatten()\n",
    "r2_mlp_ablated = r2_score(y_test, y_pred_mlp_ablated)\n",
    "importance_mlp = baseline_r2 - r2_mlp_ablated\n",
    "\n",
    "# Ablate GNN branch\n",
    "gnn_test_ablated = np.zeros_like(gnn_test)\n",
    "y_pred_gnn_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test, gnn_test_ablated)).flatten()\n",
    "r2_gnn_ablated = r2_score(y_test, y_pred_gnn_ablated)\n",
    "importance_gnn = baseline_r2 - r2_gnn_ablated\n",
    "\n",
    "print(\"\\n--- Combined Feature Importance (by Model Branch) ---\")\n",
    "print(f\"CNN Branch Importance (R\u00b2 drop): {importance_cnn:.4f}\")\n",
    "print(f\"MLP Branch Importance (R\u00b2 drop): {importance_mlp:.4f}\")\n",
    "print(f\"GNN Branch Importance (R\u00b2 drop): {importance_gnn:.4f}\")\n",
    "\n",
    "# --- 10.2 MLP Feature Importance (Permutation-based) ---\n",
    "mlp_feature_importance = {}\n",
    "for i, feature_name in enumerate(numeric_cols):\n",
    "    mlp_test_shuffled = np.copy(mlp_test)\n",
    "    np.random.shuffle(mlp_test_shuffled[:, i])\n",
    "    \n",
    "    y_pred_shuffled = model.predict((extract_patch_for_generator(\n",
    "        coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "    ), mlp_test_shuffled, gnn_test)).flatten()\n",
    "    r2_shuffled = r2_score(y_test, y_pred_shuffled)\n",
    "    \n",
    "    importance = baseline_r2 - r2_shuffled\n",
    "    mlp_feature_importance[feature_name] = importance\n",
    "\n",
    "print(\"\\n--- MLP Feature Importance (Permutation-based) ---\")\n",
    "sorted_importance = sorted(mlp_feature_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, importance in sorted_importance:\n",
    "    print(f\"{feature:<20}: {importance:.4f}\")\n",
    "    \n",
    "# Garbage collect to free up memory\n",
    "del history, train_generator\n",
    "gc.collect()\n",
    "\n",
    "# ==================== 11. Save all info to a folder ==================== #\n",
    "# Restore standard output\n",
    "sys.stdout = old_stdout\n",
    "printed_output = captured_output.getvalue()\n",
    "\n",
    "output_folder = \"dual_attention_analysis\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(f\"\\nCreating folder: '{output_folder}' and saving results...\")\n",
    "\n",
    "# Save the trained model in the .keras format\n",
    "model_path = os.path.join(output_folder, \"dual_attention.keras\")\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save the MLP feature importance to a .pkl file\n",
    "mlp_importance_path = os.path.join(output_folder, \"mlp_feature_importance.pkl\")\n",
    "with open(mlp_importance_path, 'wb') as f:\n",
    "    pickle.dump(mlp_feature_importance, f)\n",
    "print(f\"MLP feature importance saved to: {mlp_importance_path}\")\n",
    "\n",
    "# Save all relevant data to .npy files\n",
    "np.save(os.path.join(output_folder, \"coords_train.npy\"), coords_train)\n",
    "np.save(os.path.join(output_folder, \"coords_test.npy\"), coords_test)\n",
    "np.save(os.path.join(output_folder, \"mlp_train.npy\"), mlp_train)\n",
    "np.save(os.path.join(output_folder, \"mlp_test.npy\"), mlp_test)\n",
    "np.save(os.path.join(output_folder, \"gnn_train.npy\"), gnn_train)\n",
    "np.save(os.path.join(output_folder, \"gnn_test.npy\"), gnn_test)\n",
    "np.save(os.path.join(output_folder, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(output_folder, \"y_test.npy\"), y_test)\n",
    "np.save(os.path.join(output_folder, \"y_pred_train.npy\"), y_pred_train)\n",
    "np.save(os.path.join(output_folder, \"y_pred_test.npy\"), y_pred_test)\n",
    "print(f\"Coordinates, scaled data, GNN matrices, and labels/predictions saved as .npy files.\")\n",
    "\n",
    "# Save the printed output to a text file\n",
    "output_path = os.path.join(output_folder, \"analysis_output.txt\")\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(printed_output)\n",
    "print(f\"Analysis results saved to: {output_path}\")\n",
    "\n",
    "print(\"All information successfully saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d0614-9922-4848-b325-4a58ad90ebf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaEarth Integration Enabled\n",
    "\n",
    "This notebook has been enhanced with AlphaEarth satellite embeddings.\n",
    "\n",
    "## Integration Options:\n",
    "- **Option A**: Replace indices with AlphaEarth (64 bands)\n",
    "- **Option B**: Add AlphaEarth to features (RECOMMENDED)\n",
    "- **Option C**: PCA-reduced AlphaEarth (20 components)\n",
    "- **Option D**: MLP enhancement only\n",
    "\n",
    "Expected improvement: +0.5% to +0.8% in R\u00b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ALPHAEARTH CONFIGURATION ====================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Select which AlphaEarth option to use\n",
    "ALPHA_EARTH_OPTION = 'B'  # Options: A, B (recommended), C, D\n",
    "USE_ALPHA_EARTH = True\n",
    "\n",
    "# Paths to AlphaEarth data files (created by 00_AlphaEarth_Data_Preparation.ipynb)\n",
    "option_file = f'Option_{ALPHA_EARTH_OPTION}_RainyAE.csv'  # or WinterAE\n",
    "\n",
    "# Load AlphaEarth data\n",
    "if os.path.exists(option_file):\n",
    "    ae_data = pd.read_csv(option_file)\n",
    "    print(f'Loaded AlphaEarth Option {ALPHA_EARTH_OPTION}')\n",
    "    print(f'Shape: {ae_data.shape}')\n",
    "else:\n",
    "    print(f'WARNING: {option_file} not found')\n",
    "    print('Please run 00_AlphaEarth_Data_Preparation.ipynb first')\n",
    "    USE_ALPHA_EARTH = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac1c67-dc8d-45dc-a56f-cd902e7238dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c59d7fd-d9d9-4495-8552-7edd5faf122d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4444d9-7637-43c0-97b3-2aff676b1418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 26 raster layers for CNN input.\n",
      "  - bui.tif\n",
      "  - ndsi.tif\n",
      "  - savi.tif\n",
      "  - ndbsi.tif\n",
      "  - ui.tif\n",
      "  - ndwi.tif\n",
      "  - ndbi.tif\n",
      "  - awei.tif\n",
      "  - evi.tif\n",
      "  - mndwi.tif\n",
      "  - ndvi.tif\n",
      "  - LULC2020.tif\n",
      "  - LULC2021.tif\n",
      "  - LULC2022.tif\n",
      "  - LULC2019.tif\n",
      "  - LULC2018.tif\n",
      "  - LULC2017.tif\n",
      "  - Pb_R.tif\n",
      "  - ClayR.tif\n",
      "  - SandR.tif\n",
      "  - CdR.tif\n",
      "  - CrR.tif\n",
      "  - AsR.tif\n",
      "  - SiltR.tif\n",
      "  - CuR.tif\n",
      "  - NiR.tif\n",
      "================================================================================\n",
      "Starting 5-Fold Cross-Validation...\n",
      "================================================================================\n",
      "Models will be saved in: 'models/dual_attention'\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Starting model training...\n",
      "Epoch 1/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 210ms/step - loss: 36202.1406 - val_loss: 46508.8828\n",
      "Epoch 2/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 186ms/step - loss: 23518.3633 - val_loss: 60823.8008\n",
      "Epoch 3/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - loss: 11568.5938 - val_loss: 9155.0801\n",
      "Epoch 4/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 185ms/step - loss: 7565.7427 - val_loss: 17295.7754\n",
      "Epoch 5/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 182ms/step - loss: 8981.6650 - val_loss: 14006.3750\n",
      "Epoch 6/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - loss: 8333.1572 - val_loss: 7950.1182\n",
      "Epoch 7/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 194ms/step - loss: 6209.1836 - val_loss: 5908.3926\n",
      "Epoch 8/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 193ms/step - loss: 6159.8735 - val_loss: 6432.9565\n",
      "Epoch 9/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - loss: 5414.6494 - val_loss: 8437.5615\n",
      "Epoch 10/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - loss: 4621.5327 - val_loss: 7537.9463\n",
      "Epoch 11/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - loss: 5468.6807 - val_loss: 4842.8608\n",
      "Epoch 12/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 173ms/step - loss: 4060.1426 - val_loss: 4015.5903\n",
      "Epoch 13/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - loss: 4960.4072 - val_loss: 3094.3318\n",
      "Epoch 14/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 181ms/step - loss: 3637.1611 - val_loss: 3042.0374\n",
      "Epoch 15/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 170ms/step - loss: 2581.6074 - val_loss: 1610.7191\n",
      "Epoch 16/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - loss: 1922.4883 - val_loss: 837.7710\n",
      "Epoch 17/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - loss: 2804.8567 - val_loss: 432.5222\n",
      "Epoch 18/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - loss: 1022.5367 - val_loss: 322.0358\n",
      "Epoch 19/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - loss: 1061.8925 - val_loss: 294.3091\n",
      "Epoch 20/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 186ms/step - loss: 1012.8804 - val_loss: 664.6984\n",
      "Epoch 21/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - loss: 1134.0785 - val_loss: 474.7950\n",
      "Epoch 22/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 173ms/step - loss: 796.8330 - val_loss: 286.6911\n",
      "Epoch 23/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 173ms/step - loss: 1092.4050 - val_loss: 273.4096\n",
      "Epoch 24/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 179ms/step - loss: 1119.5767 - val_loss: 1283.4244\n",
      "Epoch 25/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 189ms/step - loss: 1119.0471 - val_loss: 460.8512\n",
      "Epoch 26/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - loss: 582.4391 - val_loss: 239.5727\n",
      "Epoch 27/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 179ms/step - loss: 544.6665 - val_loss: 193.5216\n",
      "Epoch 28/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - loss: 920.4571 - val_loss: 178.6676\n",
      "Epoch 29/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - loss: 1282.4772 - val_loss: 264.9625\n",
      "Epoch 30/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - loss: 756.7690 - val_loss: 387.9615\n",
      "Epoch 31/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - loss: 693.1371 - val_loss: 211.5019\n",
      "Epoch 32/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - loss: 428.6407 - val_loss: 360.0302\n",
      "Epoch 33/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - loss: 408.0171 - val_loss: 237.4766\n",
      "Epoch 34/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - loss: 1047.0630 - val_loss: 139.3112\n",
      "Epoch 35/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 182ms/step - loss: 750.6222 - val_loss: 178.2664\n",
      "Epoch 36/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - loss: 601.4440 - val_loss: 218.6202\n",
      "Epoch 37/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - loss: 531.3441 - val_loss: 445.2017\n",
      "Epoch 38/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - loss: 574.1152 - val_loss: 348.7296\n",
      "Epoch 39/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - loss: 386.7404 - val_loss: 274.9479\n",
      "Epoch 40/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 183ms/step - loss: 596.5445 - val_loss: 113.4199\n",
      "Epoch 41/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - loss: 808.5905 - val_loss: 561.0408\n",
      "Epoch 42/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - loss: 453.0096 - val_loss: 720.2901\n",
      "Epoch 43/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - loss: 617.8968 - val_loss: 153.1215\n",
      "Epoch 44/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - loss: 674.3435 - val_loss: 181.7621\n",
      "Epoch 45/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - loss: 540.5887 - val_loss: 590.1814\n",
      "Epoch 46/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196ms/step - loss: 719.9457 - val_loss: 928.0983\n",
      "Epoch 47/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - loss: 688.0359 - val_loss: 446.9001\n",
      "Epoch 48/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 173ms/step - loss: 411.4375 - val_loss: 494.8157\n",
      "Epoch 49/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 189ms/step - loss: 490.7920 - val_loss: 92.2934\n",
      "Epoch 50/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - loss: 700.3199 - val_loss: 270.3724\n",
      "Epoch 51/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - loss: 705.3426 - val_loss: 578.6185\n",
      "Epoch 52/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - loss: 506.8560 - val_loss: 359.0140\n",
      "Epoch 53/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - loss: 425.7726 - val_loss: 305.6074\n",
      "Epoch 54/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - loss: 544.2606 - val_loss: 425.2692\n",
      "Epoch 55/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - loss: 432.7957 - val_loss: 176.9388\n",
      "Epoch 56/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 180ms/step - loss: 727.6331 - val_loss: 754.1882\n",
      "Epoch 57/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 173ms/step - loss: 514.1428 - val_loss: 399.9779\n",
      "Epoch 58/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 171ms/step - loss: 425.5528 - val_loss: 176.5371\n",
      "Epoch 59/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - loss: 319.5848 - val_loss: 222.3478\n",
      "Training complete.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "Validation Metrics:\n",
      "R\u00b2: 0.9806\n",
      "MAE: 7.6165\n",
      "RMSE: 9.5451\n",
      "SMAPE: 4.7930%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\n",
      "Independent Test Set Metrics:\n",
      "R\u00b2: 0.8749\n",
      "MAE: 25.7806\n",
      "RMSE: 27.9669\n",
      "SMAPE: 14.6801%\n",
      "Model for Fold 1 saved.\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Starting model training...\n",
      "Epoch 1/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 196ms/step - loss: 38035.6992 - val_loss: 20860.1973\n",
      "Epoch 2/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 193ms/step - loss: 15895.8672 - val_loss: 6260.0801\n",
      "Epoch 3/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 184ms/step - loss: 7945.3247 - val_loss: 6743.0142\n",
      "Epoch 4/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 173ms/step - loss: 6504.8237 - val_loss: 5988.0981\n",
      "Epoch 5/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - loss: 7420.3140 - val_loss: 5868.8555\n",
      "Epoch 6/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 185ms/step - loss: 5897.3765 - val_loss: 5714.4268\n",
      "Epoch 7/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 173ms/step - loss: 5699.2578 - val_loss: 7421.1748\n",
      "Epoch 8/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - loss: 5376.9448 - val_loss: 7673.3330\n",
      "Epoch 9/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - loss: 5777.4888 - val_loss: 6939.7134\n",
      "Epoch 10/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 182ms/step - loss: 7231.5430 - val_loss: 6402.7168\n",
      "Epoch 11/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - loss: 6120.6992 - val_loss: 6164.6177\n",
      "Epoch 12/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - loss: 5806.3184 - val_loss: 6814.5264\n",
      "Epoch 13/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - loss: 4514.6104 - val_loss: 6473.9302\n",
      "Epoch 14/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - loss: 5321.9092 - val_loss: 5026.9390\n",
      "Epoch 15/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - loss: 3263.0273 - val_loss: 3841.6992\n",
      "Epoch 16/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - loss: 1768.5708 - val_loss: 2659.7847\n",
      "Epoch 17/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 183ms/step - loss: 2192.2754 - val_loss: 2438.4534\n",
      "Epoch 18/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - loss: 1766.6339 - val_loss: 1419.0126\n",
      "Epoch 19/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - loss: 1513.6196 - val_loss: 2756.8599\n",
      "Epoch 20/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - loss: 1154.1907 - val_loss: 1445.5653\n",
      "Epoch 21/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 173ms/step - loss: 1456.8765 - val_loss: 1129.3461\n",
      "Epoch 22/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - loss: 1029.1079 - val_loss: 1493.2010\n",
      "Epoch 23/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 192ms/step - loss: 867.0353 - val_loss: 1161.8411\n",
      "Epoch 24/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - loss: 1064.3807 - val_loss: 1123.2981\n",
      "Epoch 25/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 187ms/step - loss: 872.8891 - val_loss: 1320.2903\n",
      "Epoch 26/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 222ms/step - loss: 1556.7318 - val_loss: 883.2892\n",
      "Epoch 27/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 182ms/step - loss: 810.6199 - val_loss: 1123.1558\n",
      "Epoch 28/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 182ms/step - loss: 740.3407 - val_loss: 1103.3055\n",
      "Epoch 29/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - loss: 612.8273 - val_loss: 1136.6890\n",
      "Epoch 30/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 191ms/step - loss: 627.4976 - val_loss: 1232.7627\n",
      "Epoch 31/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - loss: 523.0330 - val_loss: 1197.5999\n",
      "Epoch 32/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - loss: 787.9271 - val_loss: 1083.1228\n",
      "Epoch 33/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 188ms/step - loss: 749.8826 - val_loss: 866.6294\n",
      "Epoch 34/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 178ms/step - loss: 736.7047 - val_loss: 1141.8796\n",
      "Epoch 35/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 152ms/step - loss: 725.5397 - val_loss: 1785.1765\n",
      "Epoch 36/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - loss: 808.7308 - val_loss: 1057.8503\n",
      "Epoch 37/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - loss: 573.8601 - val_loss: 557.7913\n",
      "Epoch 38/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - loss: 701.8975 - val_loss: 1014.1274\n",
      "Epoch 39/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - loss: 558.1765 - val_loss: 1080.3567\n",
      "Epoch 40/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - loss: 568.7530 - val_loss: 845.9797\n",
      "Epoch 41/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - loss: 826.1613 - val_loss: 941.0776\n",
      "Epoch 42/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 180ms/step - loss: 578.0251 - val_loss: 966.6088\n",
      "Epoch 43/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 203ms/step - loss: 592.1334 - val_loss: 1341.7826\n",
      "Epoch 44/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - loss: 897.8297 - val_loss: 1141.9028\n",
      "Epoch 45/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - loss: 759.0145 - val_loss: 1211.3923\n",
      "Epoch 46/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - loss: 518.0706 - val_loss: 1766.8451\n",
      "Epoch 47/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - loss: 900.3137 - val_loss: 1392.5466\n",
      "Training complete.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Validation Metrics:\n",
      "R\u00b2: 0.8929\n",
      "MAE: 16.0809\n",
      "RMSE: 22.6144\n",
      "SMAPE: 10.3200%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\n",
      "Independent Test Set Metrics:\n",
      "R\u00b2: 0.9346\n",
      "MAE: 16.5220\n",
      "RMSE: 20.2182\n",
      "SMAPE: 10.2324%\n",
      "Model for Fold 2 saved.\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Starting model training...\n",
      "Epoch 1/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 197ms/step - loss: 46593.3242 - val_loss: 19870.7324\n",
      "Epoch 2/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 191ms/step - loss: 16521.7559 - val_loss: 22786.1758\n",
      "Epoch 3/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 178ms/step - loss: 8067.4458 - val_loss: 10273.7500\n",
      "Epoch 4/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 187ms/step - loss: 8384.5527 - val_loss: 10919.4775\n",
      "Epoch 5/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - loss: 7758.5537 - val_loss: 6279.8291\n",
      "Epoch 6/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - loss: 7523.8398 - val_loss: 8202.9160\n",
      "Epoch 7/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - loss: 6387.4033 - val_loss: 4650.8667\n",
      "Epoch 8/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - loss: 8234.8799 - val_loss: 9026.0293\n",
      "Epoch 9/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 179ms/step - loss: 8194.8828 - val_loss: 5473.3525\n",
      "Epoch 10/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - loss: 5968.1353 - val_loss: 8368.3643\n",
      "Epoch 11/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 179ms/step - loss: 4042.1257 - val_loss: 5562.1211\n",
      "Epoch 12/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 192ms/step - loss: 5119.3721 - val_loss: 5046.8906\n",
      "Epoch 13/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - loss: 4710.6655 - val_loss: 4887.0093\n",
      "Epoch 14/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - loss: 3692.7373 - val_loss: 4797.0654\n",
      "Epoch 15/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - loss: 5047.4717 - val_loss: 5911.1431\n",
      "Epoch 16/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 178ms/step - loss: 5366.4302 - val_loss: 6973.2109\n",
      "Epoch 17/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 189ms/step - loss: 4505.4185 - val_loss: 7878.9014\n",
      "Training complete.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Validation Metrics:\n",
      "R\u00b2: -0.4161\n",
      "MAE: 57.7146\n",
      "RMSE: 82.6163\n",
      "SMAPE: 31.0811%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\n",
      "Independent Test Set Metrics:\n",
      "R\u00b2: -0.1496\n",
      "MAE: 61.5264\n",
      "RMSE: 84.7770\n",
      "SMAPE: 34.3442%\n",
      "Model for Fold 3 saved.\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Starting model training...\n",
      "Epoch 1/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - loss: 35849.4023 - val_loss: 17608.1367\n",
      "Epoch 2/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - loss: 14598.1699 - val_loss: 6297.0205\n",
      "Epoch 3/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - loss: 6915.6216 - val_loss: 4007.9136\n",
      "Epoch 4/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 186ms/step - loss: 6930.7031 - val_loss: 3846.7590\n",
      "Epoch 5/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 179ms/step - loss: 8476.8545 - val_loss: 4021.5977\n",
      "Epoch 6/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - loss: 7353.1733 - val_loss: 4001.0437\n",
      "Epoch 7/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 179ms/step - loss: 8125.7432 - val_loss: 3956.2866\n",
      "Epoch 8/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 178ms/step - loss: 6119.1470 - val_loss: 3522.4387\n",
      "Epoch 9/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - loss: 5565.4561 - val_loss: 3383.7407\n",
      "Epoch 10/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 178ms/step - loss: 4825.0127 - val_loss: 3101.7539\n",
      "Epoch 11/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 179ms/step - loss: 3180.2117 - val_loss: 3041.7876\n",
      "Epoch 12/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 198ms/step - loss: 5292.5732 - val_loss: 2293.7078\n",
      "Epoch 13/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 190ms/step - loss: 4716.2188 - val_loss: 1852.3832\n",
      "Epoch 14/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 189ms/step - loss: 2854.4553 - val_loss: 1451.8102\n",
      "Epoch 15/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - loss: 2203.3032 - val_loss: 1023.5337\n",
      "Epoch 16/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - loss: 2099.0503 - val_loss: 864.4641\n",
      "Epoch 17/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 179ms/step - loss: 1811.7805 - val_loss: 470.1447\n",
      "Epoch 18/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 194ms/step - loss: 1140.4467 - val_loss: 373.6095\n",
      "Epoch 19/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 191ms/step - loss: 1096.0978 - val_loss: 490.2304\n",
      "Epoch 20/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 233ms/step - loss: 1498.6670 - val_loss: 655.3498\n",
      "Epoch 21/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 198ms/step - loss: 809.5922 - val_loss: 532.2347\n",
      "Epoch 22/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 178ms/step - loss: 720.4910 - val_loss: 286.3166\n",
      "Epoch 23/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 192ms/step - loss: 558.9530 - val_loss: 256.4294\n",
      "Epoch 24/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 195ms/step - loss: 862.9539 - val_loss: 495.4268\n",
      "Epoch 25/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 207ms/step - loss: 904.8303 - val_loss: 290.5353\n",
      "Epoch 26/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 186ms/step - loss: 821.2467 - val_loss: 346.6176\n",
      "Epoch 27/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 213ms/step - loss: 658.2886 - val_loss: 316.6950\n",
      "Epoch 28/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 226ms/step - loss: 598.5873 - val_loss: 511.1721\n",
      "Epoch 29/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 213ms/step - loss: 486.5780 - val_loss: 444.8293\n",
      "Epoch 30/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 235ms/step - loss: 619.9629 - val_loss: 311.5328\n",
      "Epoch 31/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 220ms/step - loss: 823.4698 - val_loss: 436.7088\n",
      "Epoch 32/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 238ms/step - loss: 717.8747 - val_loss: 250.4944\n",
      "Epoch 33/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 222ms/step - loss: 688.2518 - val_loss: 620.9490\n",
      "Epoch 34/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 226ms/step - loss: 457.6847 - val_loss: 862.0607\n",
      "Epoch 35/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 221ms/step - loss: 594.0432 - val_loss: 221.2736\n",
      "Epoch 36/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 238ms/step - loss: 780.6506 - val_loss: 222.5981\n",
      "Epoch 37/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 225ms/step - loss: 587.1041 - val_loss: 234.8180\n",
      "Epoch 38/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 225ms/step - loss: 514.5051 - val_loss: 171.2196\n",
      "Epoch 39/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 224ms/step - loss: 537.4988 - val_loss: 162.2872\n",
      "Epoch 40/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 228ms/step - loss: 670.6351 - val_loss: 153.6308\n",
      "Epoch 41/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 217ms/step - loss: 540.0270 - val_loss: 496.2028\n",
      "Epoch 42/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 192ms/step - loss: 427.0364 - val_loss: 729.9487\n",
      "Epoch 43/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 235ms/step - loss: 437.6095 - val_loss: 461.3664\n",
      "Epoch 44/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 214ms/step - loss: 455.5087 - val_loss: 344.4756\n",
      "Epoch 45/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 217ms/step - loss: 366.7024 - val_loss: 175.7654\n",
      "Epoch 46/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 214ms/step - loss: 642.9810 - val_loss: 440.0678\n",
      "Epoch 47/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 230ms/step - loss: 426.0742 - val_loss: 286.8554\n",
      "Epoch 48/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 252ms/step - loss: 488.8185 - val_loss: 293.9329\n",
      "Epoch 49/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 224ms/step - loss: 535.4410 - val_loss: 599.5895\n",
      "Epoch 50/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 231ms/step - loss: 482.2255 - val_loss: 87.4757\n",
      "Epoch 51/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 216ms/step - loss: 477.4433 - val_loss: 1152.9642\n",
      "Epoch 52/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 215ms/step - loss: 635.0801 - val_loss: 308.3963\n",
      "Epoch 53/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 219ms/step - loss: 358.7769 - val_loss: 1111.9084\n",
      "Epoch 54/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 225ms/step - loss: 753.7982 - val_loss: 906.6556\n",
      "Epoch 55/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 229ms/step - loss: 481.9627 - val_loss: 530.7381\n",
      "Epoch 56/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 229ms/step - loss: 346.0007 - val_loss: 787.9487\n",
      "Epoch 57/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 220ms/step - loss: 410.1654 - val_loss: 663.6478\n",
      "Epoch 58/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 218ms/step - loss: 298.9441 - val_loss: 425.0006\n",
      "Epoch 59/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 222ms/step - loss: 264.7376 - val_loss: 415.1155\n",
      "Epoch 60/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 233ms/step - loss: 301.1030 - val_loss: 674.4079\n",
      "Training complete.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "Validation Metrics:\n",
      "R\u00b2: 0.9610\n",
      "MAE: 8.8751\n",
      "RMSE: 12.5339\n",
      "SMAPE: 5.2982%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "Independent Test Set Metrics:\n",
      "R\u00b2: 0.9609\n",
      "MAE: 13.1906\n",
      "RMSE: 15.6439\n",
      "SMAPE: 8.6330%\n",
      "Model for Fold 4 saved.\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Starting model training...\n",
      "Epoch 1/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 251ms/step - loss: 31297.9082 - val_loss: 12617.6865\n",
      "Epoch 2/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 222ms/step - loss: 8751.3721 - val_loss: 7545.2759\n",
      "Epoch 3/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 227ms/step - loss: 7122.3145 - val_loss: 7741.5752\n",
      "Epoch 4/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 222ms/step - loss: 8680.2363 - val_loss: 7318.0381\n",
      "Epoch 5/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 221ms/step - loss: 6516.0259 - val_loss: 6692.9258\n",
      "Epoch 6/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 244ms/step - loss: 5082.5952 - val_loss: 6717.0142\n",
      "Epoch 7/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 217ms/step - loss: 5987.2505 - val_loss: 5618.4497\n",
      "Epoch 8/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 198ms/step - loss: 4288.1465 - val_loss: 5439.4463\n",
      "Epoch 9/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 220ms/step - loss: 4959.0146 - val_loss: 4541.0640\n",
      "Epoch 10/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 227ms/step - loss: 3395.8003 - val_loss: 3620.7671\n",
      "Epoch 11/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 228ms/step - loss: 2005.0509 - val_loss: 2825.6206\n",
      "Epoch 12/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 187ms/step - loss: 1716.0721 - val_loss: 2738.6809\n",
      "Epoch 13/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - loss: 1097.9741 - val_loss: 2036.9199\n",
      "Epoch 14/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - loss: 1264.1215 - val_loss: 1549.4612\n",
      "Epoch 15/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 173ms/step - loss: 1596.2273 - val_loss: 1680.3461\n",
      "Epoch 16/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 192ms/step - loss: 867.4861 - val_loss: 1561.3855\n",
      "Epoch 17/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - loss: 725.0885 - val_loss: 1715.4993\n",
      "Epoch 18/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - loss: 855.9058 - val_loss: 1478.8623\n",
      "Epoch 19/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 178ms/step - loss: 965.7094 - val_loss: 1059.9193\n",
      "Epoch 20/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 200ms/step - loss: 613.7482 - val_loss: 923.5741\n",
      "Epoch 21/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - loss: 728.7402 - val_loss: 882.3080\n",
      "Epoch 22/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - loss: 623.6656 - val_loss: 840.9771\n",
      "Epoch 23/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 189ms/step - loss: 629.0397 - val_loss: 883.3003\n",
      "Epoch 24/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - loss: 536.0858 - val_loss: 750.8531\n",
      "Epoch 25/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - loss: 688.9240 - val_loss: 901.6437\n",
      "Epoch 26/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - loss: 688.8694 - val_loss: 821.7488\n",
      "Epoch 27/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 184ms/step - loss: 844.3852 - val_loss: 566.9980\n",
      "Epoch 28/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 178ms/step - loss: 696.3343 - val_loss: 450.1576\n",
      "Epoch 29/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - loss: 764.8401 - val_loss: 290.4492\n",
      "Epoch 30/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 208ms/step - loss: 633.2536 - val_loss: 348.7090\n",
      "Epoch 31/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 184ms/step - loss: 647.1519 - val_loss: 718.2091\n",
      "Epoch 32/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 210ms/step - loss: 563.5236 - val_loss: 696.0675\n",
      "Epoch 33/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 189ms/step - loss: 650.5378 - val_loss: 528.5392\n",
      "Epoch 34/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - loss: 852.1181 - val_loss: 661.7416\n",
      "Epoch 35/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - loss: 547.1390 - val_loss: 346.5970\n",
      "Epoch 36/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 207ms/step - loss: 502.0598 - val_loss: 491.6921\n",
      "Epoch 37/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - loss: 424.5213 - val_loss: 489.9561\n",
      "Epoch 38/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - loss: 358.2312 - val_loss: 520.2050\n",
      "Epoch 39/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 194ms/step - loss: 574.7954 - val_loss: 492.2306\n",
      "Training complete.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "Validation Metrics:\n",
      "R\u00b2: 0.9247\n",
      "MAE: 14.4579\n",
      "RMSE: 19.0630\n",
      "SMAPE: 9.2361%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\n",
      "Independent Test Set Metrics:\n",
      "R\u00b2: 0.7667\n",
      "MAE: 33.3504\n",
      "RMSE: 38.1949\n",
      "SMAPE: 18.7843%\n",
      "Model for Fold 5 saved.\n",
      "\n",
      "================================================================================\n",
      "Final Average Metrics Across All Folds:\n",
      "================================================================================\n",
      "\n",
      "Average Validation Metrics:\n",
      "Average R\u00b2: 0.6686\n",
      "Average MAE: 20.9490\n",
      "Average RMSE: 29.2746\n",
      "Average SMAPE: 12.1457%\n",
      "\n",
      "Average Independent Test Set Metrics:\n",
      "Average R\u00b2: 0.6775\n",
      "Average MAE: 30.0740\n",
      "Average RMSE: 37.3602\n",
      "Average SMAPE: 17.3348%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Concatenate,\n",
    "    Dropout,\n",
    "    Layer,\n",
    "    GlobalAveragePooling2D,\n",
    "    Reshape,\n",
    "    Multiply\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import sys\n",
    "from io import StringIO\n",
    "import pickle\n",
    "",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "N_SPLITS = 5 # Number of folds for cross-validation\n",
    "",
    "# ==================== 1. Load Data ==================== #\n",
    "# NOTE: The data loading logic remains the same.\n",
    "# Replace with your actual data paths if needed\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "",
    "# Train-test split\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "",
    "print(f\"Using {len(raster_paths)} raster layers for CNN input.\")\n",
    "for r in raster_paths:\n",
    "    print(\"  -\", os.path.basename(r))\n",
    "",
    "# ==================== 3. Define Metric Functions ==================== #\n",
    "",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Symmetric Mean Absolute Percentage Error (SMAPE).\n",
    "    \"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    # Avoid division by zero by setting the result to 0 where denominator is 0\n",
    "    return np.mean(np.where(denominator == 0, 0, numerator / denominator)) * 100\n",
    "",
    "# ==================== 4. Create a Custom Data Generator ==================== #\n",
    "",
    "def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):\n",
    "    \"\"\"\n",
    "    Extracts a batch of patches from rasters for a given set of coordinates.\n",
    "    This function is optimized to be called by the data generator for each batch.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    # Loop through each coordinate pair in the batch\n",
    "    for lon, lat in coords:\n",
    "        channels = []\n",
    "        # Loop through each raster file to get a single patch for each raster\n",
    "        for rfile in raster_files:\n",
    "            with rasterio.open(rfile) as src:\n",
    "                try:\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)\n",
    "                    arr = src.read(1, window=win, boundless=True, fill_value=0)\n",
    "                    arr = arr.astype(np.float32)\n",
    "",
    "                    if np.nanmax(arr) != 0:\n",
    "                        arr /= np.nanmax(arr)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {rfile} for coordinates ({lon}, {lat}): {e}\")\n",
    "                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)\n",
    "            channels.append(arr)\n",
    "        patches.append(np.stack(channels, axis=-1))\n",
    "    \n",
    "    return np.array(patches)\n",
    "",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.coords = coords\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.raster_paths = raster_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.buffer_meters = buffer_meters\n",
    "",
    "        # Pre-calculate patch size from the first raster\n",
    "        with rasterio.open(raster_paths[0]) as src:\n",
    "            res_x, res_y = src.res\n",
    "            self.buffer_pixels_x = int(self.buffer_meters / res_x)\n",
    "            self.buffer_pixels_y = int(self.buffer_meters / res_y)\n",
    "            self.patch_width = 2 * self.buffer_pixels_x\n",
    "            self.patch_height = 2 * self.buffer_pixels_y\n",
    "",
    "        self.on_epoch_end()\n",
    "",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "",
    "        # Get batch data\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        \n",
    "        # Slice the GNN adjacency matrix for the current batch\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "",
    "        batch_y = self.y[batch_indices]\n",
    "",
    "        # Extract CNN patches for the current batch\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            self.raster_paths,\n",
    "            self.buffer_pixels_x,\n",
    "            self.buffer_pixels_y,\n",
    "            self.patch_width,\n",
    "            self.patch_height\n",
    "        )\n",
    "",
    "        # Return a tuple of inputs and the target, which Keras expects\n",
    "        return (batch_cnn, batch_mlp, batch_gnn), batch_y\n",
    "",
    "# ==================== 5. Define Custom Attention Layers ==================== #\n",
    "",
    "class SpatialAttention(Layer):\n",
    "    \"\"\"\n",
    "    A custom layer to apply spatial attention to a feature map.\n",
    "    It generates a spatial attention map and multiplies it with the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SpatialAttention, self).__init__(**kwargs)\n",
    "        self.conv1 = Conv2D(1, (1, 1), activation='sigmoid')\n",
    "",
    "    def call(self, inputs):\n",
    "        # Squeeze the channels and generate a 2D attention map\n",
    "        attention_map = self.conv1(inputs)\n",
    "        # Multiply the input feature map by the attention map\n",
    "        return Multiply()([inputs, attention_map])\n",
    "",
    "class FeatureAttention(Layer):\n",
    "    \"\"\"\n",
    "    A custom layer to apply feature-wise attention.\n",
    "    It learns a weight for each feature channel and multiplies it with the input.\n",
    "    Inspired by Squeeze-and-Excitation networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction_ratio=16, **kwargs):\n",
    "        super(FeatureAttention, self).__init__(**kwargs)\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "",
    "    def build(self, input_shape):\n",
    "        super(FeatureAttention, self).build(input_shape)\n",
    "        if len(input_shape) == 4: # CNN output\n",
    "            self.avg_pool = GlobalAveragePooling2D()\n",
    "            self.dense1 = Dense(units=input_shape[-1] // self.reduction_ratio, activation='relu')\n",
    "            self.dense2 = Dense(units=input_shape[-1], activation='sigmoid')\n",
    "            self.reshape_output = Reshape((1, 1, input_shape[-1]))\n",
    "        else: # MLP or GNN output\n",
    "            self.dense1 = Dense(units=input_shape[-1] // self.reduction_ratio, activation='relu')\n",
    "            self.dense2 = Dense(units=input_shape[-1], activation='sigmoid')\n",
    "",
    "    def call(self, inputs):\n",
    "        if len(inputs.shape) == 4: # CNN branch\n",
    "            x = self.avg_pool(inputs)\n",
    "            x = self.dense1(x)\n",
    "            x = self.dense2(x)\n",
    "            x = self.reshape_output(x)\n",
    "        else: # MLP or GNN branch\n",
    "            x = self.dense1(inputs)\n",
    "            x = self.dense2(x)\n",
    "            \n",
    "        return Multiply()([inputs, x])\n",
    "",
    "# ==================== 6. Define the Dual Attention Model ==================== #\n",
    "def build_dual_attention_model(patch_shape, gnn_dim, mlp_dim):\n",
    "    # Inputs for all branches\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    \n",
    "    # --- CNN Branch with Spatial and Feature Attention ---\n",
    "    cnn_branch = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_branch = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(cnn_branch)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    \n",
    "    # Spatial Attention\n",
    "    cnn_spatial_attn = SpatialAttention()(cnn_branch)\n",
    "    \n",
    "    # Feature Attention\n",
    "    cnn_feature_attn = FeatureAttention()(cnn_spatial_attn)\n",
    "    \n",
    "    # Flatten and get embedding\n",
    "    cnn_embedding = Flatten()(cnn_feature_attn)\n",
    "    cnn_embedding = Dense(128, activation=\"relu\", name=\"cnn_embedding\")(cnn_embedding)\n",
    "",
    "    # --- MLP Branch with Embedding ---\n",
    "    mlp_embedding = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(32, activation=\"relu\", name=\"mlp_embedding\")(mlp_embedding)\n",
    "",
    "    # --- GNN Branch with Feature Attention and Embedding ---\n",
    "    gnn_branch = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    \n",
    "    # Feature Attention\n",
    "    gnn_feature_attn = FeatureAttention()(gnn_branch)\n",
    "    gnn_embedding = Dense(32, activation=\"relu\", name=\"gnn_embedding\")(gnn_feature_attn)\n",
    "",
    "    # --- Attention Fusion ---\n",
    "    # Concatenate all embeddings\n",
    "    combined_embedding = Concatenate(name=\"combined_embedding\")([cnn_embedding, mlp_embedding, gnn_embedding])\n",
    "    \n",
    "    # Final dense layers for prediction\n",
    "    f = Dense(128, activation=\"relu\")(combined_embedding)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "",
    "def evaluate_model(model, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4):\n",
    "    \"\"\"\n",
    "    Evaluates a model on the given data and returns a dictionary of metrics.\n",
    "    \"\"\"\n",
    "    num_samples = len(y)\n",
    "    y_pred_list = []\n",
    "    \n",
    "    # Get patch dimensions\n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, res_y = src.res\n",
    "        buffer_pixels_x = int(buffer_meters / res_x)\n",
    "        buffer_pixels_y = int(buffer_meters / res_y)\n",
    "        patch_width = 2 * buffer_pixels_x\n",
    "        patch_height = 2 * buffer_pixels_y\n",
    "",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_coords = coords[i:i+batch_size]\n",
    "        batch_mlp = mlp_data[i:i+batch_size]\n",
    "        batch_gnn = gnn_data[i:i+batch_size, :]\n",
    "        \n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            raster_paths,\n",
    "            buffer_pixels_x,\n",
    "            buffer_pixels_y,\n",
    "            patch_width,\n",
    "            patch_height\n",
    "        )\n",
    "        \n",
    "        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn)).flatten())\n",
    "        \n",
    "    y_pred = np.concatenate(y_pred_list)\n",
    "    \n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    smape_val = smape(y, y_pred)\n",
    "",
    "    return {\n",
    "        'R2': r2,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'SMAPE': smape_val\n",
    "    }\n",
    "",
    "# ==================== 7. Run K-Fold Cross-Validation ==================== #\n",
    "",
    "",
    "print(\"=\"*80)\n",
    "print(f\"Starting {N_SPLITS}-Fold Cross-Validation...\")\n",
    "print(\"=\"*80)\n",
    "",
    "# Create a folder to save models\n",
    "model_save_dir = \"models/dual_attention\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "print(f\"Models will be saved in: '{model_save_dir}'\")\n",
    "",
    "# Prepare data for K-Fold\n",
    "combined_indices = np.arange(len(train_combined))\n",
    "coords_all = train_combined[['Long','Lat']].values\n",
    "data_all = train_combined[numeric_cols].values\n",
    "gnn_input_all = np.exp(-distance_matrix(coords_all, coords_all)/10)\n",
    "y_all = train_combined['RI'].values\n",
    "batch_size = 4\n",
    "",
    "# Store metrics for each fold\n",
    "fold_metrics = []\n",
    "test_metrics = []\n",
    "",
    "# Initialize K-Fold\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(combined_indices)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} ---\")\n",
    "",
    "    # Split data for the current fold\n",
    "    coords_train_fold = coords_all[train_indices]\n",
    "    coords_val_fold = coords_all[val_indices]\n",
    "    mlp_train_fold = data_all[train_indices]\n",
    "    mlp_val_fold = data_all[val_indices]\n",
    "    y_train_fold = y_all[train_indices]\n",
    "    y_val_fold = y_all[val_indices]\n",
    "",
    "    # Scale MLP data and prepare GNN matrices for the current fold\n",
    "    scaler_fold = StandardScaler()\n",
    "    mlp_train_scaled = scaler_fold.fit_transform(mlp_train_fold)\n",
    "    mlp_val_scaled = scaler_fold.transform(mlp_val_fold)\n",
    "    \n",
    "    dist_mat_train_fold = distance_matrix(coords_train_fold, coords_train_fold)\n",
    "    gnn_train_fold = np.exp(-dist_mat_train_fold/10)\n",
    "    \n",
    "    dist_mat_val_fold = distance_matrix(coords_val_fold, coords_train_fold)\n",
    "    gnn_val_fold = np.exp(-dist_mat_val_fold/10)\n",
    "",
    "    # Re-initialize and compile the model for each fold\n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, res_y = src.res\n",
    "        buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "        patch_width = 2 * buffer_pixels_x\n",
    "        cnn_patch_shape = (patch_width, patch_width, len(raster_paths))\n",
    "    \n",
    "    model = build_dual_attention_model(cnn_patch_shape, len(coords_train_fold), mlp_train_fold.shape[1])\n",
    "    \n",
    "    # Create Data Generators for the current fold\n",
    "    train_generator = DataGenerator(\n",
    "        coords=coords_train_fold,\n",
    "        mlp_data=mlp_train_scaled,\n",
    "        gnn_data=gnn_train_fold,\n",
    "        y=y_train_fold,\n",
    "        raster_paths=raster_paths,\n",
    "        buffer_meters=BUFFER_METERS,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_generator = DataGenerator(\n",
    "        coords=coords_val_fold,\n",
    "        mlp_data=mlp_val_scaled,\n",
    "        gnn_data=gnn_val_fold,\n",
    "        y=y_val_fold,\n",
    "        raster_paths=raster_paths,\n",
    "        buffer_meters=BUFFER_METERS,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "",
    "    print(\"Starting model training...\")\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=100,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping],\n",
    "        validation_data=val_generator\n",
    "    )\n",
    "    print(\"Training complete.\")\n",
    "",
    "    # Evaluate on the validation set\n",
    "    fold_result = evaluate_model(model, coords_val_fold, mlp_val_scaled, gnn_val_fold, y_val_fold, raster_paths, BUFFER_METERS, batch_size)\n",
    "    fold_metrics.append(fold_result)\n",
    "    print(\"Validation Metrics:\")\n",
    "    print(f\"R\u00b2: {fold_result['R2']:.4f}\")\n",
    "    print(f\"MAE: {fold_result['MAE']:.4f}\")\n",
    "    print(f\"RMSE: {fold_result['RMSE']:.4f}\")\n",
    "    print(f\"SMAPE: {fold_result['SMAPE']:.4f}%\")\n",
    "",
    "    # Prepare and evaluate on the independent test set\n",
    "    dist_mat_test_train = distance_matrix(test_orig[['Long','Lat']].values, coords_train_fold)\n",
    "    gnn_test_fold = np.exp(-dist_mat_test_train/10)\n",
    "    \n",
    "    mlp_test_scaled = scaler_fold.transform(test_orig[numeric_cols].values)\n",
    "",
    "    test_result = evaluate_model(model, test_orig[['Long','Lat']].values, mlp_test_scaled, gnn_test_fold, test_orig['RI'].values, raster_paths, BUFFER_METERS, batch_size)\n",
    "    test_metrics.append(test_result)\n",
    "    print(\"\\nIndependent Test Set Metrics:\")\n",
    "    print(f\"R\u00b2: {test_result['R2']:.4f}\")\n",
    "    print(f\"MAE: {test_result['MAE']:.4f}\")\n",
    "    print(f\"RMSE: {test_result['RMSE']:.4f}\")\n",
    "    print(f\"SMAPE: {test_result['SMAPE']:.4f}%\")\n",
    "    \n",
    "    # Save the trained model for the current fold\n",
    "    model.save(os.path.join(model_save_dir, f\"fold_{fold+1}.keras\"))\n",
    "    print(f\"Model for Fold {fold+1} saved.\")\n",
    "",
    "    # Clean up to free memory\n",
    "    del model, train_generator, val_generator, early_stopping, history\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "",
    "# ==================== 8. Print Final Averages ==================== #\n",
    "",
    "# Calculate average metrics\n",
    "avg_fold_metrics = pd.DataFrame(fold_metrics).mean().to_dict()\n",
    "avg_test_metrics = pd.DataFrame(test_metrics).mean().to_dict()\n",
    "",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Final Average Metrics Across All Folds:\")\n",
    "print(\"=\"*80)\n",
    "",
    "print(\"\\nAverage Validation Metrics:\")\n",
    "print(f\"Average R\u00b2: {avg_fold_metrics['R2']:.4f}\")\n",
    "print(f\"Average MAE: {avg_fold_metrics['MAE']:.4f}\")\n",
    "print(f\"Average RMSE: {avg_fold_metrics['RMSE']:.4f}\")\n",
    "print(f\"Average SMAPE: {avg_fold_metrics['SMAPE']:.4f}%\")\n",
    "",
    "print(\"\\nAverage Independent Test Set Metrics:\")\n",
    "print(f\"Average R\u00b2: {avg_test_metrics['R2']:.4f}\")\n",
    "print(f\"Average MAE: {avg_test_metrics['MAE']:.4f}\")\n",
    "print(f\"Average RMSE: {avg_test_metrics['RMSE']:.4f}\")\n",
    "print(f\"Average SMAPE: {avg_test_metrics['SMAPE']:.4f}%\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ab8c0e-c3ae-4c92-bbd7-cc771fc936d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 26 raster layers for CNN input.\n",
      "  - bui.tif\n",
      "  - ndsi.tif\n",
      "  - savi.tif\n",
      "  - ndbsi.tif\n",
      "  - ui.tif\n",
      "  - ndwi.tif\n",
      "  - ndbi.tif\n",
      "  - awei.tif\n",
      "  - evi.tif\n",
      "  - mndwi.tif\n",
      "  - ndvi.tif\n",
      "  - LULC2020.tif\n",
      "  - LULC2021.tif\n",
      "  - LULC2022.tif\n",
      "  - LULC2019.tif\n",
      "  - LULC2018.tif\n",
      "  - LULC2017.tif\n",
      "  - Pb_R.tif\n",
      "  - ClayR.tif\n",
      "  - SandR.tif\n",
      "  - CdR.tif\n",
      "  - CrR.tif\n",
      "  - AsR.tif\n",
      "  - SiltR.tif\n",
      "  - CuR.tif\n",
      "  - NiR.tif\n",
      "\n",
      "================================================================================\n",
      "Analyzing for BUFFER_METERS = 500m\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n",
       "\u2503<span style=\"font-weight: bold\"> Layer (type)        </span>\u2503<span style=\"font-weight: bold\"> Output Shape      </span>\u2503<span style=\"font-weight: bold\">    Param # </span>\u2503<span style=\"font-weight: bold\"> Connected to      </span>\u2503\n",
       "\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n",
       "\u2502 cnn_input           \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  \u2502          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502 -                 \u2502\n",
       "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        \u2502 <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)               \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  \u2502      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,520</span> \u2502 cnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \u2502\n",
       "\u2502                     \u2502 <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 max_pooling2d_2     \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    \u2502          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502 conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \u2502\n",
       "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      \u2502 <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>,    \u2502     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> \u2502 max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">\u2026</span> \u2502\n",
       "\u2502                     \u2502 <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 max_pooling2d_3     \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>,    \u2502          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502 conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \u2502\n",
       "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      \u2502 <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 spatial_attention_1 \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>,    \u2502         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> \u2502 max_pooling2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">\u2026</span> \u2502\n",
       "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialAttention</span>)  \u2502 <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 gnn_input           \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>)       \u2502          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502 -                 \u2502\n",
       "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 feature_attention_2 \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>,    \u2502        <span style=\"color: #00af00; text-decoration-color: #00af00\">580</span> \u2502 spatial_attentio\u2026 \u2502\n",
       "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">FeatureAttention</span>)  \u2502 <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 mlp_input           \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        \u2502          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502 -                 \u2502\n",
       "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        \u2502      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,104</span> \u2502 gnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40000</span>)     \u2502          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502 feature_attentio\u2026 \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        \u2502      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> \u2502 mlp_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 feature_attention_3 \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        \u2502        <span style=\"color: #00af00; text-decoration-color: #00af00\">580</span> \u2502 dense_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \u2502\n",
       "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">FeatureAttention</span>)  \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 cnn_embedding       \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       \u2502  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,128</span> \u2502 flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \u2502\n",
       "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 mlp_embedding       \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        \u2502      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> \u2502 dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \u2502\n",
       "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 gnn_embedding       \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        \u2502      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> \u2502 feature_attentio\u2026 \u2502\n",
       "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 combined_embedding  \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       \u2502          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502 cnn_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]\u2026 \u2502\n",
       "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       \u2502                   \u2502            \u2502 mlp_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]\u2026 \u2502\n",
       "\u2502                     \u2502                   \u2502            \u2502 gnn_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]\u2026 \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       \u2502     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> \u2502 combined_embeddi\u2026 \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       \u2502          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502 dense_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        \u2502      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> \u2502 dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 final_output        \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         \u2502         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> \u2502 dense_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \u2502\n",
       "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             \u2502                   \u2502            \u2502                   \u2502\n",
       "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n",
       "\u2503\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u2503\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u2503\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u2503\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\u2503\n",
       "\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n",
       "\u2502 cnn_input           \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  \u2502          \u001b[38;5;34m0\u001b[0m \u2502 -                 \u2502\n",
       "\u2502 (\u001b[38;5;33mInputLayer\u001b[0m)        \u2502 \u001b[38;5;34m26\u001b[0m)               \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)   \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  \u2502      \u001b[38;5;34m7,520\u001b[0m \u2502 cnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \u2502\n",
       "\u2502                     \u2502 \u001b[38;5;34m32\u001b[0m)               \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 max_pooling2d_2     \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    \u2502          \u001b[38;5;34m0\u001b[0m \u2502 conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \u2502\n",
       "\u2502 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      \u2502 \u001b[38;5;34m32\u001b[0m)               \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)   \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m,    \u2502     \u001b[38;5;34m18,496\u001b[0m \u2502 max_pooling2d_2[\u001b[38;5;34m\u2026\u001b[0m \u2502\n",
       "\u2502                     \u2502 \u001b[38;5;34m64\u001b[0m)               \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 max_pooling2d_3     \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m,    \u2502          \u001b[38;5;34m0\u001b[0m \u2502 conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \u2502\n",
       "\u2502 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      \u2502 \u001b[38;5;34m64\u001b[0m)               \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 spatial_attention_1 \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m,    \u2502         \u001b[38;5;34m65\u001b[0m \u2502 max_pooling2d_3[\u001b[38;5;34m\u2026\u001b[0m \u2502\n",
       "\u2502 (\u001b[38;5;33mSpatialAttention\u001b[0m)  \u2502 \u001b[38;5;34m64\u001b[0m)               \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 gnn_input           \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m110\u001b[0m)       \u2502          \u001b[38;5;34m0\u001b[0m \u2502 -                 \u2502\n",
       "\u2502 (\u001b[38;5;33mInputLayer\u001b[0m)        \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 feature_attention_2 \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m,    \u2502        \u001b[38;5;34m580\u001b[0m \u2502 spatial_attentio\u2026 \u2502\n",
       "\u2502 (\u001b[38;5;33mFeatureAttention\u001b[0m)  \u2502 \u001b[38;5;34m64\u001b[0m)               \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 mlp_input           \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        \u2502          \u001b[38;5;34m0\u001b[0m \u2502 -                 \u2502\n",
       "\u2502 (\u001b[38;5;33mInputLayer\u001b[0m)        \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 dense_11 (\u001b[38;5;33mDense\u001b[0m)    \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        \u2502      \u001b[38;5;34m7,104\u001b[0m \u2502 gnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 flatten_1 (\u001b[38;5;33mFlatten\u001b[0m) \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40000\u001b[0m)     \u2502          \u001b[38;5;34m0\u001b[0m \u2502 feature_attentio\u2026 \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 dense_10 (\u001b[38;5;33mDense\u001b[0m)    \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        \u2502      \u001b[38;5;34m1,024\u001b[0m \u2502 mlp_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 feature_attention_3 \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        \u2502        \u001b[38;5;34m580\u001b[0m \u2502 dense_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \u2502\n",
       "\u2502 (\u001b[38;5;33mFeatureAttention\u001b[0m)  \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 cnn_embedding       \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       \u2502  \u001b[38;5;34m5,120,128\u001b[0m \u2502 flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \u2502\n",
       "\u2502 (\u001b[38;5;33mDense\u001b[0m)             \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 mlp_embedding       \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u2502      \u001b[38;5;34m2,080\u001b[0m \u2502 dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \u2502\n",
       "\u2502 (\u001b[38;5;33mDense\u001b[0m)             \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 gnn_embedding       \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u2502      \u001b[38;5;34m2,080\u001b[0m \u2502 feature_attentio\u2026 \u2502\n",
       "\u2502 (\u001b[38;5;33mDense\u001b[0m)             \u2502                   \u2502            \u2502                   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 combined_embedding  \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       \u2502          \u001b[38;5;34m0\u001b[0m \u2502 cnn_embedding[\u001b[38;5;34m0\u001b[0m]\u2026 \u2502\n",
       "\u2502 (\u001b[38;5;33mConcatenate\u001b[0m)       \u2502                   \u2502            \u2502 mlp_embedding[\u001b[38;5;34m0\u001b[0m]\u2026 \u2502\n",
       "\u2502                     \u2502                   \u2502            \u2502 gnn_embedding[\u001b[38;5;34m0\u001b[0m]\u2026 \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 dense_14 (\u001b[38;5;33mDense\u001b[0m)    \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       \u2502     \u001b[38;5;34m24,704\u001b[0m \u2502 combined_embeddi\u2026 \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 dropout_1 (\u001b[38;5;33mDropout\u001b[0m) \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       \u2502          \u001b[38;5;34m0\u001b[0m \u2502 dense_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 dense_15 (\u001b[38;5;33mDense\u001b[0m)    \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        \u2502      \u001b[38;5;34m8,256\u001b[0m \u2502 dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \u2502\n",
       "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
       "\u2502 final_output        \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         \u2502         \u001b[38;5;34m65\u001b[0m \u2502 dense_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \u2502\n",
       "\u2502 (\u001b[38;5;33mDense\u001b[0m)             \u2502                   \u2502            \u2502                   \u2502\n",
       "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,192,682</span> (19.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,192,682\u001b[0m (19.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,192,682</span> (19.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,192,682\u001b[0m (19.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n",
      "Epoch 1/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 238ms/step - loss: 72274.9688 - val_loss: 23896.0723\n",
      "Epoch 2/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 224ms/step - loss: 19804.1426 - val_loss: 6201.9434\n",
      "Epoch 3/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 228ms/step - loss: 6295.6782 - val_loss: 5821.9165\n",
      "Epoch 4/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 232ms/step - loss: 6049.5356 - val_loss: 5852.0933\n",
      "Epoch 5/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 243ms/step - loss: 5586.8765 - val_loss: 5668.2544\n",
      "Epoch 6/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 260ms/step - loss: 7313.9126 - val_loss: 5344.6621\n",
      "Epoch 7/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 217ms/step - loss: 6672.3389 - val_loss: 4762.2036\n",
      "Epoch 8/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 216ms/step - loss: 5590.9268 - val_loss: 4319.7251\n",
      "Epoch 9/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 204ms/step - loss: 7149.6211 - val_loss: 3512.4905\n",
      "Epoch 10/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 218ms/step - loss: 3007.7678 - val_loss: 2839.8123\n",
      "Epoch 11/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 209ms/step - loss: 4228.4570 - val_loss: 1926.9413\n",
      "Epoch 12/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 239ms/step - loss: 2337.9185 - val_loss: 1266.9658\n",
      "Epoch 13/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 226ms/step - loss: 1882.9987 - val_loss: 803.5512\n",
      "Epoch 14/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 219ms/step - loss: 1564.1317 - val_loss: 572.2331\n",
      "Epoch 15/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 228ms/step - loss: 1183.5950 - val_loss: 824.6211\n",
      "Epoch 16/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 232ms/step - loss: 1256.6593 - val_loss: 396.8949\n",
      "Epoch 17/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 219ms/step - loss: 726.6584 - val_loss: 263.1011\n",
      "Epoch 18/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 209ms/step - loss: 1071.5629 - val_loss: 276.7889\n",
      "Epoch 19/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 224ms/step - loss: 1177.7902 - val_loss: 686.6694\n",
      "Epoch 20/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 225ms/step - loss: 653.8049 - val_loss: 771.8450\n",
      "Epoch 21/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 220ms/step - loss: 952.5395 - val_loss: 481.2414\n",
      "Epoch 22/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 219ms/step - loss: 1012.4254 - val_loss: 431.3610\n",
      "Epoch 23/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 218ms/step - loss: 609.7820 - val_loss: 780.0534\n",
      "Epoch 24/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 208ms/step - loss: 577.8860 - val_loss: 232.3553\n",
      "Epoch 25/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 212ms/step - loss: 602.0228 - val_loss: 440.9515\n",
      "Epoch 26/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 210ms/step - loss: 474.8898 - val_loss: 243.0340\n",
      "Epoch 27/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 214ms/step - loss: 407.3933 - val_loss: 289.9147\n",
      "Epoch 28/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 211ms/step - loss: 464.5904 - val_loss: 410.5615\n",
      "Epoch 29/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 221ms/step - loss: 682.3040 - val_loss: 926.0228\n",
      "Epoch 30/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 214ms/step - loss: 615.4860 - val_loss: 835.5005\n",
      "Epoch 31/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 215ms/step - loss: 737.9633 - val_loss: 168.5296\n",
      "Epoch 32/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 214ms/step - loss: 400.9777 - val_loss: 276.1775\n",
      "Epoch 33/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 217ms/step - loss: 450.7703 - val_loss: 566.4335\n",
      "Epoch 34/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 248ms/step - loss: 422.9298 - val_loss: 93.6559\n",
      "Epoch 35/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 237ms/step - loss: 766.7156 - val_loss: 363.7502\n",
      "Epoch 36/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 218ms/step - loss: 651.5408 - val_loss: 212.2534\n",
      "Epoch 37/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 215ms/step - loss: 391.5264 - val_loss: 148.3123\n",
      "Epoch 38/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 245ms/step - loss: 361.1339 - val_loss: 229.3793\n",
      "Epoch 39/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 219ms/step - loss: 669.2627 - val_loss: 326.3340\n",
      "Epoch 40/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 214ms/step - loss: 753.1210 - val_loss: 179.4583\n",
      "Epoch 41/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 218ms/step - loss: 399.7587 - val_loss: 352.1147\n",
      "Epoch 42/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 209ms/step - loss: 433.5223 - val_loss: 478.8716\n",
      "Epoch 43/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 206ms/step - loss: 553.6403 - val_loss: 377.4162\n",
      "Epoch 44/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 209ms/step - loss: 390.2715 - val_loss: 135.4992\n",
      "Epoch 45/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 205ms/step - loss: 410.1841 - val_loss: 379.2508\n",
      "Epoch 46/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 213ms/step - loss: 396.8198 - val_loss: 182.6138\n",
      "Epoch 47/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 216ms/step - loss: 472.0125 - val_loss: 240.8907\n",
      "Epoch 48/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 210ms/step - loss: 411.2181 - val_loss: 184.6344\n",
      "Epoch 49/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 214ms/step - loss: 392.6853 - val_loss: 68.5414\n",
      "Epoch 50/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 215ms/step - loss: 429.3143 - val_loss: 495.3206\n",
      "Epoch 51/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 218ms/step - loss: 432.7992 - val_loss: 337.9482\n",
      "Epoch 52/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 214ms/step - loss: 459.2827 - val_loss: 267.8995\n",
      "Epoch 53/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 216ms/step - loss: 261.1363 - val_loss: 315.6107\n",
      "Epoch 54/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 214ms/step - loss: 330.9416 - val_loss: 660.7816\n",
      "Epoch 55/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 218ms/step - loss: 318.4052 - val_loss: 414.7080\n",
      "Epoch 56/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 231ms/step - loss: 378.2579 - val_loss: 346.2991\n",
      "Epoch 57/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 228ms/step - loss: 364.4584 - val_loss: 610.1438\n",
      "Epoch 58/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 220ms/step - loss: 344.1521 - val_loss: 708.3749\n",
      "Epoch 59/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 226ms/step - loss: 520.5579 - val_loss: 208.1188\n",
      "Epoch 60/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 221ms/step - loss: 394.5730 - val_loss: 602.6772\n",
      "Epoch 61/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 219ms/step - loss: 392.4204 - val_loss: 194.1962\n",
      "Epoch 62/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 223ms/step - loss: 415.6519 - val_loss: 673.9833\n",
      "Epoch 63/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 225ms/step - loss: 283.4548 - val_loss: 650.7072\n",
      "Epoch 64/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 237ms/step - loss: 299.8112 - val_loss: 776.8445\n",
      "Training complete.\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\n",
      " Dual Attention Model Performance (500m):\n",
      "R\u00b2 Train: -0.9789 | RMSE Train: 96.0540 | MAE Train: 72.7091 | SMAPE Train: 39.2596\n",
      "R\u00b2 Test: 0.8608 | RMSE Test: 29.4955 | MAE Test: 24.6829 | SMAPE Test: 13.3766\n",
      "\n",
      "--------------------------------------------------\n",
      "Feature Importance Analysis for 500m\n",
      "--------------------------------------------------\n",
      "\n",
      "Baseline Performance on Test Set: R\u00b2 = 0.8608, MAE = 24.6829, SMAPE = 13.3766\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "--- Combined Feature Importance (by Model Branch) ---\n",
      "CNN Branch Importance (R\u00b2 drop): 1.0794\n",
      "MLP Branch Importance (R\u00b2 drop): 0.7508\n",
      "GNN Branch Importance (R\u00b2 drop): -0.0240\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\n",
      "--- MLP Feature Importance (Permutation-based) ---\n",
      "hydro_dist_brick    : R\u00b2 drop=-0.0000, MAE increase=-0.0000, SMAPE increase=-0.0000\n",
      "num_brick_field     : R\u00b2 drop=0.0016, MAE increase=0.8777, SMAPE increase=0.6949\n",
      "hydro_dist_ind      : R\u00b2 drop=-0.0000, MAE increase=-0.0000, SMAPE increase=-0.0000\n",
      "num_industry        : R\u00b2 drop=-0.0123, MAE increase=2.4994, SMAPE increase=2.2189\n",
      "CrR                 : R\u00b2 drop=-0.0078, MAE increase=-1.4254, SMAPE increase=-0.6325\n",
      "NiR                 : R\u00b2 drop=0.1183, MAE increase=10.0682, SMAPE increase=5.9606\n",
      "CuR                 : R\u00b2 drop=0.1531, MAE increase=9.2015, SMAPE increase=2.5795\n",
      "AsR                 : R\u00b2 drop=0.0242, MAE increase=1.4835, SMAPE increase=-0.5222\n",
      "CdR                 : R\u00b2 drop=-0.0147, MAE increase=-0.3599, SMAPE increase=-0.2028\n",
      "PbR                 : R\u00b2 drop=0.1514, MAE increase=12.6309, SMAPE increase=6.7625\n",
      "MR                  : R\u00b2 drop=-0.0066, MAE increase=-0.2061, SMAPE increase=-0.0968\n",
      "SandR               : R\u00b2 drop=-0.0287, MAE increase=-2.7944, SMAPE increase=-1.5885\n",
      "SiltR               : R\u00b2 drop=0.0114, MAE increase=1.4369, SMAPE increase=0.5570\n",
      "ClayR               : R\u00b2 drop=-0.0471, MAE increase=-4.5383, SMAPE increase=-2.4758\n",
      "FeR                 : R\u00b2 drop=0.1451, MAE increase=4.1258, SMAPE increase=0.3364\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Concatenate,\n",
    "    Dropout,\n",
    "    Layer,\n",
    "    Lambda,\n",
    "    GlobalAveragePooling2D,\n",
    "    Reshape,\n",
    "    Multiply\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "import gc # Import garbage collector\n",
    "import sys\n",
    "from io import StringIO # To capture print output\n",
    "import pickle # For saving dictionaries and other objects\n",
    "\n",
    "# Define the single buffer size to use\n",
    "BUFFER_METERS = 500\n",
    "\n",
    "# ==================== 1. Load Data ==================== #\n",
    "# NOTE: The data loading logic remains the same.\n",
    "# Replace with your actual data paths if needed\n",
    "orig = pd.read_csv(\"../../data/RainySeason.csv\")\n",
    "river_100 = pd.read_csv(\"../data/Samples_100.csv\")\n",
    "\n",
    "drop_cols = ['Stations','River','Lat','Long','geometry']\n",
    "numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')\n",
    "\n",
    "# Train-test split\n",
    "train_orig = orig.sample(10, random_state=42)\n",
    "test_orig = orig.drop(train_orig.index)\n",
    "train_combined = pd.concat([river_100, train_orig], ignore_index=True)\n",
    "\n",
    "# ==================== 2. Collect ALL Rasters ==================== #\n",
    "raster_paths = []\n",
    "raster_paths += glob.glob(\"../CalIndices/*.tif\")\n",
    "raster_paths += glob.glob(\"../LULCMerged/*.tif\")\n",
    "raster_paths += glob.glob(\"../IDW/*.tif\")\n",
    "\n",
    "print(f\"Using {len(raster_paths)} raster layers for CNN input.\")\n",
    "for r in raster_paths:\n",
    "    print(\"  -\", os.path.basename(r))\n",
    "\n",
    "# ==================== 3. Create a Custom Data Generator ==================== #\n",
    "def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):\n",
    "    \"\"\"\n",
    "    Extracts a batch of patches from rasters for a given set of coordinates.\n",
    "    This function is optimized to be called by the data generator for each batch.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    # Loop through each coordinate pair in the batch\n",
    "    for lon, lat in coords:\n",
    "        channels = []\n",
    "        # Loop through each raster file to get a single patch for each raster\n",
    "        for rfile in raster_files:\n",
    "            with rasterio.open(rfile) as src:\n",
    "                try:\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)\n",
    "                    arr = src.read(1, window=win, boundless=True, fill_value=0)\n",
    "                    arr = arr.astype(np.float32)\n",
    "\n",
    "                    if np.nanmax(arr) != 0:\n",
    "                        arr /= np.nanmax(arr)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {rfile} for coordinates ({lon}, {lat}): {e}\")\n",
    "                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)\n",
    "            channels.append(arr)\n",
    "        patches.append(np.stack(channels, axis=-1))\n",
    "    \n",
    "    return np.array(patches)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.coords = coords\n",
    "        self.mlp_data = mlp_data\n",
    "        self.gnn_data = gnn_data\n",
    "        self.y = y\n",
    "        self.raster_paths = raster_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.y))\n",
    "        self.buffer_meters = buffer_meters\n",
    "\n",
    "        # Pre-calculate patch size from the first raster\n",
    "        with rasterio.open(raster_paths[0]) as src:\n",
    "            res_x, res_y = src.res\n",
    "            self.buffer_pixels_x = int(self.buffer_meters / res_x)\n",
    "            self.buffer_pixels_y = int(self.buffer_meters / res_y)\n",
    "            self.patch_width = 2 * self.buffer_pixels_x\n",
    "            self.patch_height = 2 * self.buffer_pixels_y\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Get batch data\n",
    "        batch_coords = self.coords[batch_indices]\n",
    "        batch_mlp = self.mlp_data[batch_indices]\n",
    "        \n",
    "        # Slice the GNN adjacency matrix for the current batch\n",
    "        batch_gnn = self.gnn_data[batch_indices, :]\n",
    "\n",
    "        batch_y = self.y[batch_indices]\n",
    "\n",
    "        # Extract CNN patches for the current batch\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            self.raster_paths,\n",
    "            self.buffer_pixels_x,\n",
    "            self.buffer_pixels_y,\n",
    "            self.patch_width,\n",
    "            self.patch_height\n",
    "        )\n",
    "\n",
    "        # Return a tuple of inputs and the target, which Keras expects\n",
    "        return (batch_cnn, batch_mlp, batch_gnn), batch_y\n",
    "\n",
    "\n",
    "# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #\n",
    "coords_train = train_combined[['Long','Lat']].values\n",
    "coords_test = test_orig[['Long','Lat']].values\n",
    "dist_mat_train = distance_matrix(coords_train, coords_train)\n",
    "gnn_train = np.exp(-dist_mat_train/10)\n",
    "dist_mat_test_train = distance_matrix(coords_test, coords_train)\n",
    "gnn_test = np.exp(-dist_mat_test_train/10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mlp_train = scaler.fit_transform(train_combined[numeric_cols])\n",
    "mlp_test = scaler.transform(test_orig[numeric_cols])\n",
    "y_train = train_combined['RI'].values\n",
    "y_test = test_orig['RI'].values\n",
    "\n",
    "# ==================== 5. Define Custom Attention Layers ==================== #\n",
    "\n",
    "class SpatialAttention(Layer):\n",
    "    \"\"\"\n",
    "    A custom layer to apply spatial attention to a feature map.\n",
    "    It generates a spatial attention map and multiplies it with the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SpatialAttention, self).__init__(**kwargs)\n",
    "        self.conv1 = Conv2D(1, (1, 1), activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Squeeze the channels and generate a 2D attention map\n",
    "        attention_map = self.conv1(inputs)\n",
    "        # Multiply the input feature map by the attention map\n",
    "        return Multiply()([inputs, attention_map])\n",
    "\n",
    "class FeatureAttention(Layer):\n",
    "    \"\"\"\n",
    "    A custom layer to apply feature-wise attention.\n",
    "    It learns a weight for each feature channel and multiplies it with the input.\n",
    "    Inspired by Squeeze-and-Excitation networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction_ratio=16, **kwargs):\n",
    "        super(FeatureAttention, self).__init__(**kwargs)\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(FeatureAttention, self).build(input_shape)\n",
    "        if len(input_shape) == 4: # CNN output\n",
    "            self.avg_pool = GlobalAveragePooling2D()\n",
    "            self.dense1 = Dense(units=input_shape[-1] // self.reduction_ratio, activation='relu')\n",
    "            self.dense2 = Dense(units=input_shape[-1], activation='sigmoid')\n",
    "            self.reshape_output = Reshape((1, 1, input_shape[-1]))\n",
    "        else: # MLP or GNN output\n",
    "            self.dense1 = Dense(units=input_shape[-1] // self.reduction_ratio, activation='relu')\n",
    "            self.dense2 = Dense(units=input_shape[-1], activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if len(inputs.shape) == 4: # CNN branch\n",
    "            x = self.avg_pool(inputs)\n",
    "            x = self.dense1(x)\n",
    "            x = self.dense2(x)\n",
    "            x = self.reshape_output(x)\n",
    "        else: # MLP or GNN branch\n",
    "            x = self.dense1(inputs)\n",
    "            x = self.dense2(x)\n",
    "            \n",
    "        return Multiply()([inputs, x])\n",
    "\n",
    "# ==================== 6. Define the Dual Attention Model ==================== #\n",
    "def build_dual_attention_model(patch_shape, gnn_dim, mlp_dim):\n",
    "    # Inputs for all branches\n",
    "    cnn_input = Input(shape=patch_shape, name=\"cnn_input\")\n",
    "    mlp_input = Input(shape=(mlp_dim,), name=\"mlp_input\")\n",
    "    gnn_input = Input(shape=(gnn_dim,), name=\"gnn_input\")\n",
    "    \n",
    "    # --- CNN Branch with Spatial and Feature Attention ---\n",
    "    cnn_branch = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(cnn_input)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    cnn_branch = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(cnn_branch)\n",
    "    cnn_branch = MaxPooling2D((2,2))(cnn_branch)\n",
    "    \n",
    "    # Spatial Attention\n",
    "    cnn_spatial_attn = SpatialAttention()(cnn_branch)\n",
    "    \n",
    "    # Feature Attention\n",
    "    cnn_feature_attn = FeatureAttention()(cnn_spatial_attn)\n",
    "    \n",
    "    # Flatten and get embedding\n",
    "    cnn_embedding = Flatten()(cnn_feature_attn)\n",
    "    cnn_embedding = Dense(128, activation=\"relu\", name=\"cnn_embedding\")(cnn_embedding)\n",
    "\n",
    "    # --- MLP Branch with Embedding ---\n",
    "    mlp_embedding = Dense(64, activation=\"relu\")(mlp_input)\n",
    "    mlp_embedding = Dense(32, activation=\"relu\", name=\"mlp_embedding\")(mlp_embedding)\n",
    "\n",
    "    # --- GNN Branch with Feature Attention and Embedding ---\n",
    "    gnn_branch = Dense(64, activation=\"relu\")(gnn_input)\n",
    "    \n",
    "    # Feature Attention\n",
    "    gnn_feature_attn = FeatureAttention()(gnn_branch)\n",
    "    gnn_embedding = Dense(32, activation=\"relu\", name=\"gnn_embedding\")(gnn_feature_attn)\n",
    "\n",
    "    # --- Attention Fusion ---\n",
    "    # Concatenate all embeddings\n",
    "    combined_embedding = Concatenate(name=\"combined_embedding\")([cnn_embedding, mlp_embedding, gnn_embedding])\n",
    "    \n",
    "    # Final dense layers for prediction\n",
    "    f = Dense(128, activation=\"relu\")(combined_embedding)\n",
    "    f = Dropout(0.4)(f)\n",
    "    f = Dense(64, activation=\"relu\")(f)\n",
    "    output = Dense(1, activation=\"linear\", name=\"final_output\")(f)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Symmetric Mean Absolute Percentage Error (SMAPE).\n",
    "    \"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    # Handle the case where both y_true and y_pred are zero to avoid division by zero\n",
    "    return np.mean(numerator / (denominator + 1e-8)) * 100\n",
    "\n",
    "def evaluate_model(model, coords_test, mlp_test, gnn_test_matrix, y_test, raster_paths, buffer_meters, batch_size=4, return_preds=False):\n",
    "    num_samples = len(y_test)\n",
    "    y_pred_list = []\n",
    "    \n",
    "    with rasterio.open(raster_paths[0]) as src:\n",
    "        res_x, res_y = src.res\n",
    "        buffer_pixels_x = int(buffer_meters / res_x)\n",
    "        buffer_pixels_y = int(buffer_meters / res_y)\n",
    "        patch_width = 2 * buffer_pixels_x\n",
    "        patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_coords = coords_test[i:i+batch_size]\n",
    "        batch_mlp = mlp_test[i:i+batch_size]\n",
    "        \n",
    "        batch_gnn = gnn_test_matrix[i:i+batch_size, :]\n",
    "        batch_y = y_test[i:i+batch_size]\n",
    "\n",
    "        batch_cnn = extract_patch_for_generator(\n",
    "            batch_coords,\n",
    "            raster_paths,\n",
    "            buffer_pixels_x,\n",
    "            buffer_pixels_y,\n",
    "            patch_width,\n",
    "            patch_height\n",
    "        )\n",
    "        \n",
    "        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn)).flatten())\n",
    "        \n",
    "    y_pred = np.concatenate(y_pred_list)\n",
    "    \n",
    "    if return_preds:\n",
    "        return y_pred\n",
    "    else:\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        smape_val = smape(y_test, y_pred)\n",
    "        return r2, rmse, mae, smape_val\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Analyzing for BUFFER_METERS = {BUFFER_METERS}m\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_size = 4\n",
    "gnn_input_dim = len(coords_train)\n",
    "\n",
    "# Calculate CNN patch shape based on the current buffer size\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    cnn_patch_shape = (patch_width, patch_width, len(raster_paths))\n",
    "\n",
    "model = build_dual_attention_model(cnn_patch_shape, gnn_input_dim, mlp_train.shape[1])\n",
    "model.summary()\n",
    "\n",
    "# ==================== 7. Create Data Generators ==================== #\n",
    "train_generator = DataGenerator(\n",
    "    coords=coords_train,\n",
    "    mlp_data=mlp_train,\n",
    "    gnn_data=gnn_train,\n",
    "    y=y_train,\n",
    "    raster_paths=raster_paths,\n",
    "    buffer_meters=BUFFER_METERS,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# ==================== 8. Train Model ==================== #\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=train_generator\n",
    ")\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# ==================== 9. Evaluate ==================== #\n",
    "y_pred_train = model.predict(train_generator).flatten()\n",
    "r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))\n",
    "mae_train = mean_absolute_error(y_train[:len(y_pred_train)], y_pred_train)\n",
    "smape_train = smape(y_train[:len(y_pred_train)], y_pred_train)\n",
    "\n",
    "# Get test predictions for saving as .npy\n",
    "y_pred_test = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, return_preds=True)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "smape_test = smape(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\n Dual Attention Model Performance ({BUFFER_METERS}m):\")\n",
    "print(f\"R\u00b2 Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f} | MAE Train: {mae_train:.4f} | SMAPE Train: {smape_train:.4f}\")\n",
    "print(f\"R\u00b2 Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f} | MAE Test: {mae_test:.4f} | SMAPE Test: {smape_test:.4f}\")\n",
    "\n",
    "# ==================== 10. Feature Importance Analysis ==================== #\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(f\"Feature Importance Analysis for {BUFFER_METERS}m\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# --- 10.1 Combined Feature Importance (by Model Branch) ---\n",
    "y_pred_baseline = y_pred_test\n",
    "baseline_r2 = r2_test\n",
    "baseline_mae = mae_test\n",
    "baseline_smape = smape_test\n",
    "print(f\"\\nBaseline Performance on Test Set: R\u00b2 = {baseline_r2:.4f}, MAE = {baseline_mae:.4f}, SMAPE = {baseline_smape:.4f}\")\n",
    "\n",
    "# Ablate CNN branch\n",
    "with rasterio.open(raster_paths[0]) as src:\n",
    "    res_x, res_y = src.res\n",
    "    buffer_pixels_x = int(BUFFER_METERS / res_x)\n",
    "    buffer_pixels_y = int(BUFFER_METERS / res_y)\n",
    "    patch_width = 2 * buffer_pixels_x\n",
    "    patch_height = 2 * buffer_pixels_y\n",
    "\n",
    "cnn_test_ablated = np.zeros_like(extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "))\n",
    "y_pred_cnn_ablated = model.predict((cnn_test_ablated, mlp_test, gnn_test)).flatten()\n",
    "r2_cnn_ablated = r2_score(y_test, y_pred_cnn_ablated)\n",
    "mae_cnn_ablated = mean_absolute_error(y_test, y_pred_cnn_ablated)\n",
    "smape_cnn_ablated = smape(y_test, y_pred_cnn_ablated)\n",
    "importance_cnn = baseline_r2 - r2_cnn_ablated\n",
    "\n",
    "# Ablate MLP branch\n",
    "mlp_test_ablated = np.zeros_like(mlp_test)\n",
    "y_pred_mlp_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test_ablated, gnn_test)).flatten()\n",
    "r2_mlp_ablated = r2_score(y_test, y_pred_mlp_ablated)\n",
    "mae_mlp_ablated = mean_absolute_error(y_test, y_pred_mlp_ablated)\n",
    "smape_mlp_ablated = smape(y_test, y_pred_mlp_ablated)\n",
    "importance_mlp = baseline_r2 - r2_mlp_ablated\n",
    "\n",
    "# Ablate GNN branch\n",
    "gnn_test_ablated = np.zeros_like(gnn_test)\n",
    "y_pred_gnn_ablated = model.predict((extract_patch_for_generator(\n",
    "    coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "), mlp_test, gnn_test_ablated)).flatten()\n",
    "r2_gnn_ablated = r2_score(y_test, y_pred_gnn_ablated)\n",
    "mae_gnn_ablated = mean_absolute_error(y_test, y_pred_gnn_ablated)\n",
    "smape_gnn_ablated = smape(y_test, y_pred_gnn_ablated)\n",
    "importance_gnn = baseline_r2 - r2_gnn_ablated\n",
    "\n",
    "print(\"\\n--- Combined Feature Importance (by Model Branch) ---\")\n",
    "print(f\"CNN Branch Importance (R\u00b2 drop): {importance_cnn:.4f}\")\n",
    "print(f\"MLP Branch Importance (R\u00b2 drop): {importance_mlp:.4f}\")\n",
    "print(f\"GNN Branch Importance (R\u00b2 drop): {importance_gnn:.4f}\")\n",
    "\n",
    "# --- 10.2 MLP Feature Importance (Permutation-based) ---\n",
    "mlp_feature_importance = {}\n",
    "for i, feature_name in enumerate(numeric_cols):\n",
    "    mlp_test_shuffled = np.copy(mlp_test)\n",
    "    np.random.shuffle(mlp_test_shuffled[:, i])\n",
    "    \n",
    "    y_pred_shuffled = model.predict((extract_patch_for_generator(\n",
    "        coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height\n",
    "    ), mlp_test_shuffled, gnn_test)).flatten()\n",
    "    r2_shuffled = r2_score(y_test, y_pred_shuffled)\n",
    "    mae_shuffled = mean_absolute_error(y_test, y_pred_shuffled)\n",
    "    smape_shuffled = smape(y_test, y_pred_shuffled)\n",
    "    \n",
    "    importance_r2 = baseline_r2 - r2_shuffled\n",
    "    importance_mae = mae_shuffled - baseline_mae\n",
    "    importance_smape = smape_shuffled - baseline_smape\n",
    "    \n",
    "    mlp_feature_importance[feature_name] = {\n",
    "        'R2_drop': importance_r2,\n",
    "        'MAE_increase': importance_mae,\n",
    "        'SMAPE_increase': importance_smape\n",
    "    }\n",
    "\n",
    "print(\"\\n--- MLP Feature Importance (Permutation-based) ---\")\n",
    "for feature, metrics in mlp_feature_importance.items():\n",
    "    print(f\"{feature:<20}: R\u00b2 drop={metrics['R2_drop']:.4f}, MAE increase={metrics['MAE_increase']:.4f}, SMAPE increase={metrics['SMAPE_increase']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce14bf3-5e5e-4dfa-9d95-6aaa4fa00413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "alphaearth_integrated": true,
  "season": "rainy"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}