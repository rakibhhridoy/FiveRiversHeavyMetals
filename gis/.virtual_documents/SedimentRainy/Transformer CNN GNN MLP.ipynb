


import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import glob
import os
import rasterio
from rasterio.windows import Window
from scipy.spatial import distance_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout, Layer, MultiHeadAttention, LayerNormalization, Reshape
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import Sequence
import tensorflow as tf
import gc 
import sys
from io import StringIO
import pickle 
from tensorflow.keras.models import Model





BUFFER_METERS = 500

# ==================== 1. Load Data ==================== #
orig = pd.read_csv("../../data/RainySeason.csv")
river_100 = pd.read_csv("../data/Samples_100.csv")

drop_cols = ['Stations','River','Lat','Long','geometry']
numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')

# Train-test split
train_orig = orig.sample(10, random_state=42)
test_orig = orig.drop(train_orig.index)
train_combined = pd.concat([river_100, train_orig], ignore_index=True)

# ==================== 2. Collect ALL Rasters ==================== #
raster_paths = []
raster_paths += glob.glob("../CalIndices/*.tif")
raster_paths += glob.glob("../LULCMerged/*.tif")
raster_paths += glob.glob("../IDW/*.tif")

print(f"Using {len(raster_paths)} raster layers for CNN input.")
for r in raster_paths:
    print("  -", os.path.basename(r))

# ==================== 3. Create a Custom Data Generator ==================== #
def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):
    """
    Extracts a batch of patches from rasters for a given set of coordinates.
    This function is optimized to be called by the data generator for each batch.
    """
    patches = []
    # Loop through each coordinate pair in the batch
    for lon, lat in coords:
        channels = []
        # Loop through each raster file to get a single patch for each raster
        for rfile in raster_files:
            with rasterio.open(rfile) as src:
                try:
                    row, col = src.index(lon, lat)
                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)
                    arr = src.read(1, window=win, boundless=True, fill_value=0)
                    arr = arr.astype(np.float32)

                    if np.nanmax(arr) != 0:
                        arr /= np.nanmax(arr)
                except Exception as e:
                    print(f"Error processing {rfile} for coordinates ({lon}, {lat}): {e}")
                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)
            channels.append(arr)
        patches.append(np.stack(channels, axis=-1))
    
    return np.array(patches)

class DataGenerator(Sequence):
    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):
        super().__init__(**kwargs)
        self.coords = coords
        self.mlp_data = mlp_data
        self.gnn_data = gnn_data
        self.y = y
        self.raster_paths = raster_paths
        self.buffer_meters = buffer_meters
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.indices = np.arange(len(self.y))

        # Pre-calculate patch size from the first raster
        with rasterio.open(raster_paths[0]) as src:
            res_x, res_y = src.res
            self.buffer_pixels_x = int(self.buffer_meters / res_x)
            self.buffer_pixels_y = int(self.buffer_meters / res_y)
            self.patch_width = 2 * self.buffer_pixels_x
            self.patch_height = 2 * self.buffer_pixels_y

        self.on_epoch_end()

    def __len__(self):
        return int(np.floor(len(self.y) / self.batch_size))

    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indices)
            
    def __getitem__(self, index):
        # Get batch indices
        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]

        # Get batch data
        batch_coords = self.coords[batch_indices]
        batch_mlp = self.mlp_data[batch_indices]
        batch_gnn = self.gnn_data[batch_indices, :]
        batch_y = self.y[batch_indices]

        # Extract CNN patches for the current batch
        batch_cnn = extract_patch_for_generator(
            batch_coords,
            self.raster_paths,
            self.buffer_pixels_x,
            self.buffer_pixels_y,
            self.patch_width,
            self.patch_height
        )
        
        return (batch_cnn, batch_mlp, batch_gnn), batch_y

# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #
coords_train = train_combined[['Long','Lat']].values
coords_test = test_orig[['Long','Lat']].values
dist_mat_train = distance_matrix(coords_train, coords_train)
gnn_train = np.exp(-dist_mat_train/10)
dist_mat_test_train = distance_matrix(coords_test, coords_train)
gnn_test = np.exp(-dist_mat_test_train/10)

scaler = StandardScaler()
mlp_train = scaler.fit_transform(train_combined[numeric_cols])
mlp_test = scaler.transform(test_orig[numeric_cols])
y_train = train_combined['RI'].values
y_test = test_orig['RI'].values

# ==================== 5. Define Transformer-based Fusion Model ==================== #
def build_transformer_fusion_model(patch_shape, gnn_dim, mlp_dim):
    # Inputs for all branches
    cnn_input = Input(shape=patch_shape, name="cnn_input")
    mlp_input = Input(shape=(mlp_dim,), name="mlp_input")
    gnn_input = Input(shape=(gnn_dim,), name="gnn_input")
    
    # --- CNN Branch ---
    cnn_branch = Conv2D(32, (3,3), activation="relu", padding="same")(cnn_input)
    cnn_branch = MaxPooling2D((2,2))(cnn_branch)
    cnn_branch = Conv2D(64, (3,3), activation="relu", padding="same")(cnn_branch)
    cnn_branch = MaxPooling2D((2,2))(cnn_branch)
    cnn_embedding = Flatten(name="cnn_embedding_flatten")(cnn_branch)
    
    # --- MLP Branch ---
    mlp_embedding = Dense(128, activation="relu")(mlp_input)
    mlp_embedding = Dense(64, activation="relu", name="mlp_embedding")(mlp_embedding)

    # --- GNN Branch ---
    gnn_embedding = Dense(128, activation="relu")(gnn_input)
    gnn_embedding = Dense(64, activation="relu", name="gnn_embedding")(gnn_embedding)

    # --- Transformer Fusion ---
    # To feed into the transformer, we need to make all embeddings have the same dimension.
    # Let's use a dense layer to project them to a common size.
    projection_dim = 64
    cnn_proj = Dense(projection_dim)(cnn_embedding)
    mlp_proj = Dense(projection_dim)(mlp_embedding)
    gnn_proj = Dense(projection_dim)(gnn_embedding)

    # Stack the embeddings to create a sequence for the transformer
    # Shape becomes (None, 3, projection_dim)
    # Corrected code to use Keras-compatible operations
    cnn_expanded = Reshape((1, projection_dim))(cnn_proj)
    mlp_expanded = Reshape((1, projection_dim))(mlp_proj)
    gnn_expanded = Reshape((1, projection_dim))(gnn_proj)
    embeddings = Concatenate(axis=1)([cnn_expanded, mlp_expanded, gnn_expanded])

    # Transformer Encoder block
    transformer_output = MultiHeadAttention(
        num_heads=4,
        key_dim=projection_dim
    )(embeddings, embeddings)
    transformer_output = Dropout(0.2)(transformer_output)
    transformer_output = LayerNormalization(epsilon=1e-6)(embeddings + transformer_output)
    
    # The output from the transformer is a sequence of 3 vectors.
    # We flatten this for the final prediction layer.
    transformer_output_flattened = Flatten()(transformer_output)
    
    # Final dense layers for prediction
    f = Dense(128, activation="relu")(transformer_output_flattened)
    f = Dropout(0.4)(f)
    f = Dense(64, activation="relu")(f)
    output = Dense(1, activation="linear", name="final_output")(f)

    # Build and compile the model
    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)
    model.compile(optimizer=Adam(learning_rate=0.0005), loss="mse")
    return model

def evaluate_model(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=4, return_preds=False):
    """
    Evaluates the model on given data and returns R², RMSE, and predictions.
    """
    num_samples = len(y_true)
    y_pred_list = []
    
    with rasterio.open(raster_paths[0]) as src:
        res_x, res_y = src.res
        buffer_pixels_x = int(buffer_meters / res_x)
        buffer_pixels_y = int(buffer_meters / res_y)
        patch_width = 2 * buffer_pixels_x
        patch_height = 2 * buffer_pixels_y

    for i in range(0, num_samples, batch_size):
        batch_coords = coords[i:i+batch_size]
        batch_mlp = mlp_data[i:i+batch_size]
        batch_gnn = gnn_data[i:i+batch_size, :]
        
        batch_cnn = extract_patch_for_generator(
            batch_coords, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height
        )
        
        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn)).flatten())
        
    y_pred = np.concatenate(y_pred_list)
    
    if return_preds:
        return y_pred
    else:
        r2 = r2_score(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        return r2, rmse

def calculate_permutation_importance(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=4):
    """
    Calculates permutation feature importance for the three model branches.
    """
    print("\nStarting Permutation Feature Importance Analysis...")
    # Get baseline R² on the unshuffled data
    baseline_r2, _ = evaluate_model(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)
    print(f"Baseline R² on test set: {baseline_r2:.4f}")

    importance = {}
    
    # Permute CNN input
    shuffled_indices = np.random.permutation(len(y_true))
    coords_shuffled = coords[shuffled_indices]
    shuffled_r2, _ = evaluate_model(model, coords_shuffled, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)
    importance['CNN'] = baseline_r2 - shuffled_r2
    
    # Permute MLP input
    shuffled_mlp_data = mlp_data.copy()
    np.random.shuffle(shuffled_mlp_data)
    shuffled_r2, _ = evaluate_model(model, coords, shuffled_mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)
    importance['MLP'] = baseline_r2 - shuffled_r2

    # Permute GNN input
    shuffled_gnn_data = gnn_data.copy()
    np.random.shuffle(shuffled_gnn_data)
    shuffled_r2, _ = evaluate_model(model, coords, mlp_data, shuffled_gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)
    importance['GNN'] = baseline_r2 - shuffled_r2

    return importance

# --- NEW FUNCTION: Plotting the training history ---
def plot_training_history(history):
    """
    Plots the training and validation loss over the epochs.
    """
    # Get the loss and validation loss from the history object
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(1, len(loss) + 1)
    
    # Create the plot
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, loss, 'bo', label='Training Loss')
    plt.plot(epochs, val_loss, 'b', label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss (MSE)')
    plt.legend()
    plt.show()
    print("Training history plot generated.")

# ==================== Run the Analysis ==================== #

print("\n" + "="*80)
print(f"Analyzing Transformer-based Fusion Model for BUFFER_METERS = {BUFFER_METERS}m")
print("="*80)

batch_size = 4
gnn_input_dim = len(coords_train)

# Calculate CNN patch shape based on the current buffer size
with rasterio.open(raster_paths[0]) as src:
    res_x, res_y = src.res
    buffer_pixels_x = int(BUFFER_METERS / res_x)
    patch_width = 2 * buffer_pixels_x
    cnn_patch_shape = (patch_width, patch_width, len(raster_paths))

mlp_input_dim = mlp_train.shape[1]

model = build_transformer_fusion_model(cnn_patch_shape, gnn_input_dim, mlp_input_dim)
model.summary()

# ==================== 6. Create Data Generators ==================== #
train_generator = DataGenerator(
    coords=coords_train, mlp_data=mlp_train, gnn_data=gnn_train, y=y_train,
    raster_paths=raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, shuffle=True
)

# ==================== 7. Train Model ==================== #
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

history = model.fit(
    train_generator,
    epochs=100,
    verbose=1,
    callbacks=[early_stopping],
    validation_data=train_generator
)

plot_training_history(history)

# ==================== 8. Evaluate & Perform Feature Importance ==================== #
y_pred_train = model.predict(train_generator).flatten()
r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)
rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))

r2_test, rmse_test = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, BUFFER_METERS, batch_size=batch_size)

print(f"\n Transformer-based Fusion Model Performance ({BUFFER_METERS}m):")
print(f"R² Train: {r2_train:.4f} | RMSE Train: {rmse_train:.4f}")
print(f"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}")

# Calculate and print feature importance
feature_importance = calculate_permutation_importance(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, BUFFER_METERS, batch_size=batch_size)
print("\n--- Feature Importance (Permutation) ---")
sorted_importance = sorted(feature_importance.items(), key=lambda item: item[1], reverse=True)
for feature, score in sorted_importance:
    print(f"{feature}: {score:.4f}")





models_dir = 'models/transformer'
tf_save_path = os.path.join(models_dir, 'transformer_model.keras')
model.save(tf_save_path)

print(f"Model saved to: {tf_save_path}")





models_dir = 'models/transformer'
tf_save_path = os.path.join(models_dir, 'transformer_model.keras')

model = tf.keras.models.load_model(tf_save_path)
model.summary()


models_dir = 'models'
transformer_dir = os.path.join(models_dir, 'transformer')

history_save_path = os.path.join(transformer_dir, 'training_history.pkl')
print(f"Saving training history to {history_save_path}...")
with open(history_save_path, 'wb') as f:
    pickle.dump(history.history, f)

metrics_save_path = os.path.join(transformer_dir, 'test_metrics.pkl')
print(f"Saving test metrics to {metrics_save_path}...")
test_metrics = {'r2_test': r2_test, 'rmse_test': rmse_test}
with open(metrics_save_path, 'wb') as f:
    pickle.dump(test_metrics, f)

generator_params_path = os.path.join(transformer_dir, 'generator_params.pkl')
generator_params = {
    'coords_train': coords_train,
    'mlp_train': mlp_train,
    'gnn_train': gnn_train,
    'y_train': y_train,
    'coords_test': coords_test,
    'mlp_test': mlp_test,
    'gnn_test': gnn_test,
    'y_test': y_test,
    'raster_paths': raster_paths,
    'BUFFER_METERS': BUFFER_METERS,
    'batch_size': batch_size
}
with open(generator_params_path, 'wb') as f:
    pickle.dump(generator_params, f)
print("Successfully saved generator parameters.")





from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

def smape(y_true, y_pred):
    numerator = np.abs(y_pred - y_true)
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2
    denominator[denominator == 0] = 1e-10
    return np.mean(numerator / denominator) * 100


# ==================== 3. Make Predictions and Calculate Metrics ==================== #

predictions = model.predict([test_cnn, mlp_test, gnn_test])
r2 = r2_score(y_test, predictions)
mse = mean_squared_error(y_test, predictions)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, predictions)
smape_val = smape(y_test, predictions)

mr = {
    "R2": r2,
    "RMSE": rmse,
    "MAE": mae,
    "SMAPE": smape_val
}

metrics = pd.DataFrame(mr, index=["Value"])
metrics




















'''
# ==================== 1. Load Data (Required for the analysis) ==================== #
# Note: This section is included to make the script runnable.
# It loads and preprocesses the data needed for the evaluation.
orig = pd.read_csv("../../data/RainySeason.csv")
river_100 = pd.read_csv("../data/Samples_100.csv")

drop_cols = ['Stations','River','Lat','Long','geometry']
numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')

# Train-test split
train_orig = orig.sample(10, random_state=42)
test_orig = orig.drop(train_orig.index)
train_combined = pd.concat([river_100, train_orig], ignore_index=True)

# Collect ALL Rasters
raster_paths = []
raster_paths += glob.glob("../CalIndices/*.tif")
raster_paths += glob.glob("../LULCMerged/*.tif")
raster_paths += glob.glob("../IDW/*.tif")

# Prepare GNN & MLP Input
coords_test = test_orig[['Long','Lat']].values
coords_train = train_combined[['Long','Lat']].values # Need this for the gnn_test calculation
dist_mat_test_train = distance_matrix(coords_test, coords_train)
gnn_test = np.exp(-dist_mat_test_train/10)

scaler = StandardScaler()
# The scaler must be fit on the training data and transform the test data
mlp_train_df = train_combined[numeric_cols].copy()
mlp_test_df = test_orig[numeric_cols].copy()
scaler.fit(mlp_train_df)
mlp_test = scaler.transform(mlp_test_df)
y_test = test_orig['RI'].values
BUFFER_METERS = 500
batch_size = 4
'''
# ==================== 2. Helper Functions (Required for the analysis) ==================== #
def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):
    """
    Extracts a batch of patches from rasters for a given set of coordinates.
    This function is optimized to be called by the data generator for each batch.
    """
    patches = []
    for lon, lat in coords:
        channels = []
        for rfile in raster_files:
            with rasterio.open(rfile) as src:
                try:
                    row, col = src.index(lon, lat)
                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)
                    arr = src.read(1, window=win, boundless=True, fill_value=0)
                    arr = arr.astype(np.float32)

                    if np.nanmax(arr) != 0:
                        arr /= np.nanmax(arr)
                except Exception as e:
                    print(f"Error processing {rfile} for coordinates ({lon}, {lat}): {e}")
                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)
            channels.append(arr)
        patches.append(np.stack(channels, axis=-1))
    
    return np.array(patches)

def evaluate_model(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=4):
    """
    Evaluates the model on given data and returns R².
    """
    num_samples = len(y_true)
    y_pred_list = []
    
    with rasterio.open(raster_paths[0]) as src:
        res_x, res_y = src.res
        buffer_pixels_x = int(buffer_meters / res_x)
        buffer_pixels_y = int(buffer_meters / res_y)
        patch_width = 2 * buffer_pixels_x
        patch_height = 2 * buffer_pixels_y

    for i in range(0, num_samples, batch_size):
        batch_coords = coords[i:i+batch_size]
        batch_mlp = mlp_data[i:i+batch_size]
        batch_gnn = gnn_data[i:i+batch_size, :]
        
        batch_cnn = extract_patch_for_generator(
            batch_coords, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height
        )
        
        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn)).flatten())
        
    y_pred = np.concatenate(y_pred_list)
    r2 = r2_score(y_true, y_pred)
    return r2


# ==================== 3. Permutation Feature Importance Function ==================== #
def calculate_permutation_importance(model, coords, mlp_data, gnn_data, y_true, raster_paths, numeric_cols, buffer_meters, batch_size=4):
    """
    Calculates permutation feature importance for the three model branches
    and for each individual numeric feature.
    """
    print("\nStarting Permutation Feature Importance Analysis...")
    
    # Get baseline R² on the unshuffled data
    baseline_r2 = evaluate_model(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)
    print(f"Baseline R² on test set: {baseline_r2:.4f}\n")

    importance = {}
    
    # Permute CNN input (all rasters at once)
    print("Permuting CNN features...")
    shuffled_indices_cnn = np.random.permutation(len(y_true))
    coords_shuffled = coords[shuffled_indices_cnn]
    shuffled_r2 = evaluate_model(model, coords_shuffled, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)
    importance['CNN_all_rasters'] = baseline_r2 - shuffled_r2
    
    # Permute GNN input
    print("Permuting GNN features...")
    shuffled_gnn_data = gnn_data.copy()
    np.random.shuffle(shuffled_gnn_data)
    shuffled_r2 = evaluate_model(model, coords, mlp_data, shuffled_gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)
    importance['GNN_distance_matrix'] = baseline_r2 - shuffled_r2
    
    # Permute each MLP input feature individually
    print("Permuting individual numeric features...")
    for i, col in enumerate(numeric_cols):
        # Create a copy to shuffle a single column
        shuffled_mlp_data = mlp_data.copy()
        
        # Get the index of the column to shuffle in the numpy array
        mlp_data_df = pd.DataFrame(mlp_data, columns=numeric_cols)
        col_index = mlp_data_df.columns.get_loc(col)
        
        # Shuffle only the values for this specific column
        shuffled_col = shuffled_mlp_data[:, col_index].copy()
        np.random.shuffle(shuffled_col)
        shuffled_mlp_data[:, col_index] = shuffled_col
        
        # Evaluate model with the single shuffled feature
        shuffled_r2 = evaluate_model(model, coords, shuffled_mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)
        importance[f'MLP_{col}'] = baseline_r2 - shuffled_r2

    return importance

# Calculate and print feature importance
feature_importance = calculate_permutation_importance(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, numeric_cols, BUFFER_METERS, batch_size=batch_size)
print("\n--- Feature Importance (Permutation) ---")
sorted_importance = sorted(feature_importance.items(), key=lambda item: item[1], reverse=True)
for feature, score in sorted_importance:
    print(f"{feature}: {score:.4f}")


sorted_importance


'''
orig = pd.read_csv("../../data/RainySeason.csv")
river_100 = pd.read_csv("../data/Samples_100.csv")

drop_cols = ['Stations','River','Lat','Long','geometry']
numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')

train_orig = orig.sample(10, random_state=42)
test_orig = orig.drop(train_orig.index)
train_combined = pd.concat([river_100, train_orig], ignore_index=True)

raster_paths = []
raster_paths += glob.glob("../CalIndices/*.tif")
raster_paths += glob.glob("../LULCMerged/*.tif")
raster_paths += glob.glob("../IDW/*.tif")

coords_test = test_orig[['Long','Lat']].values
coords_train = train_combined[['Long','Lat']].values # Need this for the gnn_test calculation
dist_mat_test_train = distance_matrix(coords_test, coords_train)
gnn_test = np.exp(-dist_mat_test_train/10)

scaler = StandardScaler()

mlp_train_df = train_combined[numeric_cols].copy()
mlp_test_df = test_orig[numeric_cols].copy()
scaler.fit(mlp_train_df)
mlp_test = scaler.transform(mlp_test_df)
y_test = test_orig['RI'].values
BUFFER_METERS = 500
batch_size = 4
'''

def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):
    """
    Extracts a batch of patches from rasters for a given set of coordinates.
    This function is optimized to be called by the data generator for each batch.
    """
    patches = []
    for lon, lat in coords:
        channels = []
        for rfile in raster_files:
            with rasterio.open(rfile) as src:
                try:
                    row, col = src.index(lon, lat)
                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)
                    arr = src.read(1, window=win, boundless=True, fill_value=0)
                    arr = arr.astype(np.float32)

                    if np.nanmax(arr) != 0:
                        arr /= np.nanmax(arr)
                except Exception as e:
                    print(f"Error processing {rfile} for coordinates ({lon}, {lat}): {e}")
                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)
            channels.append(arr)
        patches.append(np.stack(channels, axis=-1))
    
    return np.array(patches)

def evaluate_model(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=4, shuffle_raster_idx=None):
    """
    Evaluates the model on given data and returns R².
    Includes an option to shuffle a single raster's features for importance calculation.
    """
    num_samples = len(y_true)
    y_pred_list = []
    
    with rasterio.open(raster_paths[0]) as src:
        res_x, res_y = src.res
        buffer_pixels_x = int(buffer_meters / res_x)
        buffer_pixels_y = int(buffer_meters / res_y)
        patch_width = 2 * buffer_pixels_x
        patch_height = 2 * buffer_pixels_y

    for i in range(0, num_samples, batch_size):
        batch_coords = coords[i:i+batch_size]
        batch_mlp = mlp_data[i:i+batch_size]
        batch_gnn = gnn_data[i:i+batch_size, :]
        
        batch_cnn = extract_patch_for_generator(
            batch_coords, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height
        )
        
        if shuffle_raster_idx is not None:
            # Shuffle the patches for the specific raster channel
            shuffled_indices = np.random.permutation(len(batch_cnn))
            batch_cnn_shuffled_channel = batch_cnn[:, :, :, shuffle_raster_idx][shuffled_indices]
            batch_cnn[:, :, :, shuffle_raster_idx] = batch_cnn_shuffled_channel

        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn)).flatten())
        
    y_pred = np.concatenate(y_pred_list)
    r2 = r2_score(y_true, y_pred)
    return r2


# ==================== 3. Permutation Feature Importance Function ==================== #
def calculate_permutation_importance(model, coords, mlp_data, gnn_data, y_true, raster_paths, numeric_cols, buffer_meters, batch_size=4):
    """
    Calculates permutation feature importance for the three model branches
    and for each individual numeric feature and raster.
    """
    print("\nStarting Permutation Feature Importance Analysis...")
    
    # Get baseline R² on the unshuffled data
    baseline_r2 = evaluate_model(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)
    print(f"Baseline R² on test set: {baseline_r2:.4f}\n")

    importance = {}
    
    # Permute CNN input (all rasters at once)
    print("Permuting CNN features (all rasters at once)...")
    shuffled_indices_cnn = np.random.permutation(len(y_true))
    coords_shuffled = coords[shuffled_indices_cnn]
    shuffled_r2 = evaluate_model(model, coords_shuffled, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)
    importance['CNN_all_rasters'] = baseline_r2 - shuffled_r2
    
    # Permute each individual raster separately
    print("Permuting individual rasters...")
    for i, rfile in enumerate(raster_paths):
        raster_name = os.path.basename(rfile).split('.')[0]
        shuffled_r2 = evaluate_model(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size, shuffle_raster_idx=i)
        importance[f'CNN_{raster_name}'] = baseline_r2 - shuffled_r2
        print(f"  - Finished permutation for {raster_name}")

    # Permute GNN input
    print("\nPermuting GNN features...")
    shuffled_gnn_data = gnn_data.copy()
    np.random.shuffle(shuffled_gnn_data)
    shuffled_r2 = evaluate_model(model, coords, mlp_data, shuffled_gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)
    importance['GNN_distance_matrix'] = baseline_r2 - shuffled_r2
    
    # Permute each MLP input feature individually
    print("Permuting individual numeric features...")
    for i, col in enumerate(numeric_cols):
        # Create a copy to shuffle a single column
        shuffled_mlp_data = mlp_data.copy()
        
        # Get the index of the column to shuffle in the numpy array
        mlp_data_df = pd.DataFrame(mlp_data, columns=numeric_cols)
        col_index = mlp_data_df.columns.get_loc(col)
        
        # Shuffle only the values for this specific column
        shuffled_col = shuffled_mlp_data[:, col_index].copy()
        np.random.shuffle(shuffled_col)
        shuffled_mlp_data[:, col_index] = shuffled_col
        
        # Evaluate model with the single shuffled feature
        shuffled_r2 = evaluate_model(model, coords, shuffled_mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)
        importance[f'MLP_{col}'] = baseline_r2 - shuffled_r2
        print(f"  - Finished permutation for {col}")

    return importance


feature_importance1 = calculate_permutation_importance(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, numeric_cols, BUFFER_METERS, batch_size=batch_size)
print("\n--- Feature Importance (Permutation) ---")
sorted_importance = sorted(feature_importance1.items(), key=lambda item: item[1], reverse=True)
for feature, score in sorted_importance:
    print(f"{feature}: {score:.4f}")


sorted_importance





from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.inspection import PartialDependenceDisplay
from lime.lime_tabular import LimeTabularExplainer

'''
# ==================== 1. Load Data (from previous analysis) ==================== #
# This section remains the same to ensure data is prepared for the new methods.
orig = pd.read_csv("../../data/RainySeason.csv")
river_100 = pd.read_csv("../data/Samples_100.csv")

drop_cols = ['Stations','River','Lat','Long','geometry']
numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')

train_orig = orig.sample(10, random_state=42)
test_orig = orig.drop(train_orig.index)
train_combined = pd.concat([river_100, train_orig], ignore_index=True)

raster_paths = []
raster_paths += glob.glob("../CalIndices/*.tif")
raster_paths += glob.glob("../LULCMerged/*.tif")
raster_paths += glob.glob("../IDW/*.tif")

coords_test = test_orig[['Long','Lat']].values
coords_train = train_combined[['Long','Lat']].values
dist_mat_test_train = distance_matrix(coords_test, coords_train)
gnn_test = np.exp(-dist_mat_test_train/10)

scaler = StandardScaler()
mlp_train_df = train_combined[numeric_cols].copy()
mlp_test_df = test_orig[numeric_cols].copy()
scaler.fit(mlp_train_df)
mlp_test = scaler.transform(mlp_test_df)
y_test = test_orig['RI'].values
BUFFER_METERS = 500
batch_size = 4
'''

# Helper function to get CNN patches, used for LIME and PDP
def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):
    patches = []
    for lon, lat in coords:
        channels = []
        for rfile in raster_files:
            with rasterio.open(rfile) as src:
                try:
                    row, col = src.index(lon, lat)
                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)
                    arr = src.read(1, window=win, boundless=True, fill_value=0)
                    arr = arr.astype(np.float32)

                    if np.nanmax(arr) != 0:
                        arr /= np.nanmax(arr)
                except Exception as e:
                    print(f"Error processing {rfile} for coordinates ({lon}, {lat}): {e}")
                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)
            channels.append(arr)
        patches.append(np.stack(channels, axis=-1))
    
    return np.array(patches)

# Get necessary patch dimensions
with rasterio.open(raster_paths[0]) as src:
    res_x, res_y = src.res
    buffer_pixels_x = int(BUFFER_METERS / res_x)
    buffer_pixels_y = int(BUFFER_METERS / res_y)
    patch_width = 2 * buffer_pixels_x
    patch_height = 2 * buffer_pixels_y

test_cnn = extract_patch_for_generator(coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height)


# ==================== 2. Intrinsic Feature Importance (Tree-based Model) ==================== #
# Note: Intrinsic importance is specific to tree-based models and cannot be directly
# applied to your TensorFlow model. This example trains a separate Gradient Boosting
# model on just the MLP features to demonstrate the concept. This is a common
# approach to gain initial insights into the tabular data.
print("\n--- Running Intrinsic Feature Importance Analysis ---")
print("Training a Gradient Boosting Regressor on the MLP features...")

# Create and train a tree-based model on the MLP features
gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbr_model.fit(mlp_train_df, train_combined['RI'].values)

# Get feature importances
intrinsic_importances = gbr_model.feature_importances_

# Plot the results
plt.figure(figsize=(20, 8))
plt.barh(numeric_cols, intrinsic_importances, color='skyblue')
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.title("Intrinsic Feature Importance from a Gradient Boosting Model (MLP features only)")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()


# ==================== 3. LIME (Local Interpretable Model-agnostic Explanations) ==================== #
# LIME explains individual predictions by fitting a local, simple model.
# This is a powerful way to understand *why* a single data point was predicted a certain way.
print("\n--- Running LIME Analysis for a single data point ---")

# First, we need to create a single-input wrapper function for the original model
# so LIME can treat it as a black box. This function takes a concatenated array.
def model_predict_wrapper(X):
    # This wrapper is now specifically for the LIME explainer on MLP/GNN features.
    # It takes the perturbed MLP and GNN data from LIME, but uses a fixed CNN patch
    # from the original instance being explained.
    
    # Separate the MLP and GNN data from the LIME input
    mlp_input = X[:, :mlp_test.shape[1]]
    gnn_input = X[:, mlp_test.shape[1]:]

    # For the CNN input, we'll use a fixed patch from the instance we are explaining.
    # We replicate this patch to match the number of samples LIME creates (X.shape[0]).
    fixed_cnn_input = np.tile(test_cnn[instance_to_explain_idx], (X.shape[0], 1, 1, 1))

    # Pass the inputs to the original model
    return model.predict([fixed_cnn_input, mlp_input, gnn_input]).flatten()

# The LIME analysis will now only use the MLP and GNN features.
combined_test_data = np.hstack([mlp_test, gnn_test])

# Create feature names for the LIME explainer
mlp_feature_names = numeric_cols.tolist()
gnn_feature_names = [f'GNN_{i}' for i in range(gnn_test.shape[1])]
feature_names = mlp_feature_names + gnn_feature_names

# Create the LIME explainer instance
# The kernel_width parameter controls the size of the local area LIME will investigate.
explainer = LimeTabularExplainer(
    training_data=combined_test_data,
    feature_names=feature_names,
    class_names=['RI'], # For regression, use the name of the target
    mode='regression',
    verbose=True
)

# Choose a single instance to explain (e.g., the 5th instance)
instance_to_explain_idx = 4
instance_to_explain = combined_test_data[instance_to_explain_idx]

# Generate the explanation
print(f"Generating LIME explanation for instance {instance_to_explain_idx}...")
explanation = explainer.explain_instance(
    data_row=instance_to_explain,
    predict_fn=model_predict_wrapper,
    num_features=10 # Explain the top 10 most important features for this instance
)

# Plot the LIME explanation
explanation.as_pyplot_figure()
plt.title(f'LIME Explanation for Test Instance {instance_to_explain_idx}')
plt.tight_layout()
plt.show()


# ==================== 4. Partial Dependence Plots (PDPs) ==================== #
# PDPs show the marginal effect of a feature on the predicted outcome.
# This is a global view of a feature's impact on the model's predictions.
print("\n--- Running Partial Dependence Plot (PDP) Analysis ---")
print("Available numeric columns for PDP:", numeric_cols.tolist())

# PDPs for all features using the GBR model
pdp_features_to_plot_all = numeric_cols.tolist()
pdp_feature_indices_all = list(range(len(numeric_cols)))

# Calculate grid dimensions to ensure the number of subplots matches the number of features
num_features = len(pdp_features_to_plot_all)
n_cols = 4
n_rows = (num_features + n_cols - 1) // n_cols

fig, ax = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 15))
# Flatten the axes array and slice to the correct size
axes_to_use = ax.flatten()[:num_features]
PartialDependenceDisplay.from_estimator(
    gbr_model,
    X=mlp_train_df,
    features=pdp_feature_indices_all,
    feature_names=numeric_cols.tolist(),
    ax=axes_to_use,
    grid_resolution=50
)
fig.suptitle('Partial Dependence Plots (GBR Model)', fontsize=16)
plt.tight_layout()
plt.show()


print("\n--- Manually generating PDP for the full TensorFlow model ---")

def manual_pdp(model, feature_name, X_test_cnn, X_test_mlp, X_test_gnn, feature_idx, feature_values=None):
    """Generates a manual PDP for a single feature in a multi-input model."""
    if feature_values is None:
        # Create a range of values for the feature
        feature_values = np.linspace(X_test_mlp[:, feature_idx].min(), X_test_mlp[:, feature_idx].max(), 100)
    
    # Create a copy of the MLP input data
    X_test_mlp_pdp = X_test_mlp.copy()
    
    predictions = []
    for val in feature_values:
        # For each value, set the feature to that value across all instances
        X_test_mlp_pdp[:, feature_idx] = val
        
        # Predict with the model and take the mean
        pred = model.predict([X_test_cnn, X_test_mlp_pdp, X_test_gnn], verbose=0).mean()
        predictions.append(pred)
        
    return feature_values, np.array(predictions)

# Plot the PDPs for the TensorFlow model for all features
fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 15))
axes = axes.flatten()

for i, feature_name in enumerate(pdp_features_to_plot_all):
    feature_idx = numeric_cols.get_loc(feature_name)
    feature_values, pdp_predictions = manual_pdp(model, feature_name, test_cnn, mlp_test, gnn_test, feature_idx)
    axes[i].plot(feature_values, pdp_predictions, color='red')
    axes[i].set_xlabel(feature_name)
    axes[i].set_ylabel('Predicted RI (on average)')
    axes[i].set_title(f'')

fig.suptitle('Manual Partial Dependence Plots (TensorFlow Model)', fontsize=16)
plt.tight_layout()
plt.show()

print("\n--- Analysis Complete ---")


# ==================== 1. Intrinsic Feature Importance Results ==================== #

print("--- Numeric Results: Intrinsic Feature Importance (from GBR) ---")
intrinsic_importances = gbr_model.feature_importances_
importance_dict = dict(zip(numeric_cols, intrinsic_importances))
sorted_importance = sorted(importance_dict.items(), key=lambda item: item[1], reverse=True)
print(sorted_importance)
print("\n")


# ==================== 2. LIME Results ==================== #

print("--- Numeric Results: LIME Explanation (for a single instance) ---")
sorted_lime = sorted(explanation.as_list(), key=lambda x: abs(x[1]), reverse=True)
print(sorted_lime)
print("\n")


# ==================== 3. Partial Dependence Plot (PDP) Results ==================== #

print("--- Numeric Results: Manual PDP Data (for Top 5 Features) ---")
top_5_features = [item[0] for item in sorted_importance[:5]]

for feature_name in top_5_features:
    feature_idx = numeric_cols.tolist().index(feature_name)

    feature_values, pdp_predictions = manual_pdp(model, feature_name, test_cnn, mlp_test, gnn_test, feature_idx)

    print(f"--- PDP Data for: '{feature_name}' ---")
    print(f"Feature Values:")
    print(feature_values)
    print("\nCorresponding Average Predicted RI Values:")
    print(pdp_predictions)
    print("\n")


del model, history, train_generator
gc.collect()
