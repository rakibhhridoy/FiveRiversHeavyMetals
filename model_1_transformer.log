/Users/rakibhhridoy/Library/Python/3.9/lib/python/site-packages/nbformat/__init__.py:96: MissingIDFieldWarning: Cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.
  validate(nb)
Traceback (most recent call last):
  File "/Users/rakibhhridoy/Library/Python/3.9/lib/python/site-packages/jupyter_core/utils/__init__.py", line 154, in wrapped
    asyncio.get_running_loop()
RuntimeError: no running event loop

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/rakibhhridoy/Five_Rivers/direct_model_run.py", line 50, in run_notebook_direct
    client.execute()
  File "/Users/rakibhhridoy/Library/Python/3.9/lib/python/site-packages/jupyter_core/utils/__init__.py", line 158, in wrapped
    return loop.run_until_complete(inner)
  File "/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py", line 642, in run_until_complete
    return future.result()
  File "/Users/rakibhhridoy/Library/Python/3.9/lib/python/site-packages/nbclient/client.py", line 709, in async_execute
    await self.async_execute_cell(
  File "/Users/rakibhhridoy/Library/Python/3.9/lib/python/site-packages/nbclient/client.py", line 1062, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/Users/rakibhhridoy/Library/Python/3.9/lib/python/site-packages/nbclient/client.py", line 918, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
import pandas as pd
import numpy as np
import glob
import os
import rasterio
from rasterio.windows import Window
from scipy.spatial import distance_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout, MultiHeadAttention, LayerNormalization, Reshape
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import Sequence
import tensorflow as tf
import gc # Import garbage collector
import pickle # Import the pickle library for saving objects
import matplotlib.pyplot as plt # Import matplotlib for plotting
# Define the single buffer size to use
BUFFER_METERS = 500
# ==================== 1. Load Data ==================== #
orig = pd.read_csv("../../data/RainySeason.csv")
river_100 = pd.read_csv("../data/Samples_100.csv")
combined_data = pd.concat([river_100, orig], ignore_index=True)
drop_cols = ['Stations','River','Lat','Long','geometry']
numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')
# ==================== 2. Collect ALL Rasters ==================== #
raster_paths = []
raster_paths += glob.glob("../CalIndices/*.tif")
raster_paths += glob.glob("../LULCMerged/*.tif")
raster_paths += glob.glob("../IDW/*.tif")
print(f"Using {len(raster_paths)} raster layers for CNN input.")
for r in raster_paths:
    print(" Â -", os.path.basename(r))
# ==================== 3. Create a Custom Data Generator ==================== #
def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):
    """
    Extracts a batch of patches from rasters for a given set of coordinates.
    This function is optimized to be called by the data generator for each batch.
    """
    patches = []
    # Loop through each coordinate pair in the batch
    for lon, lat in coords:
        channels = []
        # Loop through each raster file to get a single patch for each raster
        for rfile in raster_files:
            with rasterio.open(rfile) as src:
                try:
                    row, col = src.index(lon, lat)
                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)
                    arr = src.read(1, window=win, boundless=True, fill_value=0)
                    arr = arr.astype(np.float32)
                    if np.nanmax(arr) != 0:
                        arr /= np.nanmax(arr)
                except Exception as e:
                    print(f"Error processing {rfile} for coordinates ({lon}, {lat}): {e}")
                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)
            channels.append(arr)
        patches.append(np.stack(channels, axis=-1))
    
    return np.array(patches)
class DataGenerator(Sequence):
    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):
        super().__init__(**kwargs)
        self.coords = coords
        self.mlp_data = mlp_data
        self.gnn_data = gnn_data
        self.y = y
        self.raster_paths = raster_paths
        self.buffer_meters = buffer_meters
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.indices = np.arange(len(self.y))
        # Pre-calculate patch size from the first raster
        with rasterio.open(raster_paths[0]) as src:
            res_x, res_y = src.res
            self.buffer_pixels_x = int(self.buffer_meters / res_x)
            self.buffer_pixels_y = int(self.buffer_meters / res_y)
            self.patch_width = 2 * self.buffer_pixels_x
            self.patch_height = 2 * self.buffer_pixels_y
        self.on_epoch_end()
    def __len__(self):
        return int(np.floor(len(self.y) / self.batch_size))
    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indices)
            
    def __getitem__(self, index):
        # Get batch indices
        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]
        # Get batch data
        batch_coords = self.coords[batch_indices]
        batch_mlp = self.mlp_data[batch_indices]
        batch_gnn = self.gnn_data[batch_indices, :]
        batch_y = self.y[batch_indices]
        # Extract CNN patches for the current batch
        batch_cnn = extract_patch_for_generator(
            batch_coords,
            self.raster_paths,
            self.buffer_pixels_x,
            self.buffer_pixels_y,
            self.patch_width,
            self.patch_height
        )
        
        return (batch_cnn, batch_mlp, batch_gnn), batch_y
def evaluate_model(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=4, return_preds=False):
    """
    Evaluates the model on given data and returns RÂ², RMSE, and predictions.
    """
    num_samples = len(y_true)
    y_pred_list = []
    
    with rasterio.open(raster_paths[0]) as src:
        res_x, res_y = src.res
        buffer_pixels_x = int(buffer_meters / res_x)
        buffer_pixels_y = int(buffer_meters / res_y)
        patch_width = 2 * buffer_pixels_x
        patch_height = 2 * buffer_pixels_y
    for i in range(0, num_samples, batch_size):
        batch_coords = coords[i:i+batch_size]
        batch_mlp = mlp_data[i:i+batch_size]
        batch_gnn = gnn_data[i:i+batch_size, :]
        
        batch_cnn = extract_patch_for_generator(
            batch_coords, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height
        )
        
        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn)).flatten())
        
    y_pred = np.concatenate(y_pred_list)
    
    if return_preds:
        return y_pred
    else:
        r2 = r2_score(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        return r2, rmse
def calculate_permutation_importance(model, coords, mlp_data, gnn_data, y_true, raster_paths, numeric_cols, buffer_meters, batch_size=4):
    """
    Calculates permutation feature importance for the three model branches
    and for each individual numeric feature.
    """
    print("\nStarting Permutation Feature Importance Analysis...")
    
    # Get baseline RÂ² on the unshuffled data
    baseline_r2, _ = evaluate_model(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)
    print(f"Baseline RÂ² on test set: {baseline_r2:.4f}\n")
    importance = {}
    
    # Permute CNN input (all rasters at once)
    print("Permuting CNN features...")
    shuffled_indices_cnn = np.random.permutation(len(y_true))
    coords_shuffled = coords[shuffled_indices_cnn]
    shuffled_r2, _ = evaluate_model(model, coords_shuffled, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)
    importance['CNN_all_rasters'] = baseline_r2 - shuffled_r2
    
    # Permute GNN input
    print("Permuting GNN features...")
    shuffled_gnn_data = gnn_data.copy()
    np.random.shuffle(shuffled_gnn_data)
    shuffled_r2, _ = evaluate_model(model, coords, mlp_data, shuffled_gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)
    importance['GNN_distance_matrix'] = baseline_r2 - shuffled_r2
    
    # Permute each MLP input feature individually
    print("Permuting individual numeric features...")
    for i, col in enumerate(numeric_cols):
        # Create a copy to shuffle a single column
        shuffled_mlp_data = mlp_data.copy()
        
        # Get the index of the column to shuffle in the numpy array
        mlp_data_df = pd.DataFrame(mlp_data, columns=numeric_cols)
        col_index = mlp_data_df.columns.get_loc(col)
        
        # Shuffle only the values for this specific column
        shuffled_col = shuffled_mlp_data[:, col_index].copy()
        np.random.shuffle(shuffled_col)
        shuffled_mlp_data[:, col_index] = shuffled_col
        
        # Evaluate model with the single shuffled feature
        shuffled_r2, _ = evaluate_model(model, coords, shuffled_mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)
        importance[f'MLP_{col}'] = baseline_r2 - shuffled_r2
    return importance
# --- NEW FUNCTION: Plotting the training history ---
def plot_training_history(history, fold_num=None):
    """
    Plots the training and validation loss over the epochs.
    """
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(1, len(loss) + 1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, loss, 'bo', label='Training Loss')
    plt.plot(epochs, val_loss, 'b', label='Validation Loss')
    if fold_num is not None:
        plt.title(f'Training and Validation Loss for Fold {fold_num}')
    else:
        plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss (MSE)')
    plt.legend()
    plt.show()
    print("Training history plot generated.")
# ==================== 4. Prepare all data for Train/Test Single Split ==================== #
print("Preparing all data for Train/Test cross-validation...")
# Prepare GNN & MLP input for the entire dataset
coords_all = combined_data[['Long', 'Lat']].values
dist_mat_all = distance_matrix(coords_all, coords_all)
gnn_all = np.exp(-dist_mat_all/10)
scaler = StandardScaler()
mlp_all = scaler.fit_transform(combined_data[numeric_cols])
y_all = combined_data['RI'].values
# ==================== 5. Define Transformer-based Fusion Model ==================== #
def build_transformer_fusion_model(patch_shape, gnn_dim, mlp_dim):
    # Inputs for all branches
    cnn_input = Input(shape=patch_shape, name="cnn_input")
    mlp_input = Input(shape=(mlp_dim,), name="mlp_input")
    gnn_input = Input(shape=(gnn_dim,), name="gnn_input")
    
    # --- CNN Branch ---
    cnn_branch = Conv2D(32, (3,3), activation="relu", padding="same")(cnn_input)
    cnn_branch = MaxPooling2D((2,2))(cnn_branch)
    cnn_branch = Conv2D(64, (3,3), activation="relu", padding="same")(cnn_branch)
    cnn_branch = MaxPooling2D((2,2))(cnn_branch)
    cnn_embedding = Flatten(name="cnn_embedding_flatten")(cnn_branch)
    
    # --- MLP Branch ---
    mlp_embedding = Dense(128, activation="relu")(mlp_input)
    mlp_embedding = Dense(64, activation="relu", name="mlp_embedding")(mlp_embedding)
    # --- GNN Branch ---
    gnn_embedding = Dense(128, activation="relu")(gnn_input)
    gnn_embedding = Dense(64, activation="relu", name="gnn_embedding")(gnn_embedding)
    # --- Transformer Fusion ---
    # To feed into the transformer, we need to make all embeddings have the same dimension.
    # Let's use a dense layer to project them to a common size.
    projection_dim = 64
    cnn_proj = Dense(projection_dim)(cnn_embedding)
    mlp_proj = Dense(projection_dim)(mlp_embedding)
    gnn_proj = Dense(projection_dim)(gnn_embedding)
    # Stack the embeddings to create a sequence for the transformer
    # Shape becomes (None, 3, projection_dim)
    cnn_expanded = Reshape((1, projection_dim))(cnn_proj)
    mlp_expanded = Reshape((1, projection_dim))(mlp_proj)
    gnn_expanded = Reshape((1, projection_dim))(gnn_proj)
    embeddings = Concatenate(axis=1)([cnn_expanded, mlp_expanded, gnn_expanded])
    # Transformer Encoder block
    transformer_output = MultiHeadAttention(
        num_heads=4,
        key_dim=projection_dim
    )(embeddings, embeddings)
    transformer_output = Dropout(0.2)(transformer_output)
    transformer_output = LayerNormalization(epsilon=1e-6)(embeddings + transformer_output)
    
    # The output from the transformer is a sequence of 3 vectors.
    transformer_output_flattened = Flatten()(transformer_output)
    
    # Final dense layers for prediction
    f = Dense(128, activation="relu")(transformer_output_flattened)
    f = Dropout(0.4)(f)
    f = Dense(64, activation="relu")(f)
    output = Dense(1, activation="linear", name="final_output")(f)
    # Build and compile the model
    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)
    model.compile(optimizer=Adam(learning_rate=0.0005), loss="mse")
    return model
# ==================== 6. Implement Train/Test Single Split ==================== #
print("\n" + "="*80)
print("Starting Train/Test Single Split (K=5)")
print("="*80)
# Initialize Train/Test
# Single train/test split (80/20)
fold_r2_scores = []
fold_rmse_scores = []
batch_size = 4
# Calculate CNN patch shape once
with rasterio.open(raster_paths[0]) as src:
    res_x, res_y = src.res
    buffer_pixels_x = int(BUFFER_METERS / res_x)
    patch_width = 2 * buffer_pixels_x
    cnn_patch_shape = (patch_width, patch_width, len(raster_paths))
mlp_input_dim = mlp_all.shape[1]
# Loop through each fold
if True:  # Single train/test split
    print(f"\n--- Training and Validation Split ---")
    # Split the data for the current fold
    coords_train_fold, coords_val_fold = coords_all[train_index], coords_all[val_index]
    mlp_train_fold, mlp_val_fold = mlp_all[train_index], mlp_all[val_index]
    y_train_fold, y_val_fold = y_all[train_index], y_all[val_index]
    # Handle the GNN data split
    # gnn_train_fold is the sub-matrix for the training points
    gnn_train_fold = gnn_all[train_index][:, train_index]
    # gnn_val_fold is the matrix of validation points' distances to training points
    gnn_val_fold = gnn_all[val_index][:, train_index]
    # Build a fresh model for each fold
    model = build_transformer_fusion_model(cnn_patch_shape, len(gnn_train_fold), mlp_input_dim)
    
    # Create data generators for the current fold
    train_generator = DataGenerator(
        coords=coords_train_fold, mlp_data=mlp_train_fold, gnn_data=gnn_train_fold, y=y_train_fold,
        raster_paths=raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, shuffle=True
    )
    val_generator = DataGenerator(
        coords=coords_val_fold, mlp_data=mlp_val_fold, gnn_data=gnn_val_fold, y=y_val_fold,
        raster_paths=raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, shuffle=False
    )
    # Train the model with early stopping
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )
    history = model.fit(
        train_generator,
        epochs=100,
        verbose=1,
        callbacks=[early_stopping],
        validation_data=val_generator
    )
    # Evaluate the model on the validation set for this fold
    r2_fold, rmse_fold = evaluate_model(
        model, coords_val_fold, mlp_val_fold, gnn_val_fold, y_val_fold,
        raster_paths, BUFFER_METERS, batch_size=batch_size
    )
    # Store the results
    fold_r2_scores = [r2_fold]
    fold_rmse_scores = [rmse_fold]
    
    print(f"Test RÂ²: {r2_fold:.4f} | RMSE: {rmse_fold:.4f}")
    models_dir = 'models/transformer'
    tf_save_path = os.path.join(models_dir, f'transformer_cnn_gnn_mlp{fold+1}.keras')
    model.save(tf_save_path)
    # Clean up memory
    del model, history, train_generator, val_generator
    gc.collect()
# ==================== 7. Summarize Single Split Results ==================== #
print("\n" + "="*80)
print(f"Training and Validation Results")
print("="*80)
print(f"Test RÂ²: {fold_r2_scores[0]:.4f}")
print(f"Test RMSE: {fold_rmse_scores[0]:.4f}")

------------------

----- stdout -----
Using 26 raster layers for CNN input.
 Â - bui.tif
 Â - ndsi.tif
 Â - savi.tif
 Â - ndbsi.tif
 Â - ui.tif
 Â - ndwi.tif
 Â - ndbi.tif
 Â - awei.tif
 Â - evi.tif
 Â - mndwi.tif
 Â - ndvi.tif
 Â - LULC2020.tif
 Â - LULC2021.tif
 Â - LULC2022.tif
 Â - LULC2019.tif
 Â - LULC2018.tif
 Â - LULC2017.tif
 Â - Pb_R.tif
 Â - ClayR.tif
 Â - SandR.tif
 Â - CdR.tif
 Â - CrR.tif
 Â - AsR.tif
 Â - SiltR.tif
 Â - CuR.tif
 Â - NiR.tif
Preparing all data for Train/Test cross-validation...

================================================================================
Starting Train/Test Single Split (K=5)
================================================================================

--- Training and Validation Split ---
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
Cell [0;32mIn[4], line 288[0m
[1;32m    286[0m [38;5;28mprint[39m([38;5;124mf[39m[38;5;124m"[39m[38;5;130;01m\n[39;00m[38;5;124m--- Training and Validation Split ---[39m[38;5;124m"[39m)
[1;32m    287[0m [38;5;66;03m# Split the data for the current fold[39;00m
[0;32m--> 288[0m coords_train_fold, coords_val_fold [38;5;241m=[39m coords_all[[43mtrain_index[49m], coords_all[val_index]
[1;32m    289[0m mlp_train_fold, mlp_val_fold [38;5;241m=[39m mlp_all[train_index], mlp_all[val_index]
[1;32m    290[0m y_train_fold, y_val_fold [38;5;241m=[39m y_all[train_index], y_all[val_index]

[0;31mNameError[0m: name 'train_index' is not defined


Loading notebook: /Users/rakibhhridoy/Five_Rivers/gis/SedimentRainyAE/Transformer CNN GNN MLP.ipynb
Changed working directory to: /Users/rakibhhridoy/Five_Rivers/gis/SedimentRainyAE
âœ“ Modified ALPHA_EARTH_OPTION to 'B'
Executing notebook...
âœ— Notebook execution failed
Error type: CellExecutionError
Error message: An error occurred while executing the following cell:
------------------
import pandas as pd
import numpy as np
import glob
import os
import rasterio
from rasterio.windows import Window
from scipy.spatial import distance_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout, MultiHeadAttention, LayerNormalization, Reshape
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import Sequence
import tensorflow as tf
import gc # Import garbage collector
import pickle # Import the pickle library for saving objects
import matplotlib.pyplot as plt # Import matplotlib for plotting
# Define the single buffer size to use
BUFFER_METERS = 500
# ==================== 1. Load D

Full traceback:
