/Users/rakibhhridoy/Library/Python/3.9/lib/python/site-packages/nbformat/__init__.py:96: MissingIDFieldWarning: Cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.
  validate(nb)

Loading notebook: /Users/rakibhhridoy/Five_Rivers/gis/SedimentRainyAE/Transformer CNN GNN MLP.ipynb
Changed working directory to: /Users/rakibhhridoy/Five_Rivers/gis/SedimentRainyAE
âœ“ Modified ALPHA_EARTH_OPTION to 'B'
Executing notebook...
âœ— Notebook execution failed: An error occurred while executing the following cell:
------------------
import pandas as pdimport numpy as npimport globimport osimport rasteriofrom rasterio.windows import Windowfrom scipy.spatial import distance_matrixfrom sklearn.preprocessing import StandardScalerfrom sklearn.metrics import r2_score, mean_squared_errorfrom sklearn.model_selection import KFoldfrom tensorflow.keras.models import Modelfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout, MultiHeadAttention, LayerNormalization, Reshapefrom tensorflow.keras.optimizers import Adamfrom tensorflow.keras.callbacks import EarlyStoppingfrom tensorflow.keras.utils import Sequenceimport tensorflow as tfimport gc # Import garbage collectorimport pickle # Import the pickle library for saving objectsimport matplotlib.pyplot as plt # Import matplotlib for plotting# Define the single buffer size to useBUFFER_METERS = 500# ==================== 1. Load Data ==================== #orig = pd.read_csv("../../data/RainySeason.csv")river_100 = pd.read_csv("../data/Samples_100.csv")combined_data = pd.concat([river_100, orig], ignore_index=True)drop_cols = ['Stations','River','Lat','Long','geometry']numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')# ==================== 2. Collect ALL Rasters ==================== #raster_paths = []raster_paths += glob.glob("../CalIndices/*.tif")raster_paths += glob.glob("../LULCMerged/*.tif")raster_paths += glob.glob("../IDW/*.tif")print(f"Using {len(raster_paths)} raster layers for CNN input.")for r in raster_paths:    print(" Â -", os.path.basename(r))# ==================== 3. Create a Custom Data Generator ==================== #def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):    """    Extracts a batch of patches from rasters for a given set of coordinates.    This function is optimized to be called by the data generator for each batch.    """    patches = []    # Loop through each coordinate pair in the batch    for lon, lat in coords:        channels = []        # Loop through each raster file to get a single patch for each raster        for rfile in raster_files:            with rasterio.open(rfile) as src:                try:                    row, col = src.index(lon, lat)                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)                    arr = src.read(1, window=win, boundless=True, fill_value=0)                    arr = arr.astype(np.float32)                    if np.nanmax(arr) != 0:                        arr /= np.nanmax(arr)                except Exception as e:                    print(f"Error processing {rfile} for coordinates ({lon}, {lat}): {e}")                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)            channels.append(arr)        patches.append(np.stack(channels, axis=-1))        return np.array(patches)class DataGenerator(Sequence):    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):        super().__init__(**kwargs)        self.coords = coords        self.mlp_data = mlp_data        self.gnn_data = gnn_data        self.y = y        self.raster_paths = raster_paths        self.buffer_meters = buffer_meters        self.batch_size = batch_size        self.shuffle = shuffle        self.indices = np.arange(len(self.y))        # Pre-calculate patch size from the first raster        with rasterio.open(raster_paths[0]) as src:            res_x, res_y = src.res            self.buffer_pixels_x = int(self.buffer_meters / res_x)            self.buffer_pixels_y = int(self.buffer_meters / res_y)            self.patch_width = 2 * self.buffer_pixels_x            self.patch_height = 2 * self.buffer_pixels_y        self.on_epoch_end()    def __len__(self):        return int(np.floor(len(self.y) / self.batch_size))    def on_epoch_end(self):        if self.shuffle:            np.random.shuffle(self.indices)                def __getitem__(self, index):        # Get batch indices        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]        # Get batch data        batch_coords = self.coords[batch_indices]        batch_mlp = self.mlp_data[batch_indices]        batch_gnn = self.gnn_data[batch_indices, :]        batch_y = self.y[batch_indices]        # Extract CNN patches for the current batch        batch_cnn = extract_patch_for_generator(            batch_coords,            self.raster_paths,            self.buffer_pixels_x,            self.buffer_pixels_y,            self.patch_width,            self.patch_height        )                return (batch_cnn, batch_mlp, batch_gnn), batch_ydef evaluate_model(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=4, return_preds=False):    """    Evaluates the model on given data and returns RÂ², RMSE, and predictions.    """    num_samples = len(y_true)    y_pred_list = []        with rasterio.open(raster_paths[0]) as src:        res_x, res_y = src.res        buffer_pixels_x = int(buffer_meters / res_x)        buffer_pixels_y = int(buffer_meters / res_y)        patch_width = 2 * buffer_pixels_x        patch_height = 2 * buffer_pixels_y    for i in range(0, num_samples, batch_size):        batch_coords = coords[i:i+batch_size]        batch_mlp = mlp_data[i:i+batch_size]        batch_gnn = gnn_data[i:i+batch_size, :]                batch_cnn = extract_patch_for_generator(            batch_coords, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height        )                y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn)).flatten())            y_pred = np.concatenate(y_pred_list)        if return_preds:        return y_pred    else:        r2 = r2_score(y_true, y_pred)        rmse = np.sqrt(mean_squared_error(y_true, y_pred))        return r2, rmsedef calculate_permutation_importance(model, coords, mlp_data, gnn_data, y_true, raster_paths, numeric_cols, buffer_meters, batch_size=4):    """    Calculates permutation feature importance for the three model branches    and for each individual numeric feature.    """    print("\nStarting Permutation Feature Importance Analysis...")        # Get baseline RÂ² on the unshuffled data    baseline_r2, _ = evaluate_model(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)    print(f"Baseline RÂ² on test set: {baseline_r2:.4f}\n")    importance = {}        # Permute CNN input (all rasters at once)    print("Permuting CNN features...")    shuffled_indices_cnn = np.random.permutation(len(y_true))    coords_shuffled = coords[shuffled_indices_cnn]    shuffled_r2, _ = evaluate_model(model, coords_shuffled, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)    importance['CNN_all_rasters'] = baseline_r2 - shuffled_r2        # Permute GNN input    print("Permuting GNN features...")    shuffled_gnn_data = gnn_data.copy()    np.random.shuffle(shuffled_gnn_data)    shuffled_r2, _ = evaluate_model(model, coords, mlp_data, shuffled_gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)    importance['GNN_distance_matrix'] = baseline_r2 - shuffled_r2        # Permute each MLP input feature individually    print("Permuting individual numeric features...")    for i, col in enumerate(numeric_cols):        # Create a copy to shuffle a single column        shuffled_mlp_data = mlp_data.copy()                # Get the index of the column to shuffle in the numpy array        mlp_data_df = pd.DataFrame(mlp_data, columns=numeric_cols)        col_index = mlp_data_df.columns.get_loc(col)                # Shuffle only the values for this specific column        shuffled_col = shuffled_mlp_data[:, col_index].copy()        np.random.shuffle(shuffled_col)        shuffled_mlp_data[:, col_index] = shuffled_col                # Evaluate model with the single shuffled feature        shuffled_r2, _ = evaluate_model(model, coords, shuffled_mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)        importance[f'MLP_{col}'] = baseline_r2 - shuffled_r2    return importance# --- NEW FUNCTION: Plotting the training history ---def plot_training_history(history, fold_num=None):    """    Plots the training and validation loss over the epochs.    """    loss = history.history['loss']    val_loss = history.history['val_loss']    epochs = range(1, len(loss) + 1)        plt.figure(figsize=(10, 6))    plt.plot(epochs, loss, 'bo', label='Training Loss')    plt.plot(epochs, val_loss, 'b', label='Validation Loss')    if fold_num is not None:        plt.title(f'Training and Validation Loss for Fold {fold_num}')    else:        plt.title('Training and Validation Loss')    plt.xlabel('Epochs')    plt.ylabel('Loss (MSE)')    plt.legend()    plt.show()    print("Training history plot generated.")# ==================== 4. Prepare all data for Train/Test Single Split ==================== #print("Preparing all data for Train/Test cross-validation...")# Prepare GNN & MLP input for the entire datasetcoords_all = combined_data[['Long', 'Lat']].valuesdist_mat_all = distance_matrix(coords_all, coords_all)gnn_all = np.exp(-dist_mat_all/10)scaler = StandardScaler()mlp_all = scaler.fit_transform(combined_data[numeric_cols])y_all = combined_data['RI'].values# ==================== 5. Define Transformer-based Fusion Model ==================== #def build_transformer_fusion_model(patch_shape, gnn_dim, mlp_dim):    # Inputs for all branches    cnn_input = Input(shape=patch_shape, name="cnn_input")    mlp_input = Input(shape=(mlp_dim,), name="mlp_input")    gnn_input = Input(shape=(gnn_dim,), name="gnn_input")        # --- CNN Branch ---    cnn_branch = Conv2D(32, (3,3), activation="relu", padding="same")(cnn_input)    cnn_branch = MaxPooling2D((2,2))(cnn_branch)    cnn_branch = Conv2D(64, (3,3), activation="relu", padding="same")(cnn_branch)    cnn_branch = MaxPooling2D((2,2))(cnn_branch)    cnn_embedding = Flatten(name="cnn_embedding_flatten")(cnn_branch)        # --- MLP Branch ---    mlp_embedding = Dense(128, activation="relu")(mlp_input)    mlp_embedding = Dense(64, activation="relu", name="mlp_embedding")(mlp_embedding)    # --- GNN Branch ---    gnn_embedding = Dense(128, activation="relu")(gnn_input)    gnn_embedding = Dense(64, activation="relu", name="gnn_embedding")(gnn_embedding)    # --- Transformer Fusion ---    # To feed into the transformer, we need to make all embeddings have the same dimension.    # Let's use a dense layer to project them to a common size.    projection_dim = 64    cnn_proj = Dense(projection_dim)(cnn_embedding)    mlp_proj = Dense(projection_dim)(mlp_embedding)    gnn_proj = Dense(projection_dim)(gnn_embedding)    # Stack the embeddings to create a sequence for the transformer    # Shape becomes (None, 3, projection_dim)    cnn_expanded = Reshape((1, projection_dim))(cnn_proj)    mlp_expanded = Reshape((1, projection_dim))(mlp_proj)    gnn_expanded = Reshape((1, projection_dim))(gnn_proj)    embeddings = Concatenate(axis=1)([cnn_expanded, mlp_expanded, gnn_expanded])    # Transformer Encoder block    transformer_output = MultiHeadAttention(        num_heads=4,        key_dim=projection_dim    )(embeddings, embeddings)    transformer_output = Dropout(0.2)(transformer_output)    transformer_output = LayerNormalization(epsilon=1e-6)(embeddings + transformer_output)        # The output from the transformer is a sequence of 3 vectors.    transformer_output_flattened = Flatten()(transformer_output)        # Final dense layers for prediction    f = Dense(128, activation="relu")(transformer_output_flattened)    f = Dropout(0.4)(f)    f = Dense(64, activation="relu")(f)    output = Dense(1, activation="linear", name="final_output")(f)    # Build and compile the model    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)    model.compile(optimizer=Adam(learning_rate=0.0005), loss="mse")    return model# ==================== 6. Implement Train/Test Single Split ==================== #print("\n" + "="*80)print("Starting Train/Test Single Split (K=5)")print("="*80)# Initialize Train/Testn_splits = 5k_folds = KFold(n_splits=n_splits, shuffle=True, random_state=42)fold_r2_scores = []fold_rmse_scores = []batch_size = 4# Calculate CNN patch shape oncewith rasterio.open(raster_paths[0]) as src:    res_x, res_y = src.res    buffer_pixels_x = int(BUFFER_METERS / res_x)    patch_width = 2 * buffer_pixels_x    cnn_patch_shape = (patch_width, patch_width, len(raster_paths))mlp_input_dim = mlp_all.shape[1]# Loop through each foldfor fold, (train_index, val_index) in enumerate(k_folds.split(combined_data)):    print(f"\n--- Fold {fold+1}/{n_splits} ---")    # Split the data for the current fold    coords_train_fold, coords_val_fold = coords_all[train_index], coords_all[val_index]    mlp_train_fold, mlp_val_fold = mlp_all[train_index], mlp_all[val_index]    y_train_fold, y_val_fold = y_all[train_index], y_all[val_index]    # Handle the GNN data split    # gnn_train_fold is the sub-matrix for the training points    gnn_train_fold = gnn_all[train_index][:, train_index]    # gnn_val_fold is the matrix of validation points' distances to training points    gnn_val_fold = gnn_all[val_index][:, train_index]    # Build a fresh model for each fold    model = build_transformer_fusion_model(cnn_patch_shape, len(gnn_train_fold), mlp_input_dim)        # Create data generators for the current fold    train_generator = DataGenerator(        coords=coords_train_fold, mlp_data=mlp_train_fold, gnn_data=gnn_train_fold, y=y_train_fold,        raster_paths=raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, shuffle=True    )    val_generator = DataGenerator(        coords=coords_val_fold, mlp_data=mlp_val_fold, gnn_data=gnn_val_fold, y=y_val_fold,        raster_paths=raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, shuffle=False    )    # Train the model with early stopping    early_stopping = EarlyStopping(        monitor='val_loss',        patience=10,        restore_best_weights=True    )    history = model.fit(        train_generator,        epochs=100,        verbose=1,        callbacks=[early_stopping],        validation_data=val_generator    )    # Evaluate the model on the validation set for this fold    r2_fold, rmse_fold = evaluate_model(        model, coords_val_fold, mlp_val_fold, gnn_val_fold, y_val_fold,        raster_paths, BUFFER_METERS, batch_size=batch_size    )    # Store the results    fold_r2_scores.append(r2_fold)    fold_rmse_scores.append(rmse_fold)        print(f"Fold {fold+1} RÂ²: {r2_fold:.4f} | RMSE: {rmse_fold:.4f}")    models_dir = 'models/transformer'    tf_save_path = os.path.join(models_dir, f'transformer_cnn_gnn_mlp{fold+1}.keras')    model.save(tf_save_path)    # Clean up memory    del model, history, train_generator, val_generator    gc.collect()# ==================== 7. Summarize Single Split Results ==================== #print("\n" + "="*80)print(f"Overall Single Split Results ({n_splits} Folds)")print("="*80)print(f"Average RÂ²: {np.mean(fold_r2_scores):.4f} +/- {np.std(fold_r2_scores):.4f}")print(f"Average RMSE: {np.mean(fold_rmse_scores):.4f} +/- {np.std(fold_rmse_scores):.4f}")
------------------


[0;36m  Cell [0;32mIn[4], line 1[0;36m[0m
[0;31m    import pandas as pdimport numpy as npimport globimport osimport rasteriofrom rasterio.windows import Windowfrom scipy.spatial import distance_matrixfrom sklearn.preprocessing import StandardScalerfrom sklearn.metrics import r2_score, mean_squared_errorfrom sklearn.model_selection import KFoldfrom tensorflow.keras.models import Modelfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout, MultiHeadAttention, LayerNormalization, Reshapefrom tensorflow.keras.optimizers import Adamfrom tensorflow.keras.callbacks import EarlyStoppingfrom tensorflow.keras.utils import Sequenceimport tensorflow as tfimport gc # Import garbage collectorimport pickle # Import the pickle library for saving objectsimport matplotlib.pyplot as plt # Import matplotlib for plotting# Define the single buffer size to useBUFFER_METERS = 500# ==================== 1. Load Data ==================== #orig = pd.read_csv("../../data/RainySeason.csv")river_100 = pd.read_csv("../data/Samples_100.csv")combined_data = pd.concat([river_100, orig], ignore_index=True)drop_cols = ['Stations','River','Lat','Long','geometry']numeric_cols = orig.drop(columns=drop_cols).columns.drop('RI')# ==================== 2. Collect ALL Rasters ==================== #raster_paths = []raster_paths += glob.glob("../CalIndices/*.tif")raster_paths += glob.glob("../LULCMerged/*.tif")raster_paths += glob.glob("../IDW/*.tif")print(f"Using {len(raster_paths)} raster layers for CNN input.")for r in raster_paths:    print(" Â -", os.path.basename(r))# ==================== 3. Create a Custom Data Generator ==================== #def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):    """    Extracts a batch of patches from rasters for a given set of coordinates.    This function is optimized to be called by the data generator for each batch.    """    patches = []    # Loop through each coordinate pair in the batch    for lon, lat in coords:        channels = []        # Loop through each raster file to get a single patch for each raster        for rfile in raster_files:            with rasterio.open(rfile) as src:                try:                    row, col = src.index(lon, lat)                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)                    arr = src.read(1, window=win, boundless=True, fill_value=0)                    arr = arr.astype(np.float32)                    if np.nanmax(arr) != 0:                        arr /= np.nanmax(arr)                except Exception as e:                    print(f"Error processing {rfile} for coordinates ({lon}, {lat}): {e}")                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)            channels.append(arr)        patches.append(np.stack(channels, axis=-1))        return np.array(patches)class DataGenerator(Sequence):    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):        super().__init__(**kwargs)        self.coords = coords        self.mlp_data = mlp_data        self.gnn_data = gnn_data        self.y = y        self.raster_paths = raster_paths        self.buffer_meters = buffer_meters        self.batch_size = batch_size        self.shuffle = shuffle        self.indices = np.arange(len(self.y))        # Pre-calculate patch size from the first raster        with rasterio.open(raster_paths[0]) as src:            res_x, res_y = src.res            self.buffer_pixels_x = int(self.buffer_meters / res_x)            self.buffer_pixels_y = int(self.buffer_meters / res_y)            self.patch_width = 2 * self.buffer_pixels_x            self.patch_height = 2 * self.buffer_pixels_y        self.on_epoch_end()    def __len__(self):        return int(np.floor(len(self.y) / self.batch_size))    def on_epoch_end(self):        if self.shuffle:            np.random.shuffle(self.indices)                def __getitem__(self, index):        # Get batch indices        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]        # Get batch data        batch_coords = self.coords[batch_indices]        batch_mlp = self.mlp_data[batch_indices]        batch_gnn = self.gnn_data[batch_indices, :]        batch_y = self.y[batch_indices]        # Extract CNN patches for the current batch        batch_cnn = extract_patch_for_generator(            batch_coords,            self.raster_paths,            self.buffer_pixels_x,            self.buffer_pixels_y,            self.patch_width,            self.patch_height        )                return (batch_cnn, batch_mlp, batch_gnn), batch_ydef evaluate_model(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=4, return_preds=False):    """    Evaluates the model on given data and returns RÂ², RMSE, and predictions.    """    num_samples = len(y_true)    y_pred_list = []        with rasterio.open(raster_paths[0]) as src:        res_x, res_y = src.res        buffer_pixels_x = int(buffer_meters / res_x)        buffer_pixels_y = int(buffer_meters / res_y)        patch_width = 2 * buffer_pixels_x        patch_height = 2 * buffer_pixels_y    for i in range(0, num_samples, batch_size):        batch_coords = coords[i:i+batch_size]        batch_mlp = mlp_data[i:i+batch_size]        batch_gnn = gnn_data[i:i+batch_size, :]                batch_cnn = extract_patch_for_generator(            batch_coords, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height        )                y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn)).flatten())            y_pred = np.concatenate(y_pred_list)        if return_preds:        return y_pred    else:        r2 = r2_score(y_true, y_pred)        rmse = np.sqrt(mean_squared_error(y_true, y_pred))        return r2, rmsedef calculate_permutation_importance(model, coords, mlp_data, gnn_data, y_true, raster_paths, numeric_cols, buffer_meters, batch_size=4):    """    Calculates permutation feature importance for the three model branches    and for each individual numeric feature.    """    print("\nStarting Permutation Feature Importance Analysis...")        # Get baseline RÂ² on the unshuffled data    baseline_r2, _ = evaluate_model(model, coords, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)    print(f"Baseline RÂ² on test set: {baseline_r2:.4f}\n")    importance = {}        # Permute CNN input (all rasters at once)    print("Permuting CNN features...")    shuffled_indices_cnn = np.random.permutation(len(y_true))    coords_shuffled = coords[shuffled_indices_cnn]    shuffled_r2, _ = evaluate_model(model, coords_shuffled, mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)    importance['CNN_all_rasters'] = baseline_r2 - shuffled_r2        # Permute GNN input    print("Permuting GNN features...")    shuffled_gnn_data = gnn_data.copy()    np.random.shuffle(shuffled_gnn_data)    shuffled_r2, _ = evaluate_model(model, coords, mlp_data, shuffled_gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)    importance['GNN_distance_matrix'] = baseline_r2 - shuffled_r2        # Permute each MLP input feature individually    print("Permuting individual numeric features...")    for i, col in enumerate(numeric_cols):        # Create a copy to shuffle a single column        shuffled_mlp_data = mlp_data.copy()                # Get the index of the column to shuffle in the numpy array        mlp_data_df = pd.DataFrame(mlp_data, columns=numeric_cols)        col_index = mlp_data_df.columns.get_loc(col)                # Shuffle only the values for this specific column        shuffled_col = shuffled_mlp_data[:, col_index].copy()        np.random.shuffle(shuffled_col)        shuffled_mlp_data[:, col_index] = shuffled_col                # Evaluate model with the single shuffled feature        shuffled_r2, _ = evaluate_model(model, coords, shuffled_mlp_data, gnn_data, y_true, raster_paths, buffer_meters, batch_size=batch_size)        importance[f'MLP_{col}'] = baseline_r2 - shuffled_r2    return importance# --- NEW FUNCTION: Plotting the training history ---def plot_training_history(history, fold_num=None):    """    Plots the training and validation loss over the epochs.    """    loss = history.history['loss']    val_loss = history.history['val_loss']    epochs = range(1, len(loss) + 1)        plt.figure(figsize=(10, 6))    plt.plot(epochs, loss, 'bo', label='Training Loss')    plt.plot(epochs, val_loss, 'b', label='Validation Loss')    if fold_num is not None:        plt.title(f'Training and Validation Loss for Fold {fold_num}')    else:        plt.title('Training and Validation Loss')    plt.xlabel('Epochs')    plt.ylabel('Loss (MSE)')    plt.legend()    plt.show()    print("Training history plot generated.")# ==================== 4. Prepare all data for Train/Test Single Split ==================== #print("Preparing all data for Train/Test cross-validation...")# Prepare GNN & MLP input for the entire datasetcoords_all = combined_data[['Long', 'Lat']].valuesdist_mat_all = distance_matrix(coords_all, coords_all)gnn_all = np.exp(-dist_mat_all/10)scaler = StandardScaler()mlp_all = scaler.fit_transform(combined_data[numeric_cols])y_all = combined_data['RI'].values# ==================== 5. Define Transformer-based Fusion Model ==================== #def build_transformer_fusion_model(patch_shape, gnn_dim, mlp_dim):    # Inputs for all branches    cnn_input = Input(shape=patch_shape, name="cnn_input")    mlp_input = Input(shape=(mlp_dim,), name="mlp_input")    gnn_input = Input(shape=(gnn_dim,), name="gnn_input")        # --- CNN Branch ---    cnn_branch = Conv2D(32, (3,3), activation="relu", padding="same")(cnn_input)    cnn_branch = MaxPooling2D((2,2))(cnn_branch)    cnn_branch = Conv2D(64, (3,3), activation="relu", padding="same")(cnn_branch)    cnn_branch = MaxPooling2D((2,2))(cnn_branch)    cnn_embedding = Flatten(name="cnn_embedding_flatten")(cnn_branch)        # --- MLP Branch ---    mlp_embedding = Dense(128, activation="relu")(mlp_input)    mlp_embedding = Dense(64, activation="relu", name="mlp_embedding")(mlp_embedding)    # --- GNN Branch ---    gnn_embedding = Dense(128, activation="relu")(gnn_input)    gnn_embedding = Dense(64, activation="relu", name="gnn_embedding")(gnn_embedding)    # --- Transformer Fusion ---    # To feed into the transformer, we need to make all embeddings have the same dimension.    # Let's use a dense layer to project them to a common size.    projection_dim = 64    cnn_proj = Dense(projection_dim)(cnn_embedding)    mlp_proj = Dense(projection_dim)(mlp_embedding)    gnn_proj = Dense(projection_dim)(gnn_embedding)    # Stack the embeddings to create a sequence for the transformer    # Shape becomes (None, 3, projection_dim)    cnn_expanded = Reshape((1, projection_dim))(cnn_proj)    mlp_expanded = Reshape((1, projection_dim))(mlp_proj)    gnn_expanded = Reshape((1, projection_dim))(gnn_proj)    embeddings = Concatenate(axis=1)([cnn_expanded, mlp_expanded, gnn_expanded])    # Transformer Encoder block    transformer_output = MultiHeadAttention(        num_heads=4,        key_dim=projection_dim    )(embeddings, embeddings)    transformer_output = Dropout(0.2)(transformer_output)    transformer_output = LayerNormalization(epsilon=1e-6)(embeddings + transformer_output)        # The output from the transformer is a sequence of 3 vectors.    transformer_output_flattened = Flatten()(transformer_output)        # Final dense layers for prediction    f = Dense(128, activation="relu")(transformer_output_flattened)    f = Dropout(0.4)(f)    f = Dense(64, activation="relu")(f)    output = Dense(1, activation="linear", name="final_output")(f)    # Build and compile the model    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)    model.compile(optimizer=Adam(learning_rate=0.0005), loss="mse")    return model# ==================== 6. Implement Train/Test Single Split ==================== #print("\n" + "="*80)print("Starting Train/Test Single Split (K=5)")print("="*80)# Initialize Train/Testn_splits = 5k_folds = KFold(n_splits=n_splits, shuffle=True, random_state=42)fold_r2_scores = []fold_rmse_scores = []batch_size = 4# Calculate CNN patch shape oncewith rasterio.open(raster_paths[0]) as src:    res_x, res_y = src.res    buffer_pixels_x = int(BUFFER_METERS / res_x)    patch_width = 2 * buffer_pixels_x    cnn_patch_shape = (patch_width, patch_width, len(raster_paths))mlp_input_dim = mlp_all.shape[1]# Loop through each foldfor fold, (train_index, val_index) in enumerate(k_folds.split(combined_data)):    print(f"\n--- Fold {fold+1}/{n_splits} ---")    # Split the data for the current fold    coords_train_fold, coords_val_fold = coords_all[train_index], coords_all[val_index]    mlp_train_fold, mlp_val_fold = mlp_all[train_index], mlp_all[val_index]    y_train_fold, y_val_fold = y_all[train_index], y_all[val_index]    # Handle the GNN data split    # gnn_train_fold is the sub-matrix for the training points    gnn_train_fold = gnn_all[train_index][:, train_index]    # gnn_val_fold is the matrix of validation points' distances to training points    gnn_val_fold = gnn_all[val_index][:, train_index]    # Build a fresh model for each fold    model = build_transformer_fusion_model(cnn_patch_shape, len(gnn_train_fold), mlp_input_dim)        # Create data generators for the current fold    train_generator = DataGenerator(        coords=coords_train_fold, mlp_data=mlp_train_fold, gnn_data=gnn_train_fold, y=y_train_fold,        raster_paths=raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, shuffle=True    )    val_generator = DataGenerator(        coords=coords_val_fold, mlp_data=mlp_val_fold, gnn_data=gnn_val_fold, y=y_val_fold,        raster_paths=raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, shuffle=False    )    # Train the model with early stopping    early_stopping = EarlyStopping(        monitor='val_loss',        patience=10,        restore_best_weights=True    )    history = model.fit(        train_generator,        epochs=100,        verbose=1,        callbacks=[early_stopping],        validation_data=val_generator    )    # Evaluate the model on the validation set for this fold    r2_fold, rmse_fold = evaluate_model(        model, coords_val_fold, mlp_val_fold, gnn_val_fold, y_val_fold,        raster_paths, BUFFER_METERS, batch_size=batch_size    )    # Store the results    fold_r2_scores.append(r2_fold)    fold_rmse_scores.append(rmse_fold)        print(f"Fold {fold+1} RÂ²: {r2_fold:.4f} | RMSE: {rmse_fold:.4f}")    models_dir = 'models/transformer'    tf_save_path = os.path.join(models_dir, f'transformer_cnn_gnn_mlp{fold+1}.keras')    model.save(tf_save_path)    # Clean up memory    del model, history, train_generator, val_generator    gc.collect()# ==================== 7. Summarize Single Split Results ==================== #print("\n" + "="*80)print(f"Overall Single Split Results ({n_splits} Folds)")print("="*80)print(f"Average RÂ²: {np.mean(fold_r2_scores):.4f} +/- {np.std(fold_r2_scores):.4f}")print(f"Average RMSE: {np.mean(fold_rmse_scores):.4f} +/- {np.std(fold_rmse_scores):.4f}")[0m
[0m                              ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax


